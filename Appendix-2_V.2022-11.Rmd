---
title: Appendix 2. Transcript Annotation and Seasonal Gene Expression in Kidney Tissue
  of the Australian Pygmy Bluetongue Skink, _Tiliqua adelaidensis_
author: "Carmel Maher"
date: '2022'
output:
  pdf_document: default
  html_document:
    df_print: paged
numbersections: yes
header-includes: \usepackage{fvextra} \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
  \pagenumbering{gobble}
toc: yes
toc_depth: 3
urlcolor: blue
---

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE, eval=TRUE, warning=FALSE, Message=FALSE, error=FALSE, include=TRUE)
options(tinytex.verbose = TRUE)
```

```{r, echo=TRUE, results='hide', error=FALSE, warning=FALSE, message=FALSE}
library(markdown)
library(rmdformats)
library(formatR)
library(edgeR)
library(dplyr)
library(stringr)
library(stringi)
library(RColorBrewer)
library(gplots)
```

***

# Analysis Workflow

**Transcript Annotation and Seasonal Gene Expression in Kidney Tissue of the Australian Pygmy Bluetongue Skink, _Tiliqua adelaidensis_**  

This document outlines the pipeline methods for assembly of poly-a selected transcripts extracted from kidney tissues of three male and five female Australian pygmy bluetongue skinks collected in South Australia during spring (September) and autumn (March or April). Long-read transcripts were sequenced from one individual "G6" using Pacific Biosciences' IsoSeq Sequel platform, and short-reads for all eight individuals were sequenced on an Illumina HiSeq. The wet-lab library preparation up to sequencing is outlined in Thesis Methods Chapter 2.  

Sample ID  | Experimental factors
:------------ | :------------
G1 | September Female
G2 | September Female
G3 | March/April Female
G4 | March/April Male
G5 | September Male
G6 | September Male
G7 | March/April Female
G8 | March/April Female


There are three main sections to this document, which are referenced in three separate chapters of the thesis:

**2. Assembly (Chapter 3):** Assembly, cleaning and clustering of transcript isoforms from **both** sequencing formats to form a pseudo-reference for _T. adelaidensis_ and to compare these methods.

**3. Annotation (Chapter 4):** Annotation and further analysis of the full length transcript isoforms derived from the long-read analysis.

**4. Gene Expression (Chapter 5):** Gene expression analysis using the short-read data aligned to the long-read pseudo-reference produced in Chapter 3, to assess seasonal variation of gene expression.



***
\newpage

Analysis was done using a combination of computing and HPC machines with specifications as below: 

- Long-read cleaning, read frame prediction and blast searches were run on the Australian National eResearch Collaboration Tools and Resources project (NECTAR) cloud, using an ‘m1.xlarge flavour’ allocation with 8 AMD Opteron 63xx class CPUs, 32GB RAM, 10GB root disk, and 240GB of secondary disk storage. 
- The ANGEL program which required a higher memory allocation was run on a Dell PowerEdge server with 40 cores and 512G RAM.
- Short-read QC, trimming, initial trinity assembly, and alignments were completed using Flinders University’s high-performance computer “Deep thought” with 16 x 256Gb (4TB RAM), 1024 AMD x86 CPU cores at 2.0GHz across 14 standard compute and 2 data science nodes, and 100 TiB of usable storage via the Dell EMC Network Storage reference architecture.
- Downstream gene expression analysis and manual manipulation of annotation files was completed on a local Intel core i7 machine using RStudio or in-house bash scripts applied in Git Bash for Windows.

All larger scripts included in this document have also been uploaded to Github separately and are linked here at the relevant steps.  
All scripts and commands in this pipeline depend on specific directory structure, and may show inconsistencies in this structure between steps where files have been transferred between computing resources. The input and output files of note are highlighted in the text between steps to bridge these gaps. 




***
\newpage

# Assembly (Chapter 3)

A genome from a closely related species was unavailable for these data. Gene expression analysis of short-read data was initially intended to be carried out using a _de-novo_ assembly of this same data as a pseudo-reference for expression counts.  
When long-read data became available, the assembly of short read data, and the processed full length transcript datasets were compared. The long-read assembly then provided the reference for expression counts in Chapter 5.  

This section is divided into the two methods of processing; using the long-read IsoSeq data, and using the short-read HiSeq data. Both these sections contain titles referencing the main program in use at each step.  

***

## <ins>**_Long-read data_**</ins>

Cleaning and clustering of long-read IsoSeq data sequenced from poly-a selected RNA extracted from a single kidney from september male G6. These sequences were be used to annotate genes found in kidney tissue of T. adelaidensis and create a set of reference transcripts for gene expression analysis conducted in later chapters.  

Sequences were downloaded from Pacific Biosciences & checksumms verified.




### SMRT Tools - IsoSeq3

SMRT Tools provides various programs for appropriate treatment of Pacific Biosciences sequencing outputs. IsoSeq3 was used to collapse circular sequences, initial polishing, and to characterise full length transcript reads.
This analysis was conducted in an environment on the eRSA NECTAR research cloud.  

A primer reference file was created manually for reference in the Isoseq3 script using the primers listed in the Clontech SMARTer cDNA Library prep kit which was used during the synthesis of cDNA for these samples.  

[primers.fasta](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/primers.fasta):
```{bash, eval=F}
      >primer_5p
      AAGCAGTGGTATCAACGCAGAGTACATGGG
      >primer_3p
      GTACTCTGCGTTGATACCACTGCTT
```

A single script was used to complete the Pacific Biosciences Isoseq3 pipeline 
[isoseq3.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/isoseq3.sh): 

```{bash, eval=F}
      #!/bin/bash
      #
      # This script is used to characterise full length transcripts de novo using smrttools and bioconda as specified by PacBio workflow
      # see: https://github.com/PacificBiosciences/IsoSeq3/blob/master/README_v3.0.md and follow links for bioconda references
      #
      # The paths in the script assume that a specific directory structure has been set up.
      # This directory structure is for the Nectar cloud VM
      
      # open a screen
      # usage from /mnt
      # bash isoseq3.sh <ctrl-a ctrl-d>
      #
      # Carmel Maher
      # April 2019
      
      
      #---------Installed Programs---------
      
      # smrttools				-> smrttools-release_6.0.0.47835
      	# help:
      	# /pacbio/smrtlink/install/smrtlink-release_6.0.0.47841/bundles/smrttools/install/ smrttools-release_6.0.0.47835/smrtcmds/bin/isoseq3 -h
      	# /pacbio/smrtlink/install/smrtlink-release_6.0.0.47841/bundles/smrttools/install/ smrttools-release_6.0.0.47835/smrtcmds/bin/isoseq3 --version
      	# Version: isoseq3 3.0.0 (commit v3.0.0-7-gcc6cddd)
      
      # python 2.7 			-> to run miniconda - automatically installed on nectar
      # miniconda 			-> recommended python 2.7 compatible version by PacBio workflow. Miniconda2 | VER: 4.6.14
      # ccs 					-> installed through conda, inside pbccs package | VER: pbccs-3.4.1
      # lima					-> installed through conda | VER: lima-1.9.0
      
      
      #------adjust these for your run------
      
      #as per the local installation, the smrttools program directories are (in /mnt). Notable directories listed in installation: 
      SMRT_ROOT=/mnt/pacbio/smrtlink
      
      # Isoseq3 executeable location (from /mnt)
      Iso3_DIR=$SMRT_ROOT/install/smrtlink-release_6.0.0.47841/bundles/smrttools/install/ smrttools-release_6.0.0.47835/smrtcmds/bin
      
      DATA=/mnt/data
      MOVIE="MAH6260A1_m54196_190204_223227"
      THREADS=0 	#0 for the Iseseq3 tools =autodetection of threads and is the default
      
      
      #------------------------------------

      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
      
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }

      # go to the working data directory
      cd $DATA || error_exit "$LINENO: directory error"

      # Generate consensus sequnces (ccs file) from raw subread data
      # NOTE: this automatically runs with default autodetection of threads =  8
      ccs $MOVIE.subreads.bam $MOVIE.ccs.bam --noPolish --minPasses 1 || error_exit "$LINENO: consensus sequence ccs error"
      
      echo " - - - consensus sequnces generated"

      # The primers.fasta as per the recommended Clontech SMARTer cDNA library prep - no barcodes used for this sample 
      # primers.fasta has been uploaded to /mnt/data and does not need to be created here:
      		# primers.fasta
      		# >primer_5p
      		# AAGCAGTGGTATCAACGCAGAGTACATGGG
      		# >primer_3p
      		# GTACTCTGCGTTGATACCACTGCTT

      # Remove primers and demultiplex:
      lima --isoseq --dump-clips --no-pbi $MOVIE.ccs.bam primers.fasta demux.bam || error_exit "$LINENO: Primer demultiplex lima error"
      
      echo " - - - primers removed and demultiplexed"
      
          			#!#!#!#!# 
            		# CHECK #  Note: A search using Git bash on the output files for any of the primers listed for Isoseq by Pacific biosciences returns no results after this script
            		#!#!#!#!#
      
      # *********************
      # From here on, execute the following steps for each output BAM file. -- only one in this case.
      # isoseq3 tool usage: isoseq3 <tool>
      # due to nectar permissions and installation location, usage: $Iso3_DIR/isoseq3 <tool>
      # *********************

      ### cluster Tool: Cluster CCS reads and generate unpolished transcripts.
      # recommended to give this step as many cores as possible
      # Usage
      # isoseq3 cluster [options] input output
      # Example
      # isoseq3 cluster <two types of potential inputfile> demux.bam <OR> $MOVIE.consensusreadset.xml  unpolished.bam
      
      # Cluster consensus sequences to generate unpolished transcripts:
      # note the demux.bam is named after the headers used in primers.fasta
      $Iso3_DIR/isoseq3 cluster --verbose -j $THREADS demux.primer_5p--primer_3p.bam unpolished.bam || error_exit "$LINENO: Isoseq3 cluster error"
      
      echo " - - - css reads clustered"

      ### polish Tool: Polish transcripts using subreads.
      # Usage
      # isoseq3 polish [options] input_1 input_2 output
      
      # Polish transcripts using subreads:
      $Iso3_DIR/isoseq3 polish --verbose -j $THREADS unpolished.bam $MOVIE.subreads.bam polished.bam || error_exit "$LINENO: Isoseq3 polish error"
      
      echo " - - - transcripts polished"

      ### summarize Tool: Create a .csv-format barcode overview from transcripts.
      # Usage
      # isoseq3 summarize [options] input output
      $Iso3_DIR/isoseq3 summarize --verbose polished.bam summary.csv || error_exit "$LINENO: Isoseq3 summary error"
      
      echo " - - - summary file created"
      echo " - - - done"
```

All of the above output was moved into a folder ~/data/2019-04-30_isoseq3_out

The output from this script, the polished.hq.fasta.gz file was unzipped and then used as the input for the program Cogent.


\newpage

### Cogent

Transcript clustering and collapse of redundant isoforms.
Line by line instructions for running Cogent for clustering of transcript isoforms in a screen on the NECTAR machine as below. Includes instructions for Cupcake and minimap to collapse redundant transcript isoforms.    

Some notes and commentary are based on personal communication with Tessa Bradford & Terry Bertozzi at the SA Museum, as well as the [Cogent GitHub Tutorial page](https://github.com/Magdoll/Cogent/wiki/Running-Cogent)

Environments were used for installations (most often achieved with mini conda) on the NEXTAR machine.
Activate screen, activate the environment, make Cogent output directory, and go to analysis working directory
```{bash, eval=F}      
      screen -s cogent
      conda activate anaCogent
      mkdir /mnt/IsoSeq-analysis/data/Cogent
      cd /mnt/IsoSeq-analysis/data/Cogent
```

From the analysis directory the input data directory is ../2019-04-30_out/polished.hq.fasta. The first output will be in this original data directory.
Check paths (During installation, all paths were put into .profile)



##### **Family Finding**

Running [Family Finding for a small dataset](https://github.com/Magdoll/Cogent/wiki/Running-Cogent#findsmall)
      
Create a k-mer profile of the input and calculate pairwise distances  
Note: default K-mer sixe =30
```{bash, eval=F}
      python /mnt/Prog/Cogent/Cogent/run_mash.py --cpus=7 /mnt/IsoSeq-analysis/data/2019-04-30_out/polished.hq.fasta
```

The output will be \<fasta_filename\>.k\<sketch_size\>.dist  
Output written to: /mnt/IsoSeq-analysis/data/2019-04-30_out/polished.hq.fasta.s1000k30.dist
      
Process the distance file and create the partitions for each gene family
```{bash, eval=F}
      process_kmer_to_graph.py ../2019-04-30_out/polished.hq.fasta ../2019-04-30_out/polished.hq.fasta.s1000k30.dist ./hq hq
```

This generates an output log file hq.partition.txt and for each partition (isoform set), a subdirectory called hq/\<partition_number\> which contains the subset of fasta sequences belonging to that isoform set. Note that sequences that don't belong to any partition (ones with no similarity with other sequences) will be "unassigned" and noted in the partition log file.
      
      
      
##### **Coding Genome Reconstruction**
      
Each isoform set family must be reconstructed individually for coding region.  
Reconstructed contigs will contain the whole coding region. Reconstructed file is called cogent2.renamed.fasta
      
The script for an individual folder is:
```{bash, eval=F}
      reconstruct_contig.py -S T.adelaidensis hq/hq_0
```

The following script was written to loop this procedure and process hundreds of isoform set folders with sequences requiring coding region identification
[reconstructContig_1.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/reconstructContig_1.sh):
```{bash, eval=F}
      #!/bin/bash
      #
      ## this script is for coding region reconstruction of Cogent gene families as these must be completed individually.
      # This script assumes you are following Running-Cogent.txt and using the same directory structure
      # This script to be used after the process_kmer_to_graph.py step
      
      # script in /mnt/IsoSeq-analysis/src
      # usage from /mnt/IsoSeq-analysis/data/Cogent/hq
      ## bash ../../../src/reconstructContig.sh
      
      # Carmel Maher
      # August 2019
      
      #Terry's error exit
      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      cd ~/mnt/IsoSeq-analysis/data/Cogent/hq
      
      for d in */ ; do 
      
      	DIR=${d%*}	
      	echo $DIR
      	
      	reconstruct_contig.py -S T.adelaidensis ./$DIR || error_exit "$LINENO: Error "$DIR""
      	
      done
      echo compete
```

An error message prompted further checking of folder completion as below

Number of gene families:
```{bash, eval=F}
      ls ./hq/*/cogent2.renamed.fasta | wc -1
      4454
```

Finding failed reconstruction jobs (pers. comm. Terry Bertozzi, SA Museum)

Number of hq gene folders:
```{bash, eval=F}
      ls hq | wc -l #number of folders
      4459
```
\newpage

Number of /hq folders completed:
```{bash, eval=F}
      ls ./hq/*/COGENT.DONE | wc -l 
      4454
```

This matches the total gene family count so all outputs have been recorded correctly, but does not match the total /hq folder count = 5 folders not completed.

The following will list the folders that have not finished:
```{bash, eval=F}
      comm -3 <(find /mnt/IsoSeq-analysis/data/Cogent/hq -iname 'COGENT.DONE' -printf '%h\n' | sort -u) <(find /mnt/IsoSeq-analysis/data/Cogent/hq -maxdepth 1 -mindepth 1 -type d | sort) | sed -e 's/^.*hq\///'
      
      hq_2605
      hq_2738
      hq_320
      hq_6391
      hq_9030
```

Some testing was required to create a modified script with a loop for failed folders, in order to include 5 folders which failed to run with kmer length 30 informed by [this issue](https://github.com/Magdoll/Cogent/issues/40).  
Due to how the loop functions inputting kmer size of 27 on execution will actually run all folders at the desired kmer size 30.
```{bash, eval=F}
    bash /mnt/IsoSeq-analysis/src/TBreconstructContig-edit.sh /mnt/IsoSeq-analysis/data/Cogent/hq 27 T.adelaidensis
```

The modified script [TBreconstructContig-edit.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/TBreconstructContig-edit.sh):
```{bash, eval=F}
      #!/bin/bash
      
      # This script runs reconstruct_contig.py on every subdirectory contained in the top directory. reruns with a kmer parameter of +3 (to a max of 99) for failed directories.
      # This script assumes directories and files exist as up until the Coding Genome Reconstruction step in the Running Cogent GitHub wiki 
      # 
      # usage: 	bash <path/script> <full directory containing hq_folders> <starting kmer value> <Species Name>
      # e.g.	 	bash /mnt/IsoSeq-analysis/src/TBreconstructContig-edit.sh /mnt/IsoSeq-analysis/data/Cogent/hq 27 T.adelaidensis
      
      # DIRNAME=$1
      # species=$3
      
      kmerSize=$2
      
      find $1 -mindepth 1 -maxdepth 1 -type d | while read line; do #lists off all the /hq_* folders within the input directory and passes them through the script one by one (by line)
      	FILE=$line/cogent2.fa
      	if [ ! -f $FILE ]; then
      		echo "failed no cogent2.fa Increase K-mer size" #Note all jobs will "fail" on first attempt unless you are running this in a previously reconstructed directory. In the latter case only the failed directories will continue to rerun.
      		while [ ! -f "$FILE" ];do
      			kmerSize=$((kmerSize + 3))
      			rerunCMD="reconstruct_contig.py -k ${kmerSize} -S $3 $line" #reconstruction script increasing kmer parameter by 3 -note because all directories fail initially the input kmer value should be 3 below the desired actual value.
      			echo ${rerunCMD}
      			eval ${rerunCMD}
      			if [ -f $FILE ]; then 
      				touch $1/hq_kmerSize.txt #This text file will give a list of the used kmerSize per hq_* folder
      				echo "SUCCESS! The increased Kmer size $kmerSize was successful for $line." >> $1/hq_kmerSize.txt
      			fi
      			if [ ${kmerSize} -gt 99 ];then
      				cp $line/in.fa $line/cogent2.fa # if a folder reruns until 99kmer parameter length is reached the input is simply copied into cogent2.fa
      				touch $1/failed-jobs.txt #This text file will remain empty or give a list of all failed folders
      				echo "Failed to succeed reconstruction with largest Kmer on ${line}. Copied input transcripts to cogent2.fa as output." >> $1/failed-jobs.txt
      			fi
      		done
      		kmerSize=$2
      	fi
      done
      
      if [ -s $1/failed-jobs.txt ]; then
          echo "failed-jobs.txt not empty. Some jobs failed. Please re-run them."
      else
          echo "failed-jobs.txt empty or doesnt exist. All jobs completed."
      fi
```

Re-check output.

Number of completed gene families:
```{bash, eval=F}
      ls ./hq/*/COGENT.DONE | wc -l
      4459
```

OR:
```{bash, eval=F}
      ls ./hq/*/cogent2.renamed.fasta | wc -l
      4459
```

\newpage
Finding failed reconstruction jobs.

Number of items in ./hq:
```{bash, eval=F}
      ls hq | wc -l
      4460
```

The added 1 is the text file reporting the successful kmer size for each "gene" family folder. Therefore all have completed.

List the names of folders that have not finished:
```{bash, eval=F}
      comm -3 <(find /mnt/IsoSeq-analysis/data/Cogent/hq -iname 'COGENT.DONE' -printf '%h\n' | sort -u) <(find /mnt/IsoSeq-analysis/data/Cogent/hq -maxdepth 1 -mindepth 1 -type d | sort) | sed -e 's/^.*hq\///'
```

(this command listed no folders, indicating the script has completed all folders successfully)
      
See failed-jobs.txt and/or hq_kmerSize.txt. 
Note: all completed at kmer size 30 this time, and filed-jobs.txt does not exist as none failed.
    


##### **Using Cogent to collapse redundant transcripts in absence of genome** 

[Creating the "fake genome"](https://github.com/Magdoll/Cogent/wiki/Tutorial:-Using-Cogent-to-collapse-redundant-transcripts-in-absence-of-genome)

List and number of unassigned sequences  
Make sure you are in the correct Cogent directory and can see the file hq.partition.txt:
```{bash, eval=F}
      tail -n 1 hq.partition.txt |tr ',' '\n' > unassigned.list
```

Create the unassigned.list file in the same directory:
```{bash, eval=F}
      tail -n 1 hq.partition.txt |tr ',' '\n' | wc -l
      3193
```

Note: this number did not change with the addition of the five failed reconstruction folders. i.e. they failed completely and did not end up in the unassigned dataset. 
      
The unassigned hq sequences were made into a fasta file.
```{bash, eval=F}
      export PATH=$PATH:/mnt/Prog/cDNA_Cupcake/sequence
      get_seqs_from_list.py ../2019-04-30_out/polished.hq.fasta unassigned.list > unassigned.fasta
```

Unassigned sequences were concatenated with the Cogent reconstructed contigs by putting the reconstructed genes plus the unassigned single hq isoforms into a single fasta file:
```{bash, eval=F}
      mkdir collected
      cd collected
      cat ../hq/*/cogent2.renamed.fasta ../unassigned.fasta > cogent_fake_genome.fasta
```

Redundant isoforms were collapsed:

Create a SAM alignment with minimap2 (This could also be done with GMAP).  
Obtain a final set of unique (non-redundant) transcript Isoforms that can be used as a reference gene set.  
As there is natural 5' degradation in RNA some sequences will represent identical isoforms which may not all be identified in clustering.  
These parameters are default or suggested in the Cogent tutorial page.  

Create an aligned, sorted SAM file:
```{bash, eval=F}
      export PATH=$PATH:/mnt/Prog/minimap2
      minimap2 -ax splice -t 30  -uf --secondary=no cogent_fake_genome.fasta ../../2019-04-30_out/polished.hq.fasta > hq.fasta.sam
```

Output:
```{bash, eval=F}
      [M::mm_idx_gen::2.392*1.00] collected minimizers
      [M::mm_idx_gen::3.526*1.32] sorted minimizers
      [M::main::3.528*1.32] loaded/built the index for 9960 target sequence(s)
      [M::mm_mapopt_update::3.708*1.30] mid_occ = 31
      [M::mm_idx_stat] kmer size: 15; skip: 5; is_hpc: 0; #seq: 9960
      [M::mm_idx_stat::3.810*1.29] distinct minimizers: 6267388 (82.12% are singletons); average occurrences: 1.346; average spacing: 2.846
      [M::worker_pipeline::13.901*5.65] mapped 25117 sequences
      [M::main] Version: 2.11-r797
      [M::main] CMD: /mnt/Prog/pacbio/smrtlink/install/smrtlink-release_6.0.0.47841/bundles/smrttools/ install/smrttools-release_6.0.0.47835/private/thirdparty/minimap2/minimap2_2.11/binwrap/../../../../../ private/thirdparty/minimap2/minimap2_2.11/bin/minimap2 -ax splice -t 30 -uf --secondary=no cogent_fake_genome.fasta ../../2019-04-30_out/polished.hq.fasta
      [M::main] Real time: 13.942 sec; CPU: 78.558 sec
```

Then follow the collapse tutorial from [Cupcake](https://github.com/Magdoll/cDNA_Cupcake/wiki/Cupcake-ToFU:-supporting-scripts-for-Iso-Seq-after-clustering-step)  
There is another cupcake page with information but the example scripts are provided on the cogent tutorial page.
      
Sort the SAM file:
```{bash, eval=F}
      sort -k 3,3 -k 4,4n hq.fasta.sam > hq.fasta.sorted.sam
```

Colapse identical isoforms to obtain a list of full-length, unique, hq isoforms to use as reference transcripts:
```{bash, eval=F}
      cd /mnt/Prog/cDNA_Cupcake/sequence
      which collapse_isoforms_by_sam.py
      /home/ubuntu/miniconda2/envs/anaCogent/bin/collapse_isoforms_by_sam.py
      
      cd /mnt/IsoSeq-analysis/data/Cogent/collected
```
Usage is:
```{bash, eval=F}
      # usage: collapse_isoforms_by_sam.py [-h] [--input INPUT] [--fq] -s SAM -o
      #                                    PREFIX [-c MIN_ALN_COVERAGE]
      #                                    [-i MIN_ALN_IDENTITY]
      #                                    [--max_fuzzy_junction MAX_FUZZY_JUNCTION]
      #                                    [--flnc_coverage FLNC_COVERAGE]
      #                                    [--dun-merge-5-shorter]
```

Collapse:
```{bash, eval=F}
      collapse_isoforms_by_sam.py --input ../../2019-04-30_out/polished.hq.fasta -s hq.fasta.sorted.sam -c 0.94 -i 0.85 --dun-merge-5-shorter -o hq.fasta.no5merge
```

These parameters taken from pers. comm. Tessa Bradford, SA Museum.  
Output is the name 'stem' the collapsed.rep.fa is for annotation.  
The output files are \<-o\>.collapsed.gff, \<-o\>.collapsed.rep.fq, and \<-o\>.collapsed.group.txt 
i.e. hq.fasta.no5merge.collapsed.gff, hq.fasta.no5merge.collapsed.rep.fq, and hq.fasta.no5merge.collapsed.group.txt  

The naming system for the post-collapse isoform is PB.\<loci_index\>.\<isoform_index\>

More terminal output from the above has been excluded. This is an example:
```{bash, eval=F}
      Ignored IDs written to: hq.fasta.no5merge.ignored_ids.txt
      Output written to:
      hq.fasta.no5merge.collapsed.gff
      hq.fasta.no5merge.collapsed.group.txt
      hq.fasta.no5merge.collapsed.rep.fa
      Namespace(allow_extra_5exon=False, flnc_coverage=-1, fq=False, input='../../2019-04-30_out/polished.hq.fasta', max_3_diff=100, max_5_diff=1000, max_fuzzy_junction=5, min_aln_coverage=0.94, min_aln_identity=0.85, prefix='hq.fasta.no5merge', sam='hq.fasta.sorted.sam')
```

Check outputs:
```{bash, eval=F}
      ls
      
      cogent_fake_genome.fasta                       hq.fasta.no5merge.collapsed.rep.fa
      hq.fasta.no5merge.collapsed.gff                hq.fasta.no5merge.ignored_ids.txt
      hq.fasta.no5merge.collapsed.gff.unfuzzy        hq.fasta.sam
      hq.fasta.no5merge.collapsed.group.txt          hq.fasta.sorted.sam
      hq.fasta.no5merge.collapsed.group.txt.unfuzzy
```

The file hq.fasta.no5merge.collapsed.group.txt names the isoforms as PB.\<loci_index\>.\<isoform_index\> and lists the collapsed identical isoforms.  
Each 'locus' consists of a strand-specific locus with isoforms that overlap by at least 1 bp. So PB.11.1 and PB.11.2 means this locus has two isoforms.
      
Count the 'loci' (transcripts) found:
```{bash, eval=F}
      wc -l hq.fasta.no5merge.collapsed.group.txt
      15729 hq.fasta.no5merge.collapsed.group.txt
```

Write file statistics:
```{bash, eval=F}
      get_abundance_post_collapse.py hq.fasta.no5merge.collapsed /mnt/IsoSeq-analysis/data/2019-04-30_out/polished.cluster_report.csv

      WARNING: isoseq3 format detected. Output `length` column will be `NA`.

      Read stat file written to hq.fasta.no5merge.collapsed.read_stat.txt
      Abundance file written to hq.fasta.no5merge.collapsed.abundance.txt
```

Note: this is true, hq.fasta.no5merge.collapsed.read_stat.txt has no length information and a whole column of NA values, however the headers of hq.fasta.no5merge.collapsed.rep.fa do have length information. This is how IsoSeq3 format data is processed and does not affect other aspects of this script. 
      
Get count information from the abundance and group .txt files

Add the headers:
```{bash, eval=F}
      echo -e "pbid\tcount_fl" > output.collapsed.abundance.txt
```

Add the PB.loci_index.isoform_index information and number of isoforms:
```{bash, eval=F}
      paste <(awk '{print $1}' hq.fasta.no5merge.collapsed.group.txt) <(awk -F , '{print NF}' hq.fasta.no5merge.collapsed.group.txt) >> output.collapsed.abundance.txt
```

All these commands except reconstructContig_1.sh and TBreconstructContig.edit.sh are provided in the one document [Running-Cogent-2.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Running-Cogent-2.sh) on Github.  

The longest transcript for each set of isoforms is in hq.fasta.no5merge.collapsed.rep.fa. This file is used in the following step and also provides the full transcripts to be sub-set as a reference based on further clustering in Section 2.1.4.


\newpage

### ANGEL 

[ANGEL](https://github.com/PacificBiosciences/ANGEL): Robust Open Reading Frame prediction  

Line by line instructions for running ANGEL in order to predict open reading frame and peptide sequences of transcript Isoforms. Also outputs 5' and 3' untranslated regions and scores transcript completeness.  

This analysis was tested on an environment on the eRSA NECTAR research cloud, final data run was completed by Terry Bertozzi on a Dell PowerEdge server with 40 cores and 512G RAM due to memory limitations of the NECTAR allocation.  

All steps and script parameters are accurate here, however, this analysis was run on a different HPC machine due to memory constraints. Directories in these commands are consistent with input data and where output was then moved back to, and serve as an example only. All results were immediately placed back in ~IsoSeq-analysis/data/ANGEL
```{bash, eval=F}
      #The python dependencies for ANGEL are:
      
      numpy
      Biopython
      scikit-learn
      CD-HIT version 4.8.1
      conda install -n anaCogent scikit-learn

      mkdir /mnt/IsoSeq-analysis/data/ANGEL
      cd /mnt/IsoSeq-analysis/data/ANGEL
```


##### **Dumb ORF prediction**
      
dumb_predict.py takes as input a FASTA file. It outputs the longest ORF in all frames - use to create a top training dataset.
      
Usage:
```{bash, eval=F}
      dumb_predict.py <fasta_filename> <output_prefix> 
             [--min_aa_length MIN_AA_LENGTH]
             [--use_firstORF]
             [--use_rev_strand] [--cpus CPUS]
```

By default, only the forward strand is used. This is especially true for PacBio transcriptome sequencing output.
```{bash, eval=F}
      dumb_predict.py /mnt/IsoSeq-analysis/data/Cogent/collected/hq.fasta.no5merge.collapsed.rep.fa pygmy.dumb --min_aa_length 100 --cpus 24
      
      # ------
      13335  finished       7611  clusters
```


##### **Creating a non-redundant training dataset**

ANGEL classifier training:
```{bash, eval=F}
      angel_make_training_set.py pygmy.dumb.final pygmy.dumb.final.training --random --cpus 24

      angel_train.py pygmy.dumb.final.training.cds pygmy.dumb.final.training.utr pygmy.dumb.final.classifier.pickle --cpus 12
```


##### **Robust ORF prediction**

Based on both the ANGEL Classifier training and the dumb ORF prediction.

Sequences are output as fasta files with  whether they are complete, 5' partial, 3' partial, or internal in the header information, which also includes the aa length.  
Preidctions are tagged as confident, likely, or suspicious, and dumb ORF predictions as dumb.  
The proportion of each isoform that is untranslated, as well as the position of the cds sequence is also output.  
```{bash, eval=F}
      angel_predict.py ../Cogent/collected/hq.fasta.no5merge.collapsed.rep.fa pygmy.dumb.final.classifier.pickle pygmy --output_mode=best --min_angel_aa_length 100 --min_dumb_aa_length 100 --cpus 48
```

Output is written to:  

 * pygmy.ANGEL.cds
 * pygmy.ANGEL.pep
 * pygmy.ANGEL.utr  

These outputs are used further in (Section 3 below) Chapter 4 for additional clustering and BLASTx searches.  

Note: The reverse strand options within ANGEL are not necessary on Isoseq data.  

All these commands are provided in the one document [Running_ANGEL.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Running_ANGEL.sh) on Github.  


\newpage

### CD-HIT

Clustering of transcript isoforms. Notes and output recorded for CD-HIT analysis of peptide sequences output from ANGEL ORF prediction.
This analysis was conducted on the [CD-HIT web server](http://weizhong-lab.ucsd.edu/cdhit-web-server/cgi-bin/index.cgi?cmd=Server%20home).  

CD-HIT was run by uploading pygmy.ANGEL.pep to the server and selecting a cut-off of 0.99.  
CD-HIT notes and recorded output are in [CDHit notes.pep99.bash](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%204%20-%20Annotation/CDHit%20notes.pep99.bash) and below.  
```{bash, eval=F}
      cd-hit - run on pygmy.ANGEL.pep
      
      separate run cut-off at .99
      
      ***
      Your job 1598936109 is finished.
      Program you ran: cd-hit
      You input file is pygmy.ANGEL.pep and we named it as 1598936109.fas.0
      Summary information for 1598936109.fas.0 included in 1598936109.fas.0.stat
      You required 1 runs for sequence clustering
           1. Fasta file for representative sequences at 99% identity is 1598936109.fas.1
               Summary information for 1598936109.fas.1 included in 1598936109.fas.1.stat
               Corresponding cluster file is1598936109.fas.1.clstr
               Sorted cluster file by size is 1598936109.fas.1.clstr.sorted
      Generated shell script is run-1598936109.sh
      
      faa_stat.pl 1598936109.fas.0
      faa_stat.pl 1598936109.fas.1
      /data5/data/NGS-ann-project/apps/cd-hit/clstr_sort_by.pl no < 1598936109.fas.1.clstr > 1598936109.fas.1.clstr.sorted
      /data5/data/NGS-ann-project/apps/cd-hit/clstr_list.pl 1598936109.fas.1.clstr 1598936109.clstr.dump
      gnuplot1.pl < 1598936109.fas.1.clstr > 1598936109.fas.1.clstr.1; gnuplot2.pl 1598936109.fas.1.clstr.1 1598936109.fas.1.clstr.1.png
      /data5/data/NGS-ann-project/apps/cd-hit/clstr_list_sort.pl 1598936109.clstr.dump 1598936109.clstr_no.dump
      /data5/data/NGS-ann-project/apps/cd-hit/clstr_list_sort.pl 1598936109.clstr.dump 1598936109.clstr_len.dump len
      /data5/data/NGS-ann-project/apps/cd-hit/clstr_list_sort.pl 1598936109.clstr.dump 1598936109.clstr_des.dump des
```

The generated CD-HIT script is uploaded to github as [run-1598936109.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%204%20-%20Annotation/run-1598936109.sh)

The full output metadata file is uploaded to github as [1598936109.out](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%204%20-%20Annotation/1598936109.out) and the summary data provided below:
```{bash, eval=F}
      ================================================================
      Program: CD-HIT, V4.7 (+OpenMP), Sep 06 2018, 09:36:42
      Command: /data5/data/NGS-ann-project/apps/cd-hit/cd-hit -i
               1598936109.fas.0 -d 0 -o 1598936109.fas.1 -c 0.99 -n 5
               -G 1 -g 1 -b 20 -l 10 -s 0.0 -aL 0.0 -aS 0.0 -T 4 -M
               32000
      
      Started: Mon Aug 31 21:55:20 2020
      ================================================================
                                  Output                              
      ----------------------------------------------------------------
      total seq: 13882
      longest and shortest : 2513 and 99
      Total letters: 4952779
      Sequences have been sorted

      ...
      

      13882  finished       9907  clusters
```

The output 1598936109.fas.1.clstr.sorted contains the sorted cluster groups generated by CD-Hit and indicates the longest representative transcript for each cluster.


### Reading CD-HIT output in R

dplyr was used to manipulate the CD-Hit output to create a list of unique transcript IDs of the longest representative transcript for each cluster. This list file is required in the following step to subset from the full-length transcript file which contains the untranslated regions.  

Load libraries and set the working directory
```{r, warning=FALSE, Message=FALSE, include=FALSE, results='hide'}
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters")
```

Import 1598936109.fas.1.clstr.sorted, the CD-Hit output file with all transcript IDs assigned to a sorted cluster (note: ./ReferenceClusters/ has been continually included due to setwd errors).
```{r, warning=FALSE, Message=FALSE, echo = TRUE, results='hide'}
clstr <- read.csv("./ReferenceClusters/1598936109.fas.1.clstr.sorted", sep = '\t', row.names = NULL, header = FALSE, stringsAsFactors = FALSE)
#head(clstr)
```

Initial file had a blank line with "\>Cluster #" in between new clusters. Extend the \>Cluster # down the column so that all transcripts are directly associated with a cluster number in their row.
```{r, warning=FALSE, Message=FALSE, echo = TRUE, results='hide'}
clstr2 <- clstr
n = nrow(clstr)
x = 0
numbers_only <- function(x) !grepl("\\D", x)
for (row in c(1:n)) {
  if (numbers_only(clstr2[row,1]) == TRUE) {
    clstr2[row,1] <- x}
  else {NULL}
  x <- clstr2[row,1]
}
#head(clstr2,20)
```

Count the number of transcript members for each cluster.
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
clstr.sums <- data.frame(dplyr::count(clstr2,V1))
write.csv(clstr.sums, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/clstr.sums.csv", quote = FALSE, row.names = FALSE, col.names = FALSE)
head(clstr.sums)
```

Remove the additional blank row without a transcript name from between each cluster group.
```{r, warning=FALSE, Message=FALSE, echo = TRUE, results='hide'}
switch <- clstr.sums[1,2]
clstr3 <- cbind(clstr2[1], clstr)
clstr3[c((switch-5):(switch+5)),]
```

```{r, warning=FALSE, Message=FALSE, echo = TRUE, results='hide'}
clstr4 <- clstr2[-which(clstr2$V2 == ""), ]
clstr4[c(1:5,(switch-5):(switch+5)),]
```

Separate CD-Hit information into columns by delimiters:  

 - Remove \> symbol from transcript names
 - Move number of amino acids into a new column by separating "aa"
 - Move statistics of % match into a new column byt separating "... "
 - Give the columns headers  
 
```{r, warning=FALSE, Message=FALSE, echo = TRUE, results='hide'}
clstr5 <- clstr4
clstr5[] <- lapply(clstr5, gsub, pattern='>', replacement='')
clstr5.2 <- data.frame(str_split_fixed(clstr5$V2, "aa, ", 2))
clstr5.3 <- data.frame(str_split_fixed(clstr5.2$X2, "... ", 2))
clstr6 <- cbind(clstr5[1],clstr5.2[1],clstr5.3[1:2])
colnames(clstr6) <- c("cluster","aa","TranscriptID","stat")
#head(clstr6)
```


Filter based on rows that have a "*" in the stat column. These indicate the representative sequences for each cluster determined by CD-Hit and will act as the main reference.  

```{r, warning=FALSE, Message=FALSE, echo = TRUE, results='hide'}
clstr7 <- filter(clstr6, stat == "*")
head(clstr7, 50)
```

Pull out the column "TranscriptID" and export into a txt file so it can be used as a list to extract these sequences.  
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
clstrTranscriptID <- pull(clstr7, var = TranscriptID)

write.csv(clstrTranscriptID, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/clstrTranscriptID.csv", quote = FALSE, row.names = FALSE, col.names = FALSE)
head(clstrTranscriptID)
```

Pull out the first segment before the first \| from column "TranscriptID" and export into a txt file. this reduced the ID to the initial PB.\<loci_index\>.\<isoform_index\> identifier.

```{r, warning=FALSE, Message=FALSE, echo = TRUE}
WholeTranscriptID <- word(clstrTranscriptID,1,sep = "\\|")
write.csv(WholeTranscriptID, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/WholeTranscriptID.csv", quote = FALSE, row.names = FALSE, col.names = FALSE)
head(WholeTranscriptID)
```

Note there will be "duplicates" if the name is truncated all the way back to the PB.\<loci_index\>.\<isoform_index\> identifier due to multiple isoforms that are retained. A longer, but still truncated version of this ID (excluding the information appended by ANGEL), was required to subset from the hq.fasta.no5merge.collapsed.rep.fa file.  


The clstrTranscriptID.csv is now the full list of representative transcripts based on clustering of the translated proteins in predicted read frame. This file was then used in Seqtk on the NECTAR cloud to extract a .fasta of all the reference transcripts representative of these putative gene clusters.  



### Seqtk

Seqtk was used on the NECTAR machine to manipulate the fasta files and subset based on list files of transcript names.

Installation:
```{bash, eval=F}
      #Carmel July 2020
      
      #Used on NECTAR research cloud
      
      #Relies on hard-coded paths & filenames
      #run line by line in shell
      
      ----------------
      
      cd /mnt/Prog
      conda install -c bioconda seqtk
      
      #___
      The following packages will be downloaded:
      
          package                    |            build
          ---------------------------|-----------------
          certifi-2020.6.20          |   py37hc8dfbb8_0         151 KB  conda-forge
          conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge
          openssl-1.1.1g             |       h516909a_1         2.1 MB  conda-forge
          python_abi-3.7             |          1_cp37m           4 KB  conda-forge
          seqtk-1.3                  |       hed695b0_2          39 KB  bioconda
          ------------------------------------------------------------
                                                 Total:         5.3 MB
      
      The following NEW packages will be INSTALLED:
      
        python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m
        seqtk              bioconda/linux-64::seqtk-1.3-hed695b0_2
      #___
      
      Conda packages are located in /mnt/Prog/miniconda3/bin/ if you dont set a path
      ----------------
```

Manually convert clstrTranscriptID.csv to an clstrTranscriptID.lst file.  

Seqtk was used to subset the pygmy.ANGEL.cds fasta file based on the R cluster list outputs:
Moved files into /mnt/IsoSeq-analysis/data/Seqtk
```{bash, eval=F}      
      screen        #to be safe
      cd /mnt/IsoSeq-analysis/data/Seqtk
      /mnt/Prog/miniconda3/bin/seqtk subseq ../ANGEL/pygmy.ANGEL.cds clstrTranscriptID.lst > clstrTranscriptID.fa
```

check:
```{bash, eval=F}
      wc -l clstrTranscriptID.lst 
      # 9907
      wc -l clstrTranscriptID.fa
      # 19814
      
      # 19814/2 = 9907. So it is correct
```

The hq.fasta.no5merge.collapsed.rep.fa file created in Chapter 3 (Section 2.1.2 above) is a fasta file where sequence information ran over multiple "lines" within the file.  

Seqtk was used to convert this file into a .fasta with sequence on a single line. 
```{bash, eval=F}
      17/11/2020
      # just converting hq.fasta.no5merge.collapsed.rep.fa into a fasta with sequence all on one line
      # command seq is basic fasta/q conversion
      
      cd /mnt/IsoSeq-analysis/data/Seqtk
      /mnt/Prog/miniconda3/bin/seqtk seq ../Cogent/collected/hq.fasta.no5merge.collapsed.rep.fa > hq.fasta.no5merge.collapsed.rep.1L.fa
```

Checked that the file still has the correct number of sequences:
```{bash, eval=F}
      $ grep -c ">" hq.fasta.no5merge.collapsed.rep.fa
      15729
      
      grep -c ">" hq.fasta.no5merge.collapsed.rep.1L.fa
      15729
```

Seqtk was also used to search for transcripts which returned BLAST results for some genes of interest as data exploration. 
The reference list files for this were extracted from the BLASTx output were created using find and sort functions in notepad++ and excel, and then applied to the pygmy.ANGEL.cds fasta file to extract sequences.
```{bash, eval=F}
      #Usage from /mnt/IsoSeq-analysis/data/ANGEL
      
      cd /mnt/IsoSeq-analysis/data/Seqtk
      /mnt/Prog/miniconda3/bin/seqtk subseq ../ANGEL/pygmy.ANGEL.cds AQP.lst > AQP.fa
      
      /mnt/Prog/miniconda3/bin/seqtk subseq ../ANGEL/pygmy.ANGEL.cds HSP.lst > HSP.fa
      
      /mnt/Prog/miniconda3/bin/seqtk subseq ../ANGEL/pygmy.ANGEL.cds SLC.lst > SLC.fa
      
      /mnt/Prog/miniconda3/bin/seqtk subseq ../ANGEL/pygmy.ANGEL.cds MAPK.lst > MAPK.fa
```


### Subsetting Full-length Transcripts Which Represent each Putative Gene 

Creation of a subset of the longest representative transcripts for each cluster as per CD-HIT and the previous two to form the reference for counts in the following Chapter 5 (Section 4 below).  

As only the peptide coding sequence of transcripts was used in the BLASTx search and the CD-HIT clustering in order to cluster similarly expressed transcripts, the untranslated regions of transcripts are absent. Our goal is to include these untranslated regions in the reference to be used for gene expression counts.  
Rather than attempting to piece pygmy.ANGEL.cds and pygmy.ANGEL.utr back together, a truncated version of the transcript names was used to pull the full length unaltered transcript out of the earlier hq.fasta.no5merge.collapsed.rep.fa file. 

Initial checks of clstrTranscriptID.csv were carried out. 
Files are continually checked for transcript ID or sequence counts to ensure manual manipulation did not remove any data that was not being targeted. 
Exploration of discrepancies below identified a number of transcripts that with clustering of multiple isoforms, have seeded more than one cluster. This means that there were duplicates in the list file once coding region identifiers were removed and sequences had to be extracted manually and not by applying the clstrTranscriptID list to hq.fasta.no5merge.collapsed.rep.1L.fa in seqtk.  

The following was done on a local Intel core i7 machine using RStudio or in-house bash scripts applied in Git Bash for Windows. Some later steps were carried out on the NECTAR allocation.  

All files copied to "~/IsoSeq-analysis/data/Seqtk/reference COPIES 2020-11" and contents checked

The following [file](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%204%20-%20Annotation/Subset-Reference-Clustered_Following_terry_analysis_add_perl-trim.bash) contains all of the below:
```{bash, eval=F}
      Carmel@CarmelASUS MINGW64 ~
      cd "E:/@DATA/@@PBT_KidneyIsoSeq_Working Data_C.Maher/IsoSeq-analysis/data/Seqtk/reference COPIES 2020-11"
      
      2020-11-17
      
      cd "C:\Users\Carmel\Desktop\reference COPIES 2020-11"
      
      -----
      
      19814 clstrTranscriptID.fa					/2 = 9907
      9907 clstrTranscriptID.lst
      
      hq.fasta.no5merge.collapsed.rep.fa
      hq.fasta.no5merge.collapsed.rep.1L.fa
      pygmy.ANGEL.cds
      pygmy.ANGEL.pep
      pygmy.ANGEL.utr
```

Sort and filter clstrTranscriptID.lst by unique values to make sure there are no dupicates
```{bash, eval=F}
      sort clstrTranscriptID.lst | uniq > clstrUniq.txt
      wc -l clstrUniq.txt
      9907 clstrUniq.txt
```

List duplicates if there are any found (there were not)
```{bash, eval=F}
      sort clstrTranscriptID.lst | uniq -d > clstrUniqD.txt
      wc -l clstrUniqD.txt
      0 clstrUniqD.txt
```


\-\-\-
```{bash, eval=F}
      Carmel@CarmelASUS MINGW64 /e/@DATA/@@PBT_KidneyIsoSeq_Working Data_C.Maher/IsoSeq-analysis/data/Seqtk/reference COPIES 2020-11
```

\-\-\-\> **1. Isoseq HQ output file: hq.fasta.no5merge.collapsed.rep.fa**

header format
```{bash, eval=F}
      head -n 2 hq.fasta.no5merge.collapsed.rep.fa
      >PB.1.1|000643|path0:1-1879(+)|transcript/14742 transcript/14742 full_length_coverage=3;length=1887;num_subreads=52
      ATAGAGTACAAGTACTAGAGTTTTAGTGTGGTCGAAGAGAAATCCAAGATCCACCATGGG
```      
no of sequences
```{bash, eval=F}
      grep -c ">" hq.fasta.no5merge.collapsed.rep.fa
      15729
```

\newpage

\-\-\-\> **2. Angel CDS output from Isoseq HQ file: pygmy.ANGEL.cds**
      
header format
```{bash, eval=F}
      head -n 2 pygmy.ANGEL.cds
      >PB.2.1|002537|path0:1-1624(+)|transcript/18304|m.1 type:likely-NA len:135 strand:+ pos:282-686
      ATGTCCTACTGCAGGCAAGAAGGAAAGGACAGAATTATATTTGTGACAAAAGAGGACCATGAAACTCCAAGTAGTGCTGAGTTAGTAGCGGATGACCCCAATGATCCTTATGAAGATCAAGGTTTGATATTGCCTAATGGAGATATCAATTGGAATTGTCCTTGTCTCGGTGGAATGGCTAGTGGCCCTTGTGGGGAACAATTCAAATCGGCCTTTTCCTGTTTCCATTATAGCAAGGAAGAAATAAAGGGATCAGATTGTGTGGACCAGTTTCGTGCAATGCAAGAGTGCATGCAGAAGTACCCAGATCTATACCCCCAAGAGGAAGATGATGAAGAAGAAAAGCAGAGCAAAGAATTAGAAACAGCTCCTTCAGTAGCCAAAGAGGAAGAGGGATCTAGCTAA
```

Note Sequence is on a single line
      
no of sequences
```{bash, eval=F}
      grep -c ">" pygmy.ANGEL.cds
      13882
```

      
\-\-\-\> Angel CDS file clusterd with CD-HIT: 1598936109.fas.1.clstr 
      
header format
```{bash, eval=F}
      cd cd-hit\ 99
      head 1598936109.fas.1.clstr
      >Cluster 0
      0       2513aa, >PB.715.4|1b393a|path6:6-8031(+)|transcript/8|m.1216... *
      1       2250aa, >PB.715.5|1b393a|path6:278-8030(+)|transcript/13|m.1217... at 99.56%
      2       2369aa, >PB.715.6|1b393a|path6:524-8029(+)|transcript/20|m.1218... at 99.58%
      3       2148aa, >PB.715.7|1b393a|path6:1187-8030(+)|transcript/49|m.1219... at 99.53%
      4       1913aa, >PB.715.9|1b393a|path6:1893-8028(+)|transcript/118|m.1221... at 99.79%
      5       1688aa, >PB.715.11|1b393a|path6:2596-8054(+)|transcript/279|m.1223... at 99.59%
      6       1452aa, >PB.715.14|1b393a|path6:3304-8029(+)|transcript/591|m.1226... at 100.00%
      7       1333aa, >PB.715.15|1b393a|path6:3629-8031(+)|transcript/935|m.1227... at 99.77%
      8       1314aa, >PB.715.16|1b393a|path6:3916-8078(+)|transcript/962|m.1228... at 100.00%
```

File contains truncated headers from the Angel CDS file, clustered acoording to similarity. Truncation appears to be at the first whitespace
no of sequences
```{bash, eval=F}
      grep -c ', >' 1598936109.fas.1.clstr     #search pattern changed to avoid the ">" in front of each cluster label eg >Cluster 0
      13882
```

Sequence number matches Angel CDS file

\newpage
      
no of clusters
```{bash, eval=F}
      grep -c '>C' 1598936109.fas.1.clstr
      9907
```

```{bash, eval=F}
      grep -c '>C' 1598936109.fas.1.clstr.sorted
      9907
```
both match
```{bash, eval=F}      
      tail 1598936109.fas.1.clstr
      >Cluster 9902
      0       99aa, >PB.8346.1|transcript/22300:1-1143(+)|transcript/22300|m.14112... *
      >Cluster 9903
      0       99aa, >PB.8372.1|transcript/22508:1-1103(+)|transcript/22508|m.14137... *
      >Cluster 9904
      0       99aa, >PB.8489.1|transcript/23315:1-855(+)|transcript/23315|m.14244... *
      >Cluster 9905
      0       99aa, >PB.8706.1|transcript/25054:1-372(+)|transcript/25054|m.14469... *
      >Cluster 9906
      0       99aa, >PB.9028.1|transcript/4242:1-2955(+)|transcript/4242|m.14776... *
```

```{bash, eval=F}
      tail 1598936109.fas.1.clstr.sorted
      >Cluster 9902
      0       99aa, >PB.8346.1|transcript/22300:1-1143(+)|transcript/22300|m.14112... *
      >Cluster 9903
      0       99aa, >PB.8372.1|transcript/22508:1-1103(+)|transcript/22508|m.14137... *
      >Cluster 9904
      0       99aa, >PB.8489.1|transcript/23315:1-855(+)|transcript/23315|m.14244... *
      >Cluster 9905
      0       99aa, >PB.8706.1|transcript/25054:1-372(+)|transcript/25054|m.14469... *
      >Cluster 9906
      0       99aa, >PB.9028.1|transcript/4242:1-2955(+)|transcript/4242|m.14776... *
```      
Cluster numbers also match both files:Cluster 9906 + Cluster 0 = 9907
      
Check if the header changed from Isoseq HQ after processing with Angel?
```{bash, eval=F}
      cd ../
      grep '>PB.715.4|1b393a|path6:6-8031(+)|transcript/8' hq.fasta.no5merge.collapsed.rep.fa
      >PB.715.4|1b393a|path6:6-8031(+)|transcript/8 transcript/8 full_length_coverage=43;length=7837;num_subreads=60
```

note: the "|m.xxxxx" suffix in the ANGEL output file is added at the first whitespace
      
      
\-\-\-\> **3. The longest seed sequence from the CD-HIT output: clstrUniq.txt**
      
header format
```{bash, eval=F}
      head clstrUniq.txt
      PB.10.1|004815|path1:5-1713(+)|transcript/17534|m.3
      PB.100.1|049515|path5:1-1563(+)|transcript/19243|m.132
      PB.1000.1|26d4b2|path0:1-2889(+)|transcript/4521|m.1797
      PB.1000.2|26d4b2|path0:1287-2889(+)|transcript/18232|m.1798
      PB.1002.1|26ea71|path2:1-2673(+)|transcript/6271|m.1799
      PB.1002.2|26ea71|path2:11-2691(+)|transcript/8089|m.1800
      PB.1003.1|26fc93|path2:1-3987(+)|transcript/1197|m.1801
      PB.1005.1|2718e2|path0:1-5294(+)|transcript/258|m.1804
      PB.1006.1|272202|path0:1-2713(+)|transcript/5814|m.1805
      PB.1007.1|272202|path1:1-3370(+)|transcript/2554|m.1806
```

Prefix ">" and suffix "... *" have been removed in R Studio
      
no of strings
```{bash, eval=F}
      grep -c 'PB.' clstrUniq.txt
      9907
```

matches number of clusters above
      
      
strip the "|m.xxxx" from the strings in clstrUniq.txt 
```{bash, eval=F}
      sed 's/|m.*//' clstrUniq.txt > new_cluster.txt
```

check
```{bash, eval=F}
      head new_cluster.txt
      PB.10.1|004815|path1:5-1713(+)|transcript/17534
      PB.100.1|049515|path5:1-1563(+)|transcript/19243
      PB.1000.1|26d4b2|path0:1-2889(+)|transcript/4521
      PB.1000.2|26d4b2|path0:1287-2889(+)|transcript/18232
      PB.1002.1|26ea71|path2:1-2673(+)|transcript/6271
      PB.1002.2|26ea71|path2:11-2691(+)|transcript/8089
      PB.1003.1|26fc93|path2:1-3987(+)|transcript/1197
      PB.1005.1|2718e2|path0:1-5294(+)|transcript/258
      PB.1006.1|272202|path0:1-2713(+)|transcript/5814
      PB.1007.1|272202|path1:1-3370(+)|transcript/2554
```

```{bash, eval=F}
      grep -c 'PB.' new_cluster.txt
      9907
```

How many of thes strings are in the original ONE LINE isoseq file?
```{bash, eval=F}
      grep -c -Ff new_cluster.txt hq.fasta.no5merge.collapsed.rep.1L.fa 
      9813
```

94 strings are missing
      
      
\-\-\-\> **4. find the missing strings**

convert the isoform hq file into the same format as new_cluster.txt
```{bash, eval=F}
      grep '>' hq.fasta.no5merge.collapsed.rep.1L.fa| sed s'/ tra.*//' | sed s'/>//' > hq_headers 
      head hq_headers 
      PB.1.1|000643|path0:1-1879(+)|transcript/14742
      PB.2.1|002537|path0:1-1624(+)|transcript/18304
      PB.3.2|00361c|path0:43-2240(+)|transcript/10564
      PB.4.1|004079|path1:6-745(+)|transcript/23781
      PB.4.2|004079|path1:6-612(+)|transcript/24311
      PB.5.1|004079|path2:209-642(+)|transcript/24981
      PB.6.1|004079|path3:273-616(+)|transcript/25193
      PB.7.1|004079|path5:6-664(+)|transcript/24242
      PB.8.1|004079|path6:1-607(+)|transcript/24477
      PB.9.1|004079|path7:1-725(+)|transcript/23873
```

```{bash, eval=F}
      cat hq_headers |wc -l
      15729
```

Checking for the same number of matches
```{bash, eval=F}
      grep -Ff new_cluster.txt hq_headers > matching_strings
      cat matching_strings |wc -l
      9813
```

Checking for duplicates
```{bash, eval=F}
      cat hq_headers | sort |uniq -cd
      0
      
      cat new_cluster.txt | sort |uniq -cd
            2 PB.1206.1|2e8181|path5:1-4617(+)|transcript/551
            2 PB.1261.1|30ba06|path3:633-1922(+)|transcript/21324
            2 PB.1570.2|3bd5a8|path2:20-1934(+)|transcript/13977
            2 PB.1719.1|404d72|path0:1-3429(+)|transcript/2407
            2 PB.1764.3|429969|path8:44-2062(+)|transcript/12739
            2 PB.1813.1|446aed|path0:1-2871(+)|transcript/4320
            2 PB.1865.1|46bd4e|path0:1-2536(+)|transcript/6726
            2 PB.1869.1|46d663|path2:1-1320(+)|transcript/21375
            2 PB.220.1|08884f|path0:1-4120(+)|transcript/991
            2 PB.2360.1|5843a6|path32:1-2062(+)|transcript/11326
            2 PB.2541.3|5e93d1|path0:5-2495(+)|transcript/7381
            2 PB.2560.1|5f09e2|path0:1-1196(+)|transcript/21957
            2 PB.2618.1|612418|path1:1-2013(+)|transcript/12200
            2 PB.2713.3|65abaa|path0:37-2495(+)|transcript/7693
            2 PB.2892.1|6b359a|path12:1-4597(+)|transcript/526
            2 PB.3037.2|702e9b|path5:2-3018(+)|transcript/3923
            2 PB.3243.1|784f79|path0:1-2683(+)|transcript/5903
            2 PB.3259.1|792211|path3:1-5233(+)|transcript/268
            2 PB.3286.3|7a2645|path3:103-3087(+)|transcript/3780
            2 PB.3405.1|7f2f96|path0:5-2081(+)|transcript/11903
            3 PB.3430.1|8055ae|path8:1-3589(+)|transcript/2043
            2 PB.3461.1|81f072|path5:1-2492(+)|transcript/7285
            2 PB.351.4|0cccb8|path5:27-4293(+)|transcript/775
            2 PB.3562.1|864d1b|path0:1-3492(+)|transcript/2174
            2 PB.3615.1|883e51|path3:1-4044(+)|transcript/1347
            2 PB.3836.3|8ff0e9|path0:4-3972(+)|transcript/946
            2 PB.3934.1|93b547|path0:1-3353(+)|transcript/2676
            2 PB.4042.2|97fba8|path2:34-2096(+)|transcript/13603
            2 PB.4219.2|9d8e09|path3:9-3566(+)|transcript/2045
            2 PB.4442.1|a6620e|path0:1-2906(+)|transcript/4555
            2 PB.4458.1|a6e2a5|path0:1-3673(+)|transcript/1739
            2 PB.4822.1|b4950f|path1:1-3315(+)|transcript/2177
            2 PB.4822.2|b4950f|path1:1240-3315(+)|transcript/10982
            2 PB.4919.1|b7da5b|path0:1-3327(+)|transcript/2652
            2 PB.4923.1|b7fed9|path0:1-2394(+)|transcript/8643
            2 PB.5080.1|bde542|path3:1-3060(+)|transcript/3777
            2 PB.5096.2|be8c26|path4:70-2754(+)|transcript/5782
            2 PB.5149.1|c15864|path0:1-1628(+)|transcript/17934
            2 PB.5308.1|c8b5b5|path2:35-3805(+)|transcript/1487
            2 PB.5395.1|cbd93f|path0:1-1699(+)|transcript/17107
            2 PB.5483.2|cf1828|path1:23-2344(+)|transcript/9126
            2 PB.5619.1|d4d2ea|path0:1-2677(+)|transcript/6673
            2 PB.5663.1|d5e71b|path0:1-5101(+)|transcript/303
            2 PB.5939.1|e23f0e|path0:1-2960(+)|transcript/3907
            2 PB.5939.2|e23f0e|path0:5-4542(+)|transcript/622
            2 PB.613.1|16d05c|path0:1-1558(+)|transcript/18673
            2 PB.6133.1|ea032e|path9:1-2052(+)|transcript/12270
            2 PB.6276.1|ee992b|path0:1-1487(+)|transcript/19762
            2 PB.6283.1|eee48e|path1:1-3454(+)|transcript/2331
            2 PB.6286.1|ef0b10|path12:1-2744(+)|transcript/5222
            2 PB.6303.1|efc514|path0:1-3222(+)|transcript/2889
            2 PB.6378.3|f31894|path9:80-2754(+)|transcript/5975
            2 PB.6385.1|f32c31|path0:1-1500(+)|transcript/12840
            2 PB.6426.1|f4ded6|path0:1-1796(+)|transcript/16785
            2 PB.6576.2|facceb|path2:1-3314(+)|transcript/3157
            2 PB.659.1|19437d|path1:1-3483(+)|transcript/2210
            2 PB.6763.1|transcript/10186:1-2230(+)|transcript/10186
            2 PB.6877.1|transcript/1099:1-4113(+)|transcript/1099
            2 PB.6966.1|transcript/11636:1-2103(+)|transcript/11636
            2 PB.713.1|1b043a|path0:1-965(+)|transcript/23154
            2 PB.7227.1|transcript/138:1-5869(+)|transcript/138
            2 PB.7287.1|transcript/14227:1-1898(+)|transcript/14227
            3 PB.7342.1|transcript/14710:1-1879(+)|transcript/14710
            2 PB.7349.1|transcript/14730:1-1903(+)|transcript/14730
            2 PB.7353.1|transcript/14772:1-1860(+)|transcript/14772
            2 PB.74.1|02ebe4|path0:1-2606(+)|transcript/6537
            2 PB.7501.1|transcript/1596:1-3759(+)|transcript/1596
            2 PB.7528.1|transcript/16151:1-1797(+)|transcript/16151
            2 PB.7678.1|transcript/1728:1-3672(+)|transcript/1728
            2 PB.7707.1|transcript/17496:1-1683(+)|transcript/17496
            2 PB.7868.1|transcript/188:1-5558(+)|transcript/188
            2 PB.7973.1|transcript/19628:1-1476(+)|transcript/19628
            2 PB.8082.1|transcript/20433:1-1412(+)|transcript/20433
            2 PB.816.1|1e334e|path4:1-2040(+)|transcript/12316
            2 PB.8325.1|transcript/22158:1-1191(+)|transcript/22158
            2 PB.834.2|1ec273|path2:1291-2750(+)|transcript/19945
            2 PB.8389.1|transcript/22625:1-1060(+)|transcript/22625
            2 PB.8709.1|transcript/252:1-5299(+)|transcript/252
            2 PB.8883.1|transcript/3393:1-3152(+)|transcript/3393
            2 PB.8886.1|transcript/3402:1-3146(+)|transcript/3402
            2 PB.8920.1|transcript/3588:1-3076(+)|transcript/3588
            2 PB.9068.1|transcript/4442:1-2918(+)|transcript/4442
            2 PB.9162.1|transcript/4983:1-2815(+)|transcript/4983
            2 PB.9209.1|transcript/5238:1-2771(+)|transcript/5238
            2 PB.924.1|22d3cf|path3:1-1494(+)|transcript/19025
            2 PB.9277.1|transcript/5650:1-2707(+)|transcript/5650
            2 PB.944.1|241290|path4:2-2629(+)|transcript/6557
            2 PB.9535.1|transcript/7343:1-2508(+)|transcript/7343
            2 PB.9581.1|transcript/7647:1-2463(+)|transcript/7647
            2 PB.9683.1|transcript/8355:1-2402(+)|transcript/8355
            2 PB.9766.1|transcript/9024:1-2324(+)|transcript/9024
            2 PB.9784.1|transcript/915:1-4197(+)|transcript/915
```

During the clustering it appears that several of the sequences have been made seed sequences (denoted with a *) for more than one cluster.

The counts above equate to how many strings which is the discrepancy noted above.
```{bash, eval=F}
      cat new_cluster.txt | sort |uniq -cd | cut -d' ' -f7 | awk '{sum+=$1-1} END{printf("%d\n",sum)}'
      94
```

Both hq.fasta.no5merge.collapsed.rep.fa and hq.fasta.no5merge.collapsed.rep.1L.fa. were checked in this way but the single line file is required in later steps so only those commands have been included.


Final pulling out of transcripts sequences into a .fasta on a single line, also removes the skipped lines that grep outputs as '\-\-'.  
note: NOT using seqtk anymore:
```{bash, eval=F}
      grep -Ff new_cluster.txt -A 1 hq.fasta.no5merge.collapsed.rep.1L.fa | grep -v ^-- > reference_transcripts.1Lv.fasta
      
      grep -c ">" reference_transcripts.1Lv.fasta
      9813
```

\newpage

\-\-\-\> **next step**
      
Persisting poly-a tails needed to be trimmed from the following file:
```{bash, eval=F}
      reference_transcripts.1Lv.fasta
```

Note: The Trim polya command in bbduk leaves many tails intact due to single non 'A' bases and does not facilitate hamming distance command at the same time, however specifying Ktrim=r literal or "trimming twice" (poly-a command and THEN ktrim) sequences become very short even with different K values. 
      
Terry Bertozzi provided perl script as below to manually trim:
```{bash, eval=F}
      # Terry Bertozzi
      #command line to remove poly A tails
      
      #for use on /mnt/IsoSeq-analysis/data/Seqtk/reference_transcripts.1Lv.fasta
      
      cd /mnt/IsoSeq-analysis/data/Seqtk
      
      perl -pe 's/(A{5,}.{0,2})+$//gm' reference_transcripts.1L.fasta > reference_transcripts.1L.clean.fasta
      
      perl -pe 's/(A{5,}.{0,2})+$//gm' reference_transcripts.1Lv.fasta > reference_transcripts.1Lv.clean.fasta
      
      grep -c ">" reference_transcripts.1Lv.clean.fasta
      9813
      
      head reference_transcripts.1Lv.clean.fasta
      #also looks good.
```

reference_transcripts.1Lv.clean.fasta - IS THE REFERENCE  

***
\newpage

## <ins>**_Short-read Data_**</ins>

_De-novo_ assembly of short-read Transcripts:  
Short-read transcripts used for gene expression analysis in Chater 6 were initially intended for _de-novo_ assembly and analysis due to a lack of closely related genomic references for _T. adelaidensis_.  

This section outlines the methods for assembly of short-read datasets in the absence of a reference or genome for the purposes of comparing these final transcript datasets to the assembly generated from the long-reads as above.  

Other notes:
Initial plans for Chapter 5 did not include the availability of long-read sequencing to supplement a reference transcript list. Therefore, a pooled transcript assembly was created by concatenating reads from all eight samples in order to use this larger dataset as a pseudo-reference for other programs.  
This longer assembly is included in the BUSCO analysis below to illustrate the difference in assembly completeness and duplication.

Sequences were downloaded from the Australian Genomics Research Facility server & checksumms verified.  
This analysis was conducted on the Flinders University 'Deep Thought' HPC.  


### FASTQC Quality Assessment

Quality of sequencing output was quantified using the following [script](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/FastQC-run.bash) and FastQC v0.11.8 (note there are two commands because there were two sequencing run data folders AGRF_CAGRF13871_HCGYYBCXY and AGRF_CAGRF20022_CDNJBANXX).  

```{bash, eval=F}
      !/bin/bash
      
      # November 2019
      # Carmel Maher
      # This script is to submit a FastQC analysis on raw reads through Flinders' DeepThought HPC Slurm
      # This script therefore assumes submission to Flinders Deepthought with an assumed directory structure and use of available modules
      
      
      #------------------------------------
      # Required Modules:
      # module add fastqc/0.11.8
      
      
      #------------------------------------
      # Terry's error exit
      
      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
      
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      #------------------------------------
      
      cd /scratch/user/mahe0050/DE-analysis/0_rawData/AGRF_CAGRF13871_HCGYYBCXY || error_exit "$LINENO: directory error 1"
      for file in *.fastq.gz
      do
      	FILESTEM=${file%.*}
      	echo $FILESTEM
      
      	fastqc $FILESTEM.gz --extract --outdir /scratch/user/mahe0050/DE-analysis/0_rawData/FastQC || error_exit "$LINENO: Error with FastQC at "$FILESTEM""
      done
      	
      cd /scratch/user/mahe0050/DE-analysis/0_rawData/AGRF_CAGRF20022_CDNJBANXX || error_exit "$LINENO: directory error 2"
      for file in *.fastq.gz
      do
      	FILESTEM=${file%.*}
      	echo $FILESTEM
      
      	fastqc $FILESTEM.gz --extract --outdir /scratch/user/mahe0050/DE-analysis/0_rawData/FastQC || error_exit "$LINENO: Error with FastQC at "$FILESTEM""
      done
```

The FastQC Reports were then combined for visualisation using [NGSReports](https://bioconductor.org/packages/release/bioc/html/ngsReports.html)
The full FastQC output summary .html file of all eight samples PRIOR to trimming is available [here](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/FastQC-8K_ngsReports_Fastqc-edit.html).

\newpage

![FastQC Summary of scores for paired reads of eight samples prior to any adapter or quality trimming](./FigureCopyDir/FastQC-8K-summary.png)

Samples G6 and G7 were run on a second sequencing run, and have different overall lengths. This is the main source of variation in sequencing scores.


### Adapter Trimming

Trimming to remove Illumina TruSeq adapters and SMARTer PCR tags as well as trimming based on quality scores was done using Cutadapt version 4.1,

This was set up on the Flinders Deep thought machine as below
```{bash, eval=F}
      module load Miniconda3/4.9.2
      conda create -n ShortConda
      source ~/.bashrc 
      conda activate ShortConda
      conda install -c bioconda cutadapt
```


Primers were initially checked using a  simple grep command
```{bash, eval=F}
      # ***
      #Trimming Primers
      # ***
      
      
      # from within 
      cd /scratch/user/mahe0050/0_rawData:
      
      ### G6 and G7 files renamed from Prefix "G6k_" to "G6_KI_" to match the rest of the samples
      head check zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | head -n 5
      
      
      #Illumina primers check:
      #~~~~~~~~~~~~~~~~~~~~~~~~
      
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | grep "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA" -m10
      # 		=		 (found on RHS)
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | grep "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT" -m10
      # 		=		 (none found)
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R2.fastq.gz | grep "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA" -m10
      # 		=		 (none found)
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R2.fastq.gz | grep "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT" -m10
      # 		=		 (found on RHS)
      
      
      ## Check of G6 as G6&7 were sequenced separately:
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R1.fastq.gz | grep "AGATCGGAAGAGCACACGTCTGAACTCCAGTCA" -m10
      # 		=		 (found on RHS)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R2.fastq.gz | grep "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT" -m10
      # 		=		 (found on RHS)
      
      
      #Clontech primers check (unknown bases removed for grep)
      #~~~~~~~~~~~~~~~~~~~~~~~~
      
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTAC" -m10
      # 		=		 (found on LHS)
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | grep "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (none found, found if the number of A is reduced)
      zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTACTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT" -m10
      # 		=		 (found on LHS)
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R1.fastq.gz | grep "GTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (found on RHS)
      
       zcat G1_KI_HCGYYBCXY_ATCACG_L001_R2.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTAC" -m10
      # 		=		 (found on LHS)
      zcat G1_KI_HCGYYBCXY_ATCACG_L001_R2.fastq.gz | grep "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (none found, found if the number of A is reduced)
      zcat G1_KI_HCGYYBCXY_ATCACG_L001_R2.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTACTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT" -m10
      # 		=		 (found on LHS)
      zcat G1_KI_HCGYYBCXY_ATCACG_L001_R2.fastq.gz | grep "GTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (found on RHS)
      
      #note that in the absence of the longer string of unknown bases (or in the presence of poly-a tails) the forward and reverse primers have a sequence that is identical so there are instances where they are both found on the LHS of R1 and R2
      
      ## Check of G6 as G6&7 were prepared separately [not expected here]:
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R1.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTAC" -m10
      # 		=		 (none found)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R1.fastq.gz | grep "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (none found)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R1.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTACTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT" -m10
      # 		=		 (none found)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R1.fastq.gz | grep "GTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (none found)
      
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R2.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTAC" -m10
      # 		=		 (none found)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R2.fastq.gz | grep "AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (none found)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R2.fastq.gz | grep "AAGCAGTGGTATCAACGCAGAGTACTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT" -m10
      # 		=		 (none found)
       zcat G6_KI_CDNJBANXX_GGTACCTT-AAGACGTC_L008_R2.fastq.gz | grep "GTACTCTGCGTTGATACCACTGCTT" -m10
      # 		=		 (none found)
      
      
      # ***
      # Proceed with script file removeAdadpters2.sh using SLURM queueing
      # ***
      
      cd /scratch/user/mahe0050/DE-analysis/bash
      
      #test first
      sbatch --test-only Slurm-Trim_DT-2022.sh
      
      #run
      # sbatch Slurm-Trim_DT-2022.sh        #a test run without quality trimming
      sbatch Slurm-Trimq_DT-2022.sh         #run with quality trimming 5
      
      #list all current jobs for a user
      squeue -u mahe0050

```

Trimming was completed within a conda environment, using the following [Slurm script](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Slurm-Trimq_DT-2022.sh) to submit to Flinder's Deep thought machine, and to call the script [removeAdapters2q.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/removeAdapters2q.sh) to remove primers and quality trim paired reads to a phredd 33 score of 5 (as below).  

```{bash, eval=F}
      #!/bin/bash
      
      # October 2022
      # Carmel Maher
      # This script is to submit a Cutadapt script to cut adapters and quality on raw reads through Flinders' DeepThought HPC Slurm
      # This script therefore assumes submission to Flinders Deepthought with an assumed directory structure and use of available modules
      
      
      #------------------------------------
      #Run from within ShortConda environment 
      module load Miniconda3/4.9.2
      conda activate ShortConda #(this is input in every SLURM script)
      
      #Required Installations
      #cutadapt (version 4.1)
      #fastQC (v0.11.9)
      
      #------------------------------------
      
      # Illumina Truseq Primers:
      # AGATCGGAAGAGCACACGTCTGAACTCCAGTCA (to remove from R1)
      # AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT (to remove from R2)
      
      # Clontech SMARTer Primers:
      # SMARTer II A Oligonucleotide
      # 5'– AAGCAGTGGTATCAACGCAGAGTACXXXXX –3' (X = undisclosed base in the proprietary SMARTer oligo sequence)
      # 3' SMART CDS Primer II A
      # 5’– AAGCAGTGGTATCAACGCAGAGTACT(30)N-1N –3’ (N = A, C, G, or T; N-1 = A, G, or C)
      # 
      # FWDPRIMER 	= 	AAGCAGTGGTATCAACGCAGAGTACNNNNN
      # RCFWDPRIMER 	= 	NNNNNGTACTCTGCGTTGATACCACTGCTT
      # REVPRIMER 	= 	AAGCAGTGGTATCAACGCAGAGTACT
      # RCREVPRIMER 	= 	AGTACTCTGCGTTGATACCACTGCTT
      
      
      #------------------------------------
      # Terry's error exit
      
      function error_exit
      {
        # Exit function due to fatal error
        # Accepts 1 arg:
        # string - descriptive error message
      
        echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
        exit 1
      }
      
      #	in = /scratch/user/mahe0050/DE-analysis/0_rawData
      #	out = /scratch/user/mahe0050/DE-analysis/1_trimmedData
      
      #------------------------------------
      
      #this script assumes you start IN the clean data folder as per DeepThought's paralell file system
      #cd $BGFS
      
      
      cd /scratch/user/mahe0050/DE-analysis/0_rawData || error_exit "$LINENO: directory error 1"
      
      
      for file in *R1.fastq.gz
      do
      echo $file
      FILESTEM=${file%_*}
      echo $FILESTEM
      FILESTEM2=`echo $file | cut -d "_" -f1,2 --output-delimiter="_"`
      echo $FILESTEM2
      
      #Check whether a FASTQ file is properly formatted
      #    cutadapt -o /dev/null $file || error_exit "$LINENO: Fastq format error"
      
      #Remove Illumina Truseq adapters from all paired data using the 'regular 3'  paired adapters' function (these adapters are on the RHS of both R1 and R2 reads in all 8 samples)
      #Reads are also trimmed to 5 phred-33
      cutadapt -a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA -A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT --cores=12 -q 5 -o ../1_trimmedData/q/$FILESTEM2"_R1_Icleanq.fq.gz" -p ../1_trimmedData/q/$FILESTEM2"_R2_Icleanq.fq.gz" $file $FILESTEM"_R2.fastq.gz" || error_exit "$LINENO: Error removing Illumina R1 adapters in $file"
      
      
      #Then remove Clontech SMARTer adapters from all paired data using 'linked adapters' function for paired end reads (No filtering for "anchored" adapters at this step - see notes, also not expected in G6 or G7)
      
      #as the length of target sequences is varied and unknown the possibility for the reverse completment of the second adapter is allowed by trimming as linked adapters.	
      #For linked adapters even though -a and -A are used (which indicate 3' adapters in the single line above) linked adapters assume the 5' is FIRST, and the 3' is SECOND.
      #Therefore to trim FORWARD R1 reads, they are called as <forward primer in 5'-3' orientation> ... <reverse complement of reverse primer> 
      #to trim REVERSE R2 reads, they are called as <reverse primer in 5'-3' orientation> ... <reverse complement of forward primer>	[https://cutadapt.readthedocs.io/en/stable/guide.html#linked-adapters]
      #Reads shorter than 30bp are filtered out
      cutadapt -a AAGCAGTGGTATCAACGCAGAGTACNNNNN...AGTACTCTGCGTTGATACCACTGCTT -A AAGCAGTGGTATCAACGCAGAGTACT...NNNNNGTACTCTGCGTTGATACCACTGCTT --cores=12 --minimum-length 30 -o ../1_trimmedData/q/$FILESTEM2"_R1_cleanq.fq.gz" -p ../1_trimmedData/q/$FILESTEM2"_R2_cleanq.fq.gz" ../1_trimmedData/q/$FILESTEM2"_R1_Icleanq.fq.gz" ../1_trimmedData/q/$FILESTEM2"_R2_Icleanq.fq.gz" || error_exit "$LINENO: Error removing SMARTer adapters in $file"
      done 
      	
      
      #run cleaned data through fastqc

```

The trimmed files were than assessed again using FastQC version 0.11.9 [submitted](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Slurm-FQC_DT-2022.sh) to Flinder's Deep Thought as below
```{bash, eval=F}
      #FastQC installed on environment ShortConda does not work. This is an issue with the conda channels used to install
      #error with fonts on report creation.
      #Create a new environment calling jdk will make it install from ::conda-forge so that it works.
      
      cd /scratch/user/mahe0050/DE-analysis/bash
      
      conda create -n FQConda fastqc openjdk
      source ~/.bashrc 
      conda activate FQConda
      
      #File: FastQC-cl-run.bash
      #FastQC generated for trimmed reads min length 30 with no quality trimming
      # AND
      #trimmed reads min length 30 with quality trimming 5 (phred -33)
      
      # called in SLURM script:
      sbatch Slurm-FQC_DT-2022.sh
      
      # outputs summarised in NGSReports in R

```

To call the following script [FastQC-cl-run.bash](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/FastQC-cl-run.bash)

```{bash, eval=F}
      !/bin/bash
      
      # Oct 2022
      # Carmel Maher
      # This script is to submit a FastQC analysis on trimmed reads through Flinders' DeepThought HPC Slurm
      # This script therefore assumes submission to Flinders Deepthought with an assumed directory structure and use of available modules
      
      
      #------------------------------------
      # Required Modules:
      
      #Required Installations
      #fastQC (v0.11.9)
      #openjdk::conda-forge/linux-64::openjdk-17.0.3-h85293d2_2
      
      
      #------------------------------------
      # Terry's error exit
      
      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
      
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      #------------------------------------
      
      cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/q/ || error_exit "$LINENO: directory error 1"
      
      for file in *R1_cleanq.fq.gz
      do
      	echo $file
      	FILESTEM2=`echo $file | cut -d "_" -f1,2 --output-delimiter="_"`
      	echo "FILESTEM2" $FILESTEM2
      	
      	fastqc --noextract --threads 8 -o ./FastQC_Cleanq ./$FILESTEM2"_R1_cleanq.fq.gz" ./$FILESTEM2"_R2_cleanq.fq.gz" || error_exit "$LINENO: Error with FastQC-q at "$FILESTEM2""
      done
      
      	
      cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/ || error_exit "$LINENO: directory error 2"
      
      for file in *R1_clean.fq.gz
      do
      	echo $file
      	FILESTEM2=`echo $file | cut -d "_" -f1,2 --output-delimiter="_"`
      	echo "FILESTEM2" $FILESTEM2
      	
      	fastqc --noextract --threads 8 -o ./FastQC_Clean ./$FILESTEM2"_R1_clean.fq.gz" ./$FILESTEM2"_R2_clean.fq.gz" || error_exit "$LINENO: Error with FastQC at "$FILESTEM2""
      done
```


The output html file of all eight samples FastQC reports AFTER trimming is available [here](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/FastQC-clean_ngsReports_Fastqc---Copy.html).


![FastQC Summary of scores for paired reads of eight samples and resulting 'singleton' files for trimmed reads with no pair, after adapter and quality trimming in BBduk](./FigureCopyDir/FastQC-clean-summary.png)

\newpage

### _De-novo_ Assembly with TRINITY

_De-novo_ assembly of transcripts.  
Short-reads were assembled _de novo_ using the program TRINITY v2.14.0.  through the Singularity container available [here](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-in-Docker)

Sequencing run AGRF_CAGRF13871_HCGYYBCXY involved the synthesis of a non-stranded cDNA library prior to sequencing, while sequencing run AGRF_CAGRF20022_CDNJBANXX was completed with a different kit. All samples were treated as non-stranded for consistency. 

Assembly was run on each sample individually as well as on a pooled (concatenated) file of all reads.  


Individual samples were assembled using the following script on Flinders University's Deep Thought with a module of singularity v.3.6.3:  [Slurm-Trinity_DT](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Slurm-Trinity_DT-2022.sh), as below.
```{bash, eval=F}
       #!/bin/bash
      # Please note that you need to adapt this script to your job
      # Submitting as is will fail cause the job to fail 
      # The keyword command for SLURM is #SBATCH --option
      # Anything starting with a # is a comment and will be ignored
      # ##SBATCH is a commented-out #SBATCH command
      # SBATCH and sbatch are identical, SLURM is not case-sensitive
      ##################################################################
      # Change FAN to your fan account name
      # Change JOBNAME to what you want to call the job
      # This is what is shows when attempting to Monitor / interrogate the job,
      # So make sure it is something pertinent!
      #
      #SBATCH --job-name=mahe0050_Trinity
      #
      ##################################################################
      # If you want email updates form SLURM for your job.
      # Change MYEMAIL to your email address
      #SBATCH --mail-user=carmel.maher@flinders.edu.au
      #SBATCH --mail-type=ALL
      # 
      # Valid 'points of notification are': 
      # BEGIN, END, FAIL, REQUEUE. 
      # ALL means all of these
      ##################################################################
      # Tell SLURM where to put the Job 'Output Log' text file. 
      # This will aid you in debugging crashed or stalled jobs.
      # You can capture both Standard Error and Standard Out
      # %j will append the 'Job ID' from SLURM. 
      # %x will append the 'Job Name' from SLURM 
      # %
      #SBATCH --output=/home/mahe0050/%x-%j.out.txt
      #SBATCH --error=/home/mahe0050/%x-%j.err.txt
      ##################################################################
      # The default partition is 'general'. 
      # Valid partitions are general, gpu and melfu
      ##SBATCH --partition=PARTITIONNAME
      #
      ##################################################################
      # Tell SLURM how long your job should run for as a hard limit. 
      # My setting a shorter time limit, it is more likely that your
      # job will be scheduled when attempting to backfill jobs. 
      # 
      # The current cluster-wide limit is 14 Days from Start of Execution.
      # The timer is only active while your job runs, so if you suspend
      # or pause the job, it will stop the timer.
      #
      # The command format is as follows: #SBATCH --time=DAYS-HOURS
      # There are many ways to specify time, see the SchedMD Slurm 
      # manual pages for more. 
      #SBATCH --time=14-0
      #
      ##################################################################
      # How many tasks is your job going to run? 
      # Unless you are running something that is Parallel / Modular or
      # pipelined, leave this as 1. Think of each task as a 'bucket of
      # resources' that stand alone. Without MPI / IPC you can't talk to 
      # another bucket!
      #
      #SBATCH --ntasks=1
      #
      # If each task will need more that a single CPU, then alter this 
      # value. Remember, this is multiplicative, so if you ask for 
      # 4 Tasks and 4 CPU's per Task, you will be allocated 16 CPU's 
      #SBATCH --cpus-per-task=16
      ##################################################################
      # Set the memory requirements for the job in MB. Your job will be
      # allocated exclusive access to that amount of RAM. In the case it
      # overuses that amount, Slurm will kill the job. The default value is 
      # around 2GB per CPU you ask for.
      #
      # Note that the lower the requested memory, the higher the
      # chances to get scheduled to 'fill in the gaps' between other
      # jobs. Pick ONE of the below options. They are Mutually Exclusive.
      # You can ask for X Amount of RAM per CPU (MB by default).
      # Slurm understands K/M/G/T For Kilo/Mega/Giga/Tera Bytes.
      #
      ##SBATCH --mem-per-cpu=12G
      # Or, you can ask for a 'total amount of RAM'. If you have multiple 
      # tasks and ask for a 'total amount' like below, then SLURM will 
      # split the total amount to each task evenly for you.
      #SBATCH --mem=128G
      ##################################################################
      # Change the number of GPU's required for you job. The most GPU's that can be 
      # requested is 2 per node. As there are limited GPU slots, they are heavily 
      # weighted against for Fairshare Score calculations. 
      # You can request either a 'gpu:telsa_v100:X' or a 'gpu:x'
      # 
      # You can either request 0, or omit this line entirely if you 
      # a GPU is not needed. 
      #
      #SBATCH --gres="gpu:0"
      ##################################################################
      # Load any modules that are required. This is exactly the same as 
      # loading them manually, with a space-separated list, or you can 
      # write multiple lines.
      # You will need to uncomment these.
       
      module load singularity/3.6.3
      
      ##################################################################
      # This example script assumes that you have already moved your 
      # dataset to /scratch as part of your HPC Pre-Job preparations. 
      # Its best to use the $TMP/$TMPDIR setup for you here
      # to allow for the HPC to auto-clean anything you 
      # leave behind by accident. 
      # If you have a job-array and need a shared directory for 
      # data on /local, you will need to manually cleanup that 
      # directory as a part of your job script. 
      
      # Example using the SLURM $BGFS Variable (the Parallel Filesystem)
      
      cd $BGFS
      mkdir $BGFS/data/
      
      cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/q
      
      ##################################################################
      # Enter the command-line arguments that you job needs to run. 
      
      
      # Carmel Maher & Terry Bertozzi
      # Sep 2017 - last edited oct 2022
      
      
      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
      
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      #Slurm script places you in a temporary directory, this shall be treated as the same 'level' as /scratch/user/mahe0050/DE-analysis/
      
      # go to the working directory - working from $BGFS/data
      pwd
      
      # *** #
      
      # To run trinity separately on each sample
      	
      	# assemble transcripts from a single paired sample file
      	#note trinity requires output directory with "trinity" in name
      	
      # ***G1***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G1_KI_R1_cleanq.fq.gz  \
                --right /data/G1_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G1_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G1_KI_"
      
      echo "Trinity G1 complete"
      
      	
      # ***G2***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G2_KI_R1_cleanq.fq.gz  \
                --right /data/G2_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G2_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G2_KI_"
      
      echo "Trinity G2 complete"
      
      	
      # ***G3***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G3_KI_R1_cleanq.fq.gz  \
                --right /data/G3_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G3_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G3_KI_"
      
      echo "Trinity G3 complete"
      
      	
      # ***G4***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G4_KI_R1_cleanq.fq.gz  \
                --right /data/G4_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G4_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G4_KI_"
      
      echo "Trinity G4 complete"
      
      	
      # ***G5***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G5_KI_R1_cleanq.fq.gz  \
                --right /data/G5_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G5_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G5_KI_"
      
      echo "Trinity G5 complete"
      
      	
      # ***G6***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G6_KI_R1_cleanq.fq.gz  \
                --right /data/G6_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G6_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G6_KI_"
      
      echo "Trinity G6 complete"
      
      	
      # ***G7***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G7_KI_R1_cleanq.fq.gz  \
                --right /data/G7_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G7_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G7_KI_"
      
      echo "Trinity G7 complete"
      
      	
      # ***G8***	
      
      	singularity exec --home $BGFS/data:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/G8_KI_R1_cleanq.fq.gz  \
                --right /data/G8_KI_R2_cleanq.fq.gz \
                --verbose --CPU 16 --max_memory 128G \
                --output G8_KI_trinity-sep || error_exit "$LINENO: Error running trinity-sep at G8_KI_"
      
      echo "Trinity G8 complete"
      
      
      ##############
      # REMOVE all * , relative file positions, FILESTEM & other variables, they are not read within the container.
      ##############
      
      
      ##################################################################
      # Once you job has finished its processing, copy back your results 
      # and ONLY the results to /scratch, then clean-up the temporary 
      # working directory
      # This command assumes that the destination exists
      
      mkdir /scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-sep
      
      cp -r /$BGFS/data /scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-sep

```


All kidney samples were run through Trinity together using the same programs and parameters as above, in another script [Slurm-TrinityAll_DT](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Slurm-TrinityAll_DT-2022.sh), detailed below.

Sample files to be concatenated and named appropriately
```{bash, eval=F}
      #!/bin/bash
      # Please note that you need to adapt this script to your job
      # Submitting as is will fail cause the job to fail 
      # The keyword command for SLURM is #SBATCH --option
      # Anything starting with a # is a comment and will be ignored
      # ##SBATCH is a commented-out #SBATCH command
      # SBATCH and sbatch are identical, SLURM is not case-sensitive
      ##################################################################
      # Change FAN to your fan account name
      # Change JOBNAME to what you want to call the job
      # This is what is shows when attempting to Monitor / interrogate the job,
      # So make sure it is something pertinent!
      #
      #SBATCH --job-name=mahe0050_Trinity
      #
      ##################################################################
      # If you want email updates form SLURM for your job.
      # Change MYEMAIL to your email address
      #SBATCH --mail-user=carmel.maher@flinders.edu.au
      #SBATCH --mail-type=ALL
      # 
      # Valid 'points of notification are': 
      # BEGIN, END, FAIL, REQUEUE. 
      # ALL means all of these
      ##################################################################
      # Tell SLURM where to put the Job 'Output Log' text file. 
      # This will aid you in debugging crashed or stalled jobs.
      # You can capture both Standard Error and Standard Out
      # %j will append the 'Job ID' from SLURM. 
      # %x will append the 'Job Name' from SLURM 
      # %
      #SBATCH --output=/home/mahe0050/%x-%j.out.txt
      #SBATCH --error=/home/mahe0050/%x-%j.err.txt
      ##################################################################
      # The default partition is 'general'. 
      # Valid partitions are general, gpu and melfu
      ##SBATCH --partition=PARTITIONNAME
      #
      ##################################################################
      # Tell SLURM how long your job should run for as a hard limit. 
      # My setting a shorter time limit, it is more likely that your
      # job will be scheduled when attempting to backfill jobs. 
      # 
      # The current cluster-wide limit is 14 Days from Start of Execution.
      # The timer is only active while your job runs, so if you suspend
      # or pause the job, it will stop the timer.
      #
      # The command format is as follows: #SBATCH --time=DAYS-HOURS
      # There are many ways to specify time, see the SchedMD Slurm 
      # manual pages for more. 
      #SBATCH --time=14-0
      #
      ##################################################################
      # How many tasks is your job going to run? 
      # Unless you are running something that is Parallel / Modular or
      # pipelined, leave this as 1. Think of each task as a 'bucket of
      # resources' that stand alone. Without MPI / IPC you can't talk to 
      # another bucket!
      #
      #SBATCH --ntasks=1
      #
      # If each task will need more that a single CPU, then alter this 
      # value. Remember, this is multiplicative, so if you ask for 
      # 4 Tasks and 4 CPU's per Task, you will be allocated 16 CPU's 
      #SBATCH --cpus-per-task=16
      ##################################################################
      # Set the memory requirements for the job in MB. Your job will be
      # allocated exclusive access to that amount of RAM. In the case it
      # overuses that amount, Slurm will kill the job. The default value is 
      # around 2GB per CPU you ask for.
      #
      # Note that the lower the requested memory, the higher the
      # chances to get scheduled to 'fill in the gaps' between other
      # jobs. Pick ONE of the below options. They are Mutually Exclusive.
      # You can ask for X Amount of RAM per CPU (MB by default).
      # Slurm understands K/M/G/T For Kilo/Mega/Giga/Tera Bytes.
      #
      ##SBATCH --mem-per-cpu=12G
      # Or, you can ask for a 'total amount of RAM'. If you have multiple 
      # tasks and ask for a 'total amount' like below, then SLURM will 
      # split the total amount to each task evenly for you.
      #SBATCH --mem=256G
      ##################################################################
      # Change the number of GPU's required for you job. The most GPU's that can be 
      # requested is 2 per node. As there are limited GPU slots, they are heavily 
      # weighted against for Fairshare Score calculations. 
      # You can request either a 'gpu:telsa_v100:X' or a 'gpu:x'
      # 
      # You can either request 0, or omit this line entirely if you 
      # a GPU is not needed. 
      #
      #SBATCH --gres="gpu:0"
      ##################################################################
      # Load any modules that are required. This is exactly the same as 
      # loading them manually, with a space-separated list, or you can 
      # write multiple lines.
      # You will need to uncomment these.
       
      module load singularity/3.6.3
      
      ##################################################################
      # This example script assumes that you have already moved your 
      # dataset to /scratch as part of your HPC Pre-Job preparations. 
      # Its best to use the $TMP/$TMPDIR setup for you here
      # to allow for the HPC to auto-clean anything you 
      # leave behind by accident. 
      # If you have a job-array and need a shared directory for 
      # data on /local, you will need to manually cleanup that 
      # directory as a part of your job script. 
      
      # Example using the SLURM $BGFS Variable (the Parallel Filesystem)
      
      cd $BGFS
      mkdir $BGFS/data2/
      
      cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/q
      
      ##################################################################
      # Enter the command-line arguments that you job needs to run. 
      
      
      # Carmel Maher & Terry Bertozzi
      # Sep 2017 - last edited oct 2022
      
      
      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
      
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      #Slurm script places you in a temporary directory, this shall be treated as the same 'level' as /scratch/user/mahe0050/DE-analysis/
      
      # go to the working directory - working from $BGFS/data
      pwd
      
      # *** #
      
      
      ##############
      # REMOVE all * , relative file positions, FILESTEM & other variables, they are not read within the container.
      ##############
      
      
      # *** #
      
      #To create one master kidney transcript set, sample files to be concatenated and named appropriately
      
      cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/q
      
      	#concatenate relevant sample files for one transcriptome - add singletons to R1 as its not stranded anyway? 
      	cat ./*_R1_cleanq.fq.gz >> ./PBTKI_cat_R1.fq.gz || error_exit "$LINENO: Error concatenating R1"
      	cat ./*_R2_cleanq.fq.gz >> ./PBTKI_cat_R2.fq.gz || error_exit "$LINENO: Error concatenating R2"
      
      	singularity exec --home $BGFS/data2:/home -B /scratch/user/mahe0050/DE-analysis/1_trimmedData/q:/data -e /scratch/user/mahe0050/DE-analysis/trinityrnaseq.v2.14.0.simg  Trinity \
                --seqType fq \
                --left /data/PBTKI_cat_R1.fq.gz  \
                --right /data/PBTKI_cat_R2.fq.gz \
                --verbose --CPU 16 --max_memory 256G \
                --output trinity-all || error_exit "$LINENO: Error running Trinity-all"
      	
      echo "Trinity concatenated kidney samples complete"
      
      #clean up concatenated files
      
      rm -rf /scratch/user/mahe0050/DE-analysis/1_trimmedData/q/PBTKI_cat_R1.fq.gz || error_exit "$LINENO: Error cleanup concatenated R1"
      rm -rf /scratch/user/mahe0050/DE-analysis/1_trimmedData/q/PBTKI_cat_R2.fq.gz || error_exit "$LINENO: Error cleanup concatenated R1"
      
      
      ##################################################################
      # Once you job has finished its processing, copy back your results 
      # and ONLY the results to /scratch, then clean-up the temporary 
      # working directory
      # This command assumes that the destination exists
      
      mkdir /scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-all
      cp -r /$BGFS/data2 /scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-all
      
      # No need to cleanup $BGFS, SLURM handles the cleanup for you. 
      # Just dont forget to copy out your results, or you will lose them!
      
      ##################################################################
```  



\newpage

## <ins>**_Summary / Quantification_**</ins>

### Long-read Sequences Data Summary

**15,729** total transcript isoforms retained in initial cleaning and clustering of redundant isoforms.  

**13,882** sequences in a predicted open reading frame.  

**9,907** clusters identified based on translated proteins of predicted open reading frame.  

**9,813** full length transcripts were subset into a reference file as the longest representatives of one or more transcript clusters. 

### Short-read Sequences Data Summary

Total Transcript count of each TRINITY generated dataset was simply conducted using [shell commands](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/TRINITY_FastaOutput-TranscriptCount.sh). Total transcript count for long-read files are included in the BUSCO usage file below as headers required manipulation for input into BUSCO and totals were checked at this step.  


| **Sample ID** | **Experimental factors** | **File name** | **Total transcripts/sequences assembled** |
| ---: | :---: | :---: | ---: |
| G1 | September Female | G1_KI_trinity-sep | 48161 | 
| G2 | September Female | G2_KI_trinity-sep | 45075 | 
| G3 | March/April Female | G3_KI_trinity-sep | 37039 | 
| G4 | March/April Male | G4_KI_trinity-sep | 40893 | 
| G5 | September Male | G5_KI_trinity-sep | 38719 | 
| G6 | September Male | G6_KI_trinity-sep | 185868 | 
| G7 | March/April Female | G7_KI_trinity-sep | 229145 | 
| G8 | March/April Female | G8_KI_trinity-sep | 43621 | 
| All Samples combined | n/a | trinity-all | 359197 | 


## BUSCO

Completeness of assemblies were assessed using Benchmarking Universal Single-Copy Orthologs BUSCO version 5.4.2 ([BUSCO](https://busco.ezlab.org/)) and the dataset vertebrata_odb10.  

**Moving finalised assembly files into the BUSCO Directory & Editing Headers**  

BUSCO requires simplified header formats as the symbols included in appended names and quality information in the provided pasta files interfere with its processing. Long read reference files were copied into the BUSCO directory and their headers altered, and the locations for where to call the trinity assemblies from are outlined in the bash document [output-BUSCO_Name.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Output-BUSCO_Name.sh):
```{bash, eval=F}
      #!/bin/bash
      #
      #
      # These command lines were used to rename and copy assembled trinity & long read transcript datasets into the /BUSCO directory so all files have similar naming convention for input into the BUSCO script.
      #
      # The paths assume that a specific directory structure has been set up.
      #
      # Modules required: none
      #
      # 
      # Carmel Maher
      # August 2020 edited Oct 2022
      
      
      
      
      #to grab the trinity output for BUSCO:
      #-------------------------------------
      
      
      ###SHORT_READ ASSEMBLIES
      
      # go to the working directory - working from /clean
      # cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/q  || error_exit "$LINENO: Directory Error 1"
      
      #for file in *R1_cleanq.fq.gz
      #	do
      #
      #		FILESTEM=${file%_*}
      #		#this FILESTEM only cuts to _clean, the _R1 is included in stem
      #		FILESTEM=${FILESTEM/R1/}
      #		#removes R1 from FILESTEM (FILESTEM ends in _ therefore not needed in "text" names)
      
      #All individual samples are named as below:
      #		
      #		/scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-sep/data/$FILESTEM"trinity-sep.Trinity".fasta
      
      #Concatenated assembly of all samples is named:
      #		
      #		/scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-all/data2/trinity-all.Trinity.fasta /scratch/user/mahe0050/BUSCO/trinity-all_assembled.fasta  || error_exit "$LINENO: Error copying concatenated trinity-all output"
      	
      	
      #####
      #Note that using the above method all single sample trinity outputs can be called without editing or moving them.
      #####
      
      
      #-------------------------------------
      ### LONG READS (manually moved/renamed line by line)
      
      
      cp -i /scratch/user/mahe0050/IsoSeq-analysis/data/Cogent/collected/hq.fasta.no5merge.collapsed.rep.fa /scratch/user/mahe0050/BUSCO/hq.fasta.no5merge.collapsed.rep.fasta
      	
      		# -> one long-read file should end up in /scratch/user/mahe0050/IsoSeq-analysis/BUSCO named hq.fasta.no5merge.collapsed.rep_assembled.fasta This contains the full length of all non-redundant high quality transcripts
      
      cp -i /scratch/user/mahe0050/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds /scratch/user/mahe0050/BUSCO/pygmy.ANGEL.cds.fasta
      	
      		# -> one long-read file should end up in /scratch/user/mahe0050/BUSCO named pygmy.ANGEL_assembled.fasta This contains the predicted coding sequence of non-redundant unique transcripts before protein clustering
      		
      cp -i /scratch/user/mahe0050/IsoSeq-analysis/data/Seqtk/reference_transcripts.1Lv.clean.fasta /scratch/user/mahe0050/BUSCO/reference_transcripts.1Lv.clean.fasta
      	
      		# -> one long-read file should end up in /scratch/user/mahe0050/IsoSeq-analysis/BUSCO named reference_transcripts.1Lv.clean_assembled.fasta This contains the full length transcripts corresponding to the representative for clustered proteins translated from predicted open read frame
      
      
      # header formats:
      head /scratch/user/mahe0050/BUSCO/hq.fasta.no5merge.collapsed.rep.fasta
      >PB.1.1|000643|path0:1-1879(+)|transcript/14742 transcript/14742 full_length_coverage=3;length=1887;num_subreads=52
      
      head /scratch/user/mahe0050/BUSCO/pygmy.ANGEL.cds.fasta
      >PB.2.1|002537|path0:1-1624(+)|transcript/18304|m.1 type:likely-NA len:135 strand:+ pos:282-686
      
      head /scratch/user/mahe0050/BUSCO/reference_transcripts.1Lv.clean.fasta
      >PB.2.1|002537|path0:1-1624(+)|transcript/18304 transcript/18304 full_length_coverage=2;length=1627;num_subreads=26
      
      
      grep -c ">" /scratch/user/mahe0050/BUSCO/hq.fasta.no5merge.collapsed.rep.fasta
      15729
      grep -c ">" /scratch/user/mahe0050/BUSCO/pygmy.ANGEL.cds.fasta
      13882
      grep -c ">" /scratch/user/mahe0050/BUSCO/reference_transcripts.1Lv.clean.fasta
      9813
      
      
      # NOTE the files have symbols in faste headers such as "/" and "+" before transcript IDs which need to be removed for BUSCO
      
      # after sed to remove all "/" resulted in an error at metaeuk headers "ValueError: could not convert string to float: '+'"
      # after sed to remove all "+" same error
      
      # Goal is a PERCENT score of alignments to BUSCOs and which specific transcripts match is no explored further. Therefore all headers were grossly simplified & truncated to remove symbols
      
      cut -d '|' -f1 /scratch/user/mahe0050/BUSCO/hq.fasta.no5merge.collapsed.rep.fasta > hq.fasta.no5merge.collapsed.rep_.fasta
      
      cut -d '|' -f1 /scratch/user/mahe0050/BUSCO/pygmy.ANGEL.cds.fasta > pygmy.ANGEL.cds_.fasta
      
      cut -d '|' -f1 /scratch/user/mahe0050/BUSCO/reference_transcripts.1Lv.clean.fasta > reference_transcripts.1Lv.clean_.fasta
      
      
      # total number of sequences retained as counted above
      grep -c ">" /scratch/user/mahe0050/BUSCO/hq.fasta.no5merge.collapsed.rep_.fasta
      15729
      grep -c ">" /scratch/user/mahe0050/BUSCO/pygmy.ANGEL.cds_.fasta
      13882
      grep -c ">" /scratch/user/mahe0050/BUSCO/reference_transcripts.1Lv.clean_.fasta
      9813
      
      
      # two of these files will have "duplicate" headers in this format as we know that some transcripts gave rise to >1 predicted cds. numbers were appended as below to make all sequence headers unique:
      awk '/^>/{$0=$0"_"(++i)}1'  pygmy.ANGEL.cds_.fasta > pygmy.ANGEL.cds__.fasta
      awk '/^>/{$0=$0"_"(++i)}1'  hq.fasta.no5merge.collapsed.rep_.fasta > hq.fasta.no5merge.collapsed.rep__.fasta
      
      grep -c ">" hq.fasta.no5merge.collapsed.rep__.fasta
      15729
      grep -c ">" pygmy.ANGEL.cds__.fasta
      13882

```


**BUSCO Installation & Usage:**  

BUSCO was installed on Flinders Deep Thought machine using a Miniconda environment created as below
```{bash, eval=F}
# ***
# BUSCO: Comparison of 3 stages of long read filtering, to de-novo assembled transcriptomes
# ***

cd /scratch/user/mahe0050/DE-analysis/bash

source ~/.bashrc 
conda create -n BUSCOConda python=3.7
conda activate BUSCOConda
conda install -c conda-forge -c bioconda busco=5.4.2

busco --v
BUSCO 5.4.2

```

The script [Slurm-BUSCO_DT](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Slurm-BUSCO_DT.sh) was used to run the program BUSCO on all samples on the NECTAR machine.  
Run BUSCO \-\>

```{bash, eval=F}
      #!/bin/bash
      # Please note that you need to adapt this script to your job
      # Submitting as is will fail cause the job to fail 
      # The keyword command for SLURM is #SBATCH --option
      # Anything starting with a # is a comment and will be ignored
      # ##SBATCH is a commented-out #SBATCH command
      # SBATCH and sbatch are identical, SLURM is not case-sensitive
      ##################################################################
      # Change FAN to your fan account name
      # Change JOBNAME to what you want to call the job
      # This is what is shows when attempting to Monitor / interrogate the job,
      # So make sure it is something pertinent!
      #
      #SBATCH --job-name=mahe0050_BUSCO
      #
      ##################################################################
      # If you want email updates form SLURM for your job.
      # Change MYEMAIL to your email address
      #SBATCH --mail-user=carmel.maher@flinders.edu.au
      #SBATCH --mail-type=ALL
      # 
      # Valid 'points of notification are': 
      # BEGIN, END, FAIL, REQUEUE. 
      # ALL means all of these
      ##################################################################
      # Tell SLURM where to put the Job 'Output Log' text file. 
      # This will aid you in debugging crashed or stalled jobs.
      # You can capture both Standard Error and Standard Out
      # %j will append the 'Job ID' from SLURM. 
      # %x will append the 'Job Name' from SLURM 
      # %
      #SBATCH --output=/home/mahe0050/%x-%j.out.txt
      #SBATCH --error=/home/mahe0050/%x-%j.err.txt
      ##################################################################
      # The default partition is 'general'. 
      # Valid partitions are general, gpu and melfu
      ##SBATCH --partition=PARTITIONNAME
      #
      ##################################################################
      # Tell SLURM how long your job should run for as a hard limit. 
      # My setting a shorter time limit, it is more likely that your
      # job will be scheduled when attempting to backfill jobs. 
      # 
      # The current cluster-wide limit is 14 Days from Start of Execution.
      # The timer is only active while your job runs, so if you suspend
      # or pause the job, it will stop the timer.
      #
      # The command format is as follows: #SBATCH --time=DAYS-HOURS
      # There are many ways to specify time, see the SchedMD Slurm 
      # manual pages for more. 
      #SBATCH --time=5-0
      #
      ##################################################################
      # How many tasks is your job going to run? 
      # Unless you are running something that is Parallel / Modular or
      # pipelined, leave this as 1. Think of each task as a 'bucket of
      # resources' that stand alone. Without MPI / IPC you can't talk to 
      # another bucket!
      #
      #SBATCH --ntasks=1
      #
      # If each task will need more that a single CPU, then alter this 
      # value. Remember, this is multiplicative, so if you ask for 
      # 4 Tasks and 4 CPU's per Task, you will be allocated 16 CPU's 
      #SBATCH --cpus-per-task=12
      ##################################################################
      # Set the memory requirements for the job in MB. Your job will be
      # allocated exclusive access to that amount of RAM. In the case it
      # overuses that amount, Slurm will kill the job. The default value is 
      # around 2GB per CPU you ask for.
      #
      # Note that the lower the requested memory, the higher the
      # chances to get scheduled to 'fill in the gaps' between other
      # jobs. Pick ONE of the below options. They are Mutually Exclusive.
      # You can ask for X Amount of RAM per CPU (MB by default).
      # Slurm understands K/M/G/T For Kilo/Mega/Giga/Tera Bytes.
      #
      #SBATCH --mem-per-cpu=8G
      # Or, you can ask for a 'total amount of RAM'. If you have multiple 
      # tasks and ask for a 'total amount' like below, then SLURM will 
      # split the total amount to each task evenly for you.
      ##SBATCH --mem=128G
      ##################################################################
      # Change the number of GPU's required for you job. The most GPU's that can be 
      # requested is 2 per node. As there are limited GPU slots, they are heavily 
      # weighted against for Fairshare Score calculations. 
      # You can request either a 'gpu:telsa_v100:X' or a 'gpu:x'
      # 
      # You can either request 0, or omit this line entirely if you 
      # a GPU is not needed. 
      #
      #SBATCH --gres="gpu:0"
      ##################################################################
      # Load any modules that are required. This is exactly the same as 
      # loading them manually, with a space-separated list, or you can 
      # write multiple lines.
      # You will need to uncomment these.
      
      module load Miniconda3/4.9.2 
      module load singularity/3.6.3
      
      ##################################################################
      # This example script assumes that you have already moved your 
      # dataset to /scratch as part of your HPC Pre-Job preparations. 
      # Its best to use the $TMP/$TMPDIR setup for you here
      # to allow for the HPC to auto-clean anything you 
      # leave behind by accident. 
      # If you have a job-array and need a shared directory for 
      # data on /local, you will need to manually cleanup that 
      # directory as a part of your job script. 
      
      # Example using the SLURM $BGFS Variable (the Parallel Filesystem)
      #cd $BGFS
      #cp -r /scratch/user/mahe0050/DE-analysis/1_trimmedData/q ./
      
      ##################################################################
      # Enter the command-line arguments that you job needs to run. 
      
      source ~/.bashrc 
      conda activate BUSCOConda
      
      # See notes file associated with usage: Output-BUSCO_Name2020.sh and BUSCO_all_v5.0.sh
      
      # Inputs include:
      # 	Short-read Trinity outputs for all 8 Kidney samples assembled from short reads
      # 	Short-read Trinity output for one file of all 8 kidney sequencing concatenated and assembled into a single transcript file
      
      # 	Isoseq3 long-read full list of non-redundant hq transcripts
      # 	Isoseq3 long-read full list of transcript predicted open read frame coding regions
      # 	Isoseq3 long-read representative transcripts of 'putative genes' based on coding sequence protein clustering, but including full-length including UTRs, with additional poly-a tail trimming
      
      
      #------adjust these for your run-----
      
      LINEAGE="vertebrata_odb10"
      
      #------------------------------------
      
      function error_exit
      {
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      
      #go to the directory containing trimmed files to pull ID names
      cd /scratch/user/mahe0050/DE-analysis/1_trimmedData/q
      
      for file in *R1_cleanq.fq.gz
      	do
      		FILESTEM=${file%_*}
      		#this FILESTEM only cuts to _clean, the _R1 is included in stem
      		FILESTEM=${FILESTEM/R1/}
      		#removes R1 from FILESTEM (FILESTEM ends in _ therefore not needed in "text" names)
      		
      		mkdir /scratch/user/mahe0050/BUSCO/$FILESTEM"BUSCOout" || error_exit "$LINENO: Error creating trinity-sep output directory at $FILESTEM"
      		
      		cd /scratch/user/mahe0050/BUSCO/
      		
      		busco -i /scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-sep/data/$FILESTEM"trinity-sep.Trinity".fasta -l $LINEAGE -f -c 12 -o $FILESTEM"BUSCOout" -m transcriptome || error_exit "$LINENO: Error running BUSCO at $FILESTEM"
      		
       	done
      
      echo "trinity-sep BUSCOs complete"
      
      
      #
      # ***
      #
      
      
      cd /scratch/user/mahe0050/BUSCO/
      
      mkdir ./trinity-all_BUSCOout  || error_exit "$LINENO: directory error at trinity-all"
      
      busco -i /scratch/user/mahe0050/DE-analysis/2_alignedData/trinity-all/data2/trinity-all.Trinity.fasta -l $LINEAGE -f -c 12 -o trinity-all_BUSCOout -m transcriptome || error_exit "$LINENO: Error running BUSCO at trinity-all"
      
      echo "trinity-all BUSCOs complete"
      
      
      #
      # ***
      #
      
      
       cd /scratch/user/mahe0050/BUSCO/
      
       mkdir -p ./hq-fasta_BUSCOout || error_exit "$LINENO: directory error at hq-fasta"
      
       /home/mahe0050/.conda/envs/BUSCOConda/bin/busco -i hq.fasta.no5merge.collapsed.rep__.fasta -l $LINEAGE -f -c 12 -o hq-fasta_BUSCOout -m transcriptome || error_exit "$LINENO: Error running BUSCO at hq-fasta"
      
      #
      
       mkdir -p ./ANGEL.cds_BUSCOout || error_exit "$LINENO: directory error at reference-transcripts"
      
       /home/mahe0050/.conda/envs/BUSCOConda/bin/busco -i pygmy.ANGEL.cds__.fasta -l $LINEAGE -f -c 12 -o ANGEL.cds_BUSCOout -m transcriptome || error_exit "$LINENO: Error running BUSCO at ANGEL.cds"
      
      #
      
       mkdir -p ./reference-transcripts_BUSCOout || error_exit "$LINENO: directory error at reference-transcripts"
      
       /home/mahe0050/.conda/envs/BUSCOConda/bin/busco -i reference_transcripts.1Lv.clean_.fasta -l $LINEAGE -f -c 12 -o reference-transcripts_BUSCOout -m transcriptome || error_exit "$LINENO: Error running BUSCO at reference-transcripts"
      
      #
      
      echo "all BUSCOs complete"
      
      
      ##################################################################
      # Once you job has finished its processing, copy back your results 
      # and ONLY the results to /scratch, then clean-up the temporary 
      # working directory
      # This command assumes that the destination exists
      
      # cp -r /$BGFS/2_alignedData/trinity-all /scratch/user/mahe0050/DE-analysis/2_alignedData/
      
      # cp -r /$BGFS/2_alignedData/trinity-sep /scratch/user/mahe0050/DE-analysis/2_alignedData/
      
      # No need to cleanup $BGFS, SLURM handles the cleanup for you. 
      # Just dont forget to copy out your results, or you will lose them!
      
      ##################################################################
```


**Visualisation**  

BUSCO outputs a file named \<-o\>/short_summary.specific.vertebrata_odb10.\<-o\>.txt These are used to plot a summary figure of all samples.  

These summary files were manually copied into ./BUSCO_summaries to group the outputs. 
The python script generate_plot.py included in the BUSCO installation was then used using the following [script](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/Slurm-BUSCO_DT_summ.sh) to convert these scores into an R script to generate a visualisation of the output.

```{bash, eval=F}
#!/bin/bash
# Please note that you need to adapt this script to your job
# Submitting as is will fail cause the job to fail 
# The keyword command for SLURM is #SBATCH --option
# Anything starting with a # is a comment and will be ignored
# ##SBATCH is a commented-out #SBATCH command
# SBATCH and sbatch are identical, SLURM is not case-sensitive
##################################################################
# Change FAN to your fan account name
# Change JOBNAME to what you want to call the job
# This is what is shows when attempting to Monitor / interrogate the job,
# So make sure it is something pertinent!
#
#SBATCH --job-name=mahe0050_BUSCO-summmary
#
##################################################################
# If you want email updates form SLURM for your job.
# Change MYEMAIL to your email address
#SBATCH --mail-user=carmel.maher@flinders.edu.au
#SBATCH --mail-type=ALL
# 
# Valid 'points of notification are': 
# BEGIN, END, FAIL, REQUEUE. 
# ALL means all of these
##################################################################
# Tell SLURM where to put the Job 'Output Log' text file. 
# This will aid you in debugging crashed or stalled jobs.
# You can capture both Standard Error and Standard Out
# %j will append the 'Job ID' from SLURM. 
# %x will append the 'Job Name' from SLURM 
# %
#SBATCH --output=/home/mahe0050/%x-%j.out.txt
#SBATCH --error=/home/mahe0050/%x-%j.err.txt
##################################################################
# The default partition is 'general'. 
# Valid partitions are general, gpu and melfu
##SBATCH --partition=PARTITIONNAME
#
##################################################################
# Tell SLURM how long your job should run for as a hard limit. 
# My setting a shorter time limit, it is more likely that your
# job will be scheduled when attempting to backfill jobs. 
# 
# The current cluster-wide limit is 14 Days from Start of Execution.
# The timer is only active while your job runs, so if you suspend
# or pause the job, it will stop the timer.
#
# The command format is as follows: #SBATCH --time=DAYS-HOURS
# There are many ways to specify time, see the SchedMD Slurm 
# manual pages for more. 
#SBATCH --time=2-0
#
##################################################################
# How many tasks is your job going to run? 
# Unless you are running something that is Parallel / Modular or
# pipelined, leave this as 1. Think of each task as a 'bucket of
# resources' that stand alone. Without MPI / IPC you can't talk to 
# another bucket!
#
#SBATCH --ntasks=1
#
# If each task will need more that a single CPU, then alter this 
# value. Remember, this is multiplicative, so if you ask for 
# 4 Tasks and 4 CPU's per Task, you will be allocated 16 CPU's 
#SBATCH --cpus-per-task=8
##################################################################
# Set the memory requirements for the job in MB. Your job will be
# allocated exclusive access to that amount of RAM. In the case it
# overuses that amount, Slurm will kill the job. The default value is 
# around 2GB per CPU you ask for.
#
# Note that the lower the requested memory, the higher the
# chances to get scheduled to 'fill in the gaps' between other
# jobs. Pick ONE of the below options. They are Mutually Exclusive.
# You can ask for X Amount of RAM per CPU (MB by default).
# Slurm understands K/M/G/T For Kilo/Mega/Giga/Tera Bytes.
#
#SBATCH --mem-per-cpu=2G
# Or, you can ask for a 'total amount of RAM'. If you have multiple 
# tasks and ask for a 'total amount' like below, then SLURM will 
# split the total amount to each task evenly for you.
##SBATCH --mem=128G
##################################################################
# Change the number of GPU's required for you job. The most GPU's that can be 
# requested is 2 per node. As there are limited GPU slots, they are heavily 
# weighted against for Fairshare Score calculations. 
# You can request either a 'gpu:telsa_v100:X' or a 'gpu:x'
# 
# You can either request 0, or omit this line entirely if you 
# a GPU is not needed. 
#
#SBATCH --gres="gpu:0"
##################################################################
# Load any modules that are required. This is exactly the same as 
# loading them manually, with a space-separated list, or you can 
# write multiple lines.
# You will need to uncomment these.

module load Miniconda3/4.9.2 
module load python/3.9.6

##################################################################
# This example script assumes that you have already moved your 
# dataset to /scratch as part of your HPC Pre-Job preparations. 
# Its best to use the $TMP/$TMPDIR setup for you here
# to allow for the HPC to auto-clean anything you 
# leave behind by accident. 
# If you have a job-array and need a shared directory for 
# data on /local, you will need to manually cleanup that 
# directory as a part of your job script. 

# Example using the SLURM $BGFS Variable (the Parallel Filesystem)
#cd $BGFS
#cp -r /scratch/user/mahe0050/DE-analysis/1_trimmedData/q ./

##################################################################
# Enter the command-line arguments that you job needs to run. 

source ~/.bashrc 
conda activate BUSCOConda

module load Miniconda3/4.9.2 
module load python/3.9.6


# See notes file associated with usage: Output-BUSCO_Name2020.sh and BUSCO_all_v5.0.sh
# (directory creation and summary copied manually executed before this script)


# Inputs include:
# 	Short-read Trinity outputs for all 8 Kidney samples assembled from short reads
# 	Short-read Trinity output for one file of all 8 kidney sequencing concatenated and assembled into a single transcript file

# 	Isoseq3 long-read full list of non-redundant hq transcripts
# 	Isoseq3 long-read full list of transcript predicted open read frame coding regions
# 	Isoseq3 long-read representative transcripts of 'putative genes' based on coding sequence protein clustering, but including full-length including UTRs, with additional poly-a tail trimming


#------adjust these for your run-----

LINEAGE="vertebrata_odb10"

#------------------------------------

function error_exit
{
    echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
    exit 1
}


#go to the directory containing trimmed files to pull ID names
cd /scratch/user/mahe0050/BUSCO/

#mkdir BUSCO_summaries  || error_exit "$LINENO: dir error 1"

#cp G1_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G1_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G2_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G2_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G3_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G3_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G4_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G4_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G5_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G5_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G6_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G6_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G7_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G7_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp G8_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G8_KI_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp reference-transcripts_BUSCOout/short_summary.specific.vertebrata_odb10.reference-transcripts_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp ANGEL.cds_BUSCOout/short_summary.specific.vertebrata_odb10.ANGEL.cds_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp hq-fasta_BUSCOout/short_summary.specific.vertebrata_odb10.hq-fasta_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"
#cp trinity-all_BUSCOout/short_summary.specific.vertebrata_odb10.trinity-all_BUSCOout.txt BUSCO_summaries/.  || error_exit "$LINENO: copy error"

# (The path below is where the config files for conda-BUSCO environment are located)

python3 /home/mahe0050/.conda/envs/BUSCOConda/bin/generate_plot.py -wd /scratch/user/mahe0050/BUSCO/BUSCO_summaries  || error_exit "$LINENO: BUSCO error all"

echo "'all' BUSCO summary complete"

# *** *** *** 

#mkdir BUSCO_SR_summaries  || error_exit "$LINENO: dir error 2"

#cp G1_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G1_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G2_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G2_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G3_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G3_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G4_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G4_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G5_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G5_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G6_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G6_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G7_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G7_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp G8_KI_BUSCOout/short_summary.specific.vertebrata_odb10.G8_KI_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"
#cp trinity-all_BUSCOout/short_summary.specific.vertebrata_odb10.trinity-all_BUSCOout.txt BUSCO_SR_summaries/.  || error_exit "$LINENO: copy error"

python3 /home/mahe0050/.conda/envs/BUSCOConda/bin/generate_plot.py -wd /scratch/user/mahe0050/BUSCO/BUSCO_SR_summaries  || error_exit "$LINENO: BUSCO error SR"

echo "short read BUSCO summary complete"

# *** *** *** 

#mkdir BUSCO_LR_summaries  || error_exit "$LINENO: dir error 3"

#cp reference-transcripts_BUSCOout/short_summary.specific.vertebrata_odb10.reference-transcripts_BUSCOout.txt BUSCO_LR_summaries/.  || error_exit "$LINENO: copy error"
#cp ANGEL.cds_BUSCOout/short_summary.specific.vertebrata_odb10.ANGEL.cds_BUSCOout.txt BUSCO_LR_summaries/.  || error_exit "$LINENO: copy error"
#cp hq-fasta_BUSCOout/short_summary.specific.vertebrata_odb10.hq-fasta_BUSCOout.txt BUSCO_LR_summaries/.  || error_exit "$LINENO: copy error"

python3 /home/mahe0050/.conda/envs/BUSCOConda/bin/generate_plot.py -wd /scratch/user/mahe0050/BUSCO/BUSCO_LR_summaries  || error_exit "$LINENO: BUSCO error LR"

echo "long read BUSCO summary complete"


##################################################################
# Once you job has finished its processing, copy back your results 
# and ONLY the results to /scratch, then clean-up the temporary 
# working directory
# This command assumes that the destination exists

# cp -r /$BGFS/2_alignedData/trinity-all /scratch/user/mahe0050/DE-analysis/2_alignedData/

# cp -r /$BGFS/2_alignedData/trinity-sep /scratch/user/mahe0050/DE-analysis/2_alignedData/

# No need to cleanup $BGFS, SLURM handles the cleanup for you. 
# Just dont forget to copy out your results, or you will lose them!

##################################################################
```

(the ./BUSCO_SR_summaries and ./BUSCO_LR_summaries were generated so that short reads and long read data could be visualised separately but were ultimately not used)

These files were downloaded moved into an R working directory to generate the figure.  
The [R script](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%203%20-%20Assembly/busco_figure_edit.R) produced by the python command above was edited to allow for local directory structure and streamline the labelling and order of sample names.  

\newpage

![Busco Summary Statistics (BUSCO v5.0.0 run on database vertebrata_odb10. LR indicates transcript databases at different assembly steps, assembled from long-read sequencing data. SR indicates short-read sequencing sample data assembled with TRINITY)](./FigureCopyDir/busco_figure_edit.png)


***
\newpage

# Annotation (Chapter 4)

Completed on the long-read data from Chapter 3 (Section 2.1.3 above)

- pygmy.ANGEL.cds
- pygmy.ANGEL.pep
- pygmy.ANGEL.utr
  

## BLASTx

BLASTx searches were conducted on the dataset at this stage to include isoforms that are collapsed later on.  

BLASTx was run on the pygmy.ANGEL.cds file as it contains the predicted open reading frame sequence of nucleotides for translateable proteins. This was completed on the NECTAR machine.

```{bash, eval=F}
      # Package: blast 2.9.0, build Mar 11 2019 15:20:05
      # Uniprot sprot & trembl databases downloaded: 28/5/19     (note: trembl database not indexed or used)
      # Anolis .pep fasta files downloaded 12/11/19
            Anolis_carolinensis.AnoCar2.0.pep.all.fa
            Anolis_carolinensis.AnoCar2.0.pep.abinitio.fa


      # To format a database: #
      # makeblastdb –in mydb.fsa –dbtype nucl –parse_seqids
```

UniProt Swiss-Prot database:
```{bash, eval=F}
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/makeblastdb -in uniprot_sprot.fasta -parse_seqids -blastdb_version 5 -title "sprot" -dbtype prot -out sprot
```

Anolis carolinensis proteins database:
```{bash, eval=F}
    #AnoCar2.0.pep.all
    /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/makeblastdb -in Anolis_carolinensis.AnoCar2.0.pep.all.fa -parse_seqids -blastdb_version 5 -title "sprot" -dbtype prot -out AnoCar2.0.pep.all
```

Anolis Carolinensis database _ab initio_:
```{bash, eval=F}
      #AnoCar2.0.pep.abinitio
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/makeblastdb -in Anolis_carolinensis.AnoCar2.0.pep.abinitio.fa -parse_seqids -blastdb_version 5 -title "sprot" -dbtype prot -out AnoCar2.0.pep.abinitio
```

Various searches were run in a screen to examine the effects of parameters and databases. Final searches are as listed below.  

Searches included all databases listed above and variations of the parameters:
-max_target_seqs 5 or -max_target_seqs 1
-max_hsps 5 or -max_hsps 1
-evalue 0.00001 or -evalue 1e-10

\newpage

The NCBI Blast results most often referenced in the thesis are final parameters blastx -fmt6 -maxtarget1 -maxhsps1 -eval0.00001 against the UniProt Swiss-Prot database.  
```{bash, eval=F}
      # uniprot sprot
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/blastx -db /mnt/Prog/blast/blastdb/sprot/sprot -query /mnt/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds -out pygmy.ANGEL_blastx_sprot-maxtarg1-maxhsp1.out -outfmt 6 -max_target_seqs 1 -max_hsps 1 -evalue 0.00001 -num_threads 3
```

```{bash, eval=F}
      #AnoCar2.0.pep.all
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/blastx -db /mnt/Prog/blast/blastdb/AcarProt/AnoCar2.0.pep.all/AnoCar2.0.pep.all -query /mnt/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds -out pygmy.ANGEL_blastx_AnoCar.pep.all-maxtarg1-maxhsp1.out -outfmt 6 -max_target_seqs 1 -max_hsps 1 -evalue 0.00001 -num_threads 3
```

All searches except the below were output in tab separated -outfmt 6. Due to import requirements an -outfmt 5 .xml file was required for results to be visualised in the program BLAST2GO (-max_target_seqs 5 -max_hsps 5 -evalue 0.00001).  
```{bash, eval=F}
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/blastx -db /mnt/Prog/blast/blastdb/AcarProt/AnoCar2.0.pep.all/AnoCar2.0.pep.all -query /mnt/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds -out pygmy.ANGEL_blastx_AnoCar.pep.all-BLAST2GO -outfmt 5 -max_target_seqs 5 -max_hsps 5 -evalue 0.00001 -num_threads 4

      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/blastx -db /mnt/Prog/blast/blastdb/sprot/sprot -query /mnt/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds -out pygmy.ANGEL_blastx_sprot-BLAST2GO -outfmt 5 -max_target_seqs 5 -max_hsps 5 -evalue 0.00001 -num_threads 4

```

The above and other trial searches are all listed in the file [IsoSeq analysis post-ANGEL blast notes.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%204%20-%20Annotation/IsoSeq%20analysis%20post-ANGEL%20blast%20notes.sh) 


\newpage

## Gene Name IDs Assigned to Transcript ID

The ANGEL predicted open reading frame transcripts (Section 2.1.3) were searched against the UniProt Swiss-Prot database using BLASTx with parameters: -max_target_seqs 1 -max_hsps 1 -evalue 0.00001) (Section 3.1 above).  
The BLASTx results are uploaded [here](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Additional%20Supplementary%20Files/pygmy.ANGEL_blastx_sprot-maxtarg1-maxhsp1.out).  

Protein IDs from the BLASTx results were uploaded to the UniProt [Retrieve/ID mapping tool](https://www.uniprot.org/uploadlists/) and mapped to Gene names. This produced a [list](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Additional%20Supplementary%20Files/pygmy.ANGEL_blastx_sprot-maxtarg1-maxhsp1%3D%3DGene%20Name%20IDs.txt) of unique UniProt Swiss-Prot identifiers and their corresponsing gene name ID.


### Full Transcript Putative Gene Reference File

Using dplyr the BLAST results and these gene ID mappings were joined and filtered to create a reference file of transcript IDs assigned both a UniProt Swiss-Prot database protein ID, and a gene name.  

The BLASTx results were imported:

```{r, warning=FALSE, Message=FALSE, echo = TRUE}
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters")

Unisprot.BLASTx.out <- read.csv("pygmy.ANGEL_blastx_sprot-maxtarg1-maxhsp1.out", sep = "\t", header = FALSE)
head(Unisprot.BLASTx.out)
nrow(Unisprot.BLASTx.out)
```

Column Headers Renamed to match BLAST output format 6:
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Unisprot.BLASTx.out <- Unisprot.BLASTx.out %>% rename(TranscriptID = V1, UniprotID = V2, pident = V3, length = V4, mismatch = V5, gapopen = V6, qstart = V7, qend = V8, sstart = V9, send = V10, evalue = V11, bitscore = V12)

colnames(Unisprot.BLASTx.out)
#head(Unisprot.BLASTx.out)
#tail(Unisprot.BLASTx.out)
```

Gene ID mapping list for Uniprot IDs were imported:
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.to.gene <- read.csv("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/pygmy.ANGEL_blastx_sprot-maxtarg1-maxhsp1==Gene Name IDs.txt", sep = "\t", header = TRUE)

head(BLASTx.to.gene)
```

Column Headers were renamed:
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.to.gene <- BLASTx.to.gene %>% rename(UniprotID = From, Gene = To)
colnames(BLASTx.to.gene)
```

Although The UniProt database returns a non-redundant, unique list, conventions of gene and protein names of different species means there is some duplication when treated as case sensitive. Here all protein IDs will be made upper case, and all gene IDs will be made lower case.
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.to.gene <- BLASTx.to.gene %>% mutate(UniprotID = toupper(UniprotID))

BLASTx.to.gene <- BLASTx.to.gene %>% mutate(Gene = tolower(Gene))

head(BLASTx.to.gene)
nrow(BLASTx.to.gene)
```

Note that nrow = 7221, now the duplicated rows will be filtered out:
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.to.gene %>% count(UniprotID) %>% filter(n > 1)
```
UniProt ID P32969 corresponds to gene IDs:	RPL9, RPL9P7, RPL9P8, and RPL9P9. As these data are not specifically assigned to any one of these 4 genes as they all have the same protein ID, and to avoid duplication of transcripts on joining, this will be simplified to one entry of RPL9.

```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.to.gene <- BLASTx.to.gene %>% distinct(UniprotID, .keep_all= TRUE)
nrow(BLASTx.to.gene)
BLASTx.to.gene %>% group_by(Gene) %>% summarize(n=n())
```
There are now 7218 Unique Uniprot IDs listed and after making case consistent, 6343 unique Gene IDs listed. Multiple Protein IDs map to the same Gene ID. This may be due to data sourced from a variety of species' and particular transcripts returning a BLAST hit for gene orthologues, or gene haplotypes. These putative Gene IDs are for reference only, and protein IDs will not be removed from transcript data, so no information will be lost.  

Assign gene ID to the BLAST result by joining based on UniProt ID
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join <- left_join(Unisprot.BLASTx.out, BLASTx.to.gene, by ="UniprotID")
#head(BLASTx.gene.join)
```

```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join <- relocate(BLASTx.gene.join, Gene, .before = pident)
#head(BLASTx.gene.join)
#tail(BLASTx.gene.join)
```


#### Manipulation of 'BLASTx.gene.join_filt_clustered' to Match Reference Files

 - The fasta file which contains _only_ protein coding regions (and sequences which were part of the input for the BLASTx search) has the characters "|m.*" at the end of transcript IDs
 - The fasta file containing the full length of these transcripts corresponsing to the clustered protein coding regions (above), and which was used as the reference file for Kallisto does not have this name extension
 
In order to be able to filter this file based on either naming convention and compare to BLASTx outputs, genes of interest lists, and gene expression results:
Here a corresponding column with the "|m.*" removed is added

Duplicate the ID column
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_split <- cbind(BLASTx.gene.join, replicate(1,BLASTx.gene.join$TranscriptID))
BLASTx.gene.join_split <- rename(BLASTx.gene.join_split, TranscriptID_2 = "replicate(1, BLASTx.gene.join$TranscriptID)")
names(BLASTx.gene.join_split)
BLASTx.gene.join_split <- relocate(BLASTx.gene.join_split, "TranscriptID_2", .before = "UniprotID")
#head(BLASTx.gene.join_split)

```


Split the Transcript ID column based on the last "|" and return as a data frame
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_split2 <- lapply(stri_split_regex(stri_reverse(BLASTx.gene.join_split$TranscriptID_2), pattern = '[|\\s]+', n = 2), stri_reverse)
BLASTx.gene.join_split2 <- setNames(data.table::transpose(BLASTx.gene.join_split2)[2:1], c('output1', 'output2'))
BLASTx.gene.join_split2 <- as.data.frame(c(list(input = BLASTx.gene.join_split$TranscriptID_2), BLASTx.gene.join_split2))
#head(BLASTx.gene.join_split2)
```

Rename Columns and Join the truncated columns back into the original dataframe based on TranscriptID_2
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
names(BLASTx.gene.join_split2)
colnames(BLASTx.gene.join_split2) <- c("TranscriptID_2","TranscriptID_3","TranscriptID_4")
#head(BLASTx.gene.join_split2)

BLASTx.gene.join_split3 <- full_join(BLASTx.gene.join_split, BLASTx.gene.join_split2, by ="TranscriptID_2")
#head(BLASTx.gene.join_split3)
```

Check: (they all have the same numbers of rows and no data was lost)
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
nrow(BLASTx.gene.join)
nrow(BLASTx.gene.join_split)
nrow(BLASTx.gene.join_split2)
nrow(BLASTx.gene.join_split3)
```

Remove the now unneeded duplicate 'TranscriptID_2' column and 'TranscriptID_4' column 
Reorder columns so that TranscriptID_3 appears on the left next to the full TranscriptID
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_split4 <- subset(BLASTx.gene.join_split3, select = -c(TranscriptID_2,TranscriptID_4))
BLASTx.gene.join_split4 <- relocate(BLASTx.gene.join_split4, TranscriptID_3, .before = UniprotID)
#head(BLASTx.gene.join_split4)
```

Reorder the rows so that the Data is arranged by Gene in alphabetical order (and so that transcripts coresponding to the same gene are listed together).
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_split4 <- BLASTx.gene.join_split4 %>% arrange(Gene)
#head(BLASTx.gene.join_split4)
nrow(BLASTx.gene.join_split4) #confirming that number of rows remains consistent. The initial BLASTx result has 12894 rows, and 12894 rows remain.
```

The aforementioned 6343 genes are all present in this dataset, with the additional record representing rows with no Gene ID assigned to Protein ID. 
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_split4 %>% group_by(Gene) %>% summarize(n=n())
```

Write the output to .csv
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
write.csv(BLASTx.gene.join_split4, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/BLASTx.gene.join_split4.csv", quote = FALSE, row.names = FALSE, col.names = TRUE)
```

Remove all of the rows where Gene = NA
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_filt <- BLASTx.gene.join_split4 %>% filter(!is.na(Gene))
head(BLASTx.gene.join_filt)
tail(BLASTx.gene.join_filt)
nrow(BLASTx.gene.join_filt)
```
See above, 12532 transcripts remain which were mapped to one of the 6343 Gene IDs.
 
Write the output to .csv
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
write.csv(BLASTx.gene.join_filt, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/BLASTx.gene.join_filt.csv", quote = FALSE, row.names = FALSE, col.names = TRUE)
```


### Putative Genes for Clustered Transcripts used as the Gene Expression Analysis Reference 

The above list of transcripts represent all transcript isoforms assembled for _T. adelaidensis_ with a predicted open reading frame determined by ANGEL (Section 2.1.3). After further clustering using translated protein sequences in CD-HIT (Section 2.1.4-7), a smaller reference set of transcripts were used as the reference for gene expresion analysis (Section 4.1).  

The above list will be filtered based on the same list of cluster representatives used in Section 2.1 to create a smaller list of all BLAST results relating to the transcripts used as gene expression references.  

Import the list of unique clustered transcripts that were used as the reference for gene expression analysis. Note that in this case, data after the ANGEL step are being sub-set, whereas in Section 2.1.7 full length fasta sequences of corresponding transcripts were desired. As noted in section 2.1 a few full length transcripts give rise to more than one transcript isoform listed here, and thus may be assigned more than one putative gene ID.  

ClstrTranscriptID is already loaded:
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
head(clstrTranscriptID)
```

Filter the BLASTx results with gene name based on the transcript references used for the expression analysis (i.e. the clustered transcript file).
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_filt_clustered <- BLASTx.gene.join_filt %>% filter(TranscriptID %in% clstrTranscriptID)
#head(BLASTx.gene.join_filt_clustered)
nrow(BLASTx.gene.join_filt_clustered)
```
Note: of the initial 13882 sequences in the predicted open read frame set submitted for a BLASTx search, only 12602 query sequences returned a match, and not all protein ID matches returned a Gene ID match. Coupled with some transcripts that may have been identified to putative gene ID being removed at the CD-HIT clustering step, the total remaining number of transcripts with a coresponding Protein ID and Gene ID here is expected to be less than the clustered reference transcript dataset containing 9813 unique full-length transcripts.  
Ninety four full length transcripts produced more than one isoform in predicted open read frame which was retained at the clustering step, so there is also the possibility that duplicate entries here apply to a single full length 'master' transcript.  

Here 8861 transcripts in predicted open read frame have been assigned both a protein ID and putative gene ID, from a total list of 9813 corresponding full length transcripts which were used as a reference in later gene expression analysis.  



### Presence of Identified "Genes of Interest" in the _T. adelaidensis_ Transcript Set

In order to generate a list of potential "genes of interest" and narrow down the focus of analysis, an NCBI gene database search was conducted aiming for genes identified in reptiles associated with renal function, water homeostasis and heat regulation.

The final NCBI Gene database search was accessed on on June 14, 2020 including the following terms:  
 - (Sauria[Organism]) AND (Aquaporin OR Bile OR Dehydration OR Diffus* OR Excretion OR Filt* OR Fluid OR (Heat AND Stress) OR Heat Shock OR Heat Stress OR Hibernation OR Homeostasis OR Ion Channel OR Ion Transport OR Kidney OR Membrane And (Potential OR Permeability OR Pore) OR Metabolic OR Osmo* OR Permeability OR Ph Balance OR Renal OR Solute OR Stress OR Temperature OR Thermal OR Uric OR Water OR Water Retention OR Water Permeability OR Water Transport)  
 
These [full search results](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Additional%20Supplementary%20Files/2020-06-14_Sauria-FINAL-gene_result.txt) were exported and refined in excel with care taken to manually preserve gene symbol data fields.
Note: summary counts of genes IDs assigned to transcripts was initially conducted in excel (Appendix 3). Manual manipulation of the exported database data was conducted to summarise genes identified, and to create a list of unique genes returned by the search term. Duplicate gene entries were prioritised by taxa and a list of 993 genes of particular interest to this study were identified.  

Excel was used to create initial summary statistics of the taxa and genes returned, as well as record information retained at each step of manual filtering.  

Excel is not ideal for the manipulation of lists of gene names this represents preliminary data exploration, so great care was taken to ensure gene IDs integrity was maintained and all further filtering was conducted in R below. The following analysis was conducted in to subset the sequenced _T. adelaidensis_ transcript lists based on putative gene annotation. 

Import the reference "genes of interest" list built from an NCBI database search (Created for reference in Chapter 4, outlined in Appendix 3). 
[This imported list](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Additional%20Supplementary%20Files/Returned%20organisms%202020-06-15%20Sauria-RM-dup-loc%20FULL.csv) is after initial duplicate filtering in excel as well as adding a column for gene sumbol/ID in all lower case. It contains 993 unique gene symbols for the most favoured taxa (as per the list in Appendix 3).
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Sauria_2020.06.14 <- read.csv("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/Returned organisms 2020-06-15 Sauria-RM-dup-loc FULL.csv", sep = ",", header = TRUE,)
Sauria_2020.06.14 <- Sauria_2020.06.14 %>% select("tax_id",	"Org_name.new_name",	"Org_name",	"GeneID",	"Symbol",	"Symbol.CAPS",	"Aliases",	"description",	"other_designations")
nrow(Sauria_2020.06.14)
head(Sauria_2020.06.14)
```

The "Gene Symbol" column output by the NCBI database matches the format of "gene names" output by the conversion of UniProt Protein IDs generated 

Check Gene ID title case for non-unique entries
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Sauria_2020.06.14 %>% count(Symbol.CAPS) %>% filter(n > 1)
```

Rename column Symbol.CAPS to Gene so column can be compared to the Gene column in BLASTx.gene.join_filt, make this column characters to allow joins
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Sauria_2020.06.14 <- rename(Sauria_2020.06.14, "Gene" = "Symbol.CAPS")
Sauria_2020.06.14$Gene <− as.character(Sauria_2020.06.14$Gene)
#head(Sauria_2020.06.14)
```

Create a larger summary file of the transcripts which have a BLASTx result. All BLASTx results will be retained, but there is no need to retain information for 'genes of interest' which were not identified in this dataset. 
This will be used later to identify information on transcripts which may be identified in the gene expression analysis. 
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_Database.Full.PBT.Summary <- left_join(BLASTx.gene.join_filt, Sauria_2020.06.14, by = "Gene")
nrow(BLASTx.gene.join_Database.Full.PBT.Summary)
head(BLASTx.gene.join_Database.Full.PBT.Summary)
```

Export this file as a .csv
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
write.csv(BLASTx.gene.join_Database.Full.PBT.Summary, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/BLASTx.gene.join_Database.Full.PBT.Summary.csv", quote = FALSE, row.names = FALSE, col.names = TRUE)
```

The number of transcripts which returned a BLASTx result for each gene was already counted in a previous section. Make this a dataframe.
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Gene_transcript_count <- BLASTx.gene.join_split4 %>% group_by(Gene) %>% summarize(n=n())
head(Gene_transcript_count)
```

Filter the genes of interest list based on Gene IDs which match to a BLASTx result from the full transcript list
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Sauria_2020.06.14_Transcripts.Present.Full <- semi_join(Sauria_2020.06.14, BLASTx.gene.join_filt, by = "Gene")
#head(Sauria_2020.06.14_Transcripts.Present.Full)
nrow(Sauria_2020.06.14_Transcripts.Present.Full)
```

Add the counts of number of transcripts matching each 'gene of interest' as a new column to the genes of interest table, reorder columns.
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Sauria_2020.06.14_Transcripts.Present.Count <- left_join(Sauria_2020.06.14_Transcripts.Present.Full, Gene_transcript_count, by = "Gene")
Sauria_2020.06.14_Transcripts.Present.Count <- rename(Sauria_2020.06.14_Transcripts.Present.Count, "Transcript_Count" = "n")

Sauria_2020.06.14_Transcripts.Present.Count <- relocate(Sauria_2020.06.14_Transcripts.Present.Count, description, .before = Aliases)
Sauria_2020.06.14_Transcripts.Present.Count <- relocate(Sauria_2020.06.14_Transcripts.Present.Count, Transcript_Count, .before = Aliases)
Sauria_2020.06.14_Transcripts.Present.Count <- select(Sauria_2020.06.14_Transcripts.Present.Count, -Org_name.new_name) #this column was used as alphabetical factors in excel to sort the organisms by category so that the desired filtering based on taxa group could be achieved when duplicate gene database results were removed. It is no longer needed 
head(Sauria_2020.06.14_Transcripts.Present.Count)
```


Export this faile as a .csv
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
write.csv(Sauria_2020.06.14_Transcripts.Present.Full, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/Sauria_2020.06.14_Transcripts.Present.Count.csv", quote = FALSE, row.names = FALSE, col.names = TRUE)
```


Filter the genes of interest list based on Gene IDs which match to a BLASTx result from the clustered transcript list used as a reference for gene expression analysis
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
Sauria_2020.06.14_Transcripts.Present.Clustered <- semi_join(Sauria_2020.06.14, BLASTx.gene.join_filt_clustered, by = "Gene")
nrow(Sauria_2020.06.14_Transcripts.Present.Clustered)
```
Note: only a single gene that was both identified through the BLASTx search _and_ identified as a 'gene of interest' in the database search was lost in this filtering step.


filter the Full transcript list based on Gene symbol identified in NCBI database search
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
nrow(BLASTx.gene.join_filt)
BLASTx.gene.join_Database.Present <- semi_join(BLASTx.gene.join_filt, Sauria_2020.06.14, by = "Gene")
nrow(BLASTx.gene.join_Database.Present)
```
Of the 12532 transcripts which returned a BLASTx result for their predicted coding region, 955 are represented in the 'genes of interest' dataset created from an NCBI gene database search of terms relevant to renal function and water homeostasis. 


Export this faile as a .csv
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
write.csv(BLASTx.gene.join_Database.Present, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/BLASTx.gene.join_Database.Present.csv", quote = FALSE, row.names = FALSE, col.names = TRUE)
```


 
\newpage

## BLAST 2 GO

BLAST2GO (Within [OmicsBox – Bioinformatics Made Easy, BioBam Bioinformatics](https://www.biobam.com/omicsbox)) was used to perform a gene ontology analysis of the above BLASTx results. 

<font size="0.5">Götz S., Garcia-Gomez JM., Terol J., Williams TD., Nagaraj SH., Nueda MJ., Robles M., Talon M., Dopazo J. and Conesa A. (2008). High-throughput functional annotation and data mining with the Blast2GO suite. Nucleic acids research, 36(10), 3420-35.</font>

For import into the program a BLASTx search with the output set to .xml was performed using the fasta file containing the predicted open reading frame compared to the _Anolis carolinensis_ proteins database, and a second search to the UniProt Swiss-Prot database (-max_target_seqs 5 -max_hsps 5 -evalue 0.00001).

### _Anolis carolinensis_ Protein Database BLASTx

Initial data exploration was performed on the _Anolis carolinensis_ proteins database on the assumption that it may return fewer, more relevant matches to _T. adelaidensis_. Comparison to GO databases and visualisation was all completed within a Windows x64 v1.2.4 build of the OmicsBox program and figures were exported as below.  

BLASTx was run on the eRSA NECTAR machine, using BLAST 2.9.0 against Anolis protein database AnoCar 2.0 pep downloaded on 12/11/19.  
Due to import requirements for Blast2GO output format 5 was used instead of output format 6 which is used in all other sections above.
The BLASTx query was:

```{bash, eval=F}
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/blastx -db /mnt/Prog/blast/blastdb/AcarProt/AnoCar2.0.pep.all/AnoCar2.0.pep.all -query /mnt/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds -out pygmy.ANGEL_blastx_AnoCar.pep.all-BLAST2GO -outfmt 5 -max_target_seqs 5 -max_hsps 5 -evalue 0.00001 -num_threads 4
```

With maximum target sequences = 5, Maximum hits per sequence = 5, and a required e-value of 0.00001  

The BLAST output was imported into OmicsBox, where results were then mapped and annotated to GO terms.

\newpage
 
![Summary data distribution using the Anolis BLAST database. The file of predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) contains 13,882 unique sequences representing predicted open reading frame of transcripts: 12,602 of these sequences produced >1 BLASTx hit with Mapping and GO Annotation, 243 produced a BLASTx hit and mapping only, 32 produced a BLAST hit only, and 1005 did not produce BLASTx hits ](./FigureCopyDir/GO_Anolis/omicsbox_statistics_20191224_143216.png)

\newpage
 
![Percentage of sequences with length (x) that were annotated (Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the Anolis genome formatted to a BLAST database)](./FigureCopyDir/GO_Anolis/omicsbox_statistics_20191224_144456.png)

\newpage

![Top 20 annotation results (at level 8) for each category BP - Biological Process, MF - Molecular Function, and CC - Celular Component  (Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the Anolis genome formatted to a BLAST database)](./FigureCopyDir/GO_Anolis/omicsbox_statistics_20191224_165649.png)

\newpage

![Pie chart of proportion of sequences annotated to Level 2 of the Biological Process category (Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the Anolis genome formatted to a BLAST database)](./FigureCopyDir/GO_Anolis/omicsbox_statistics_20191227_013949.png)

\newpage

![Wordcloud of all GO terms annotated based on _T. adelaidensis transcript BLASTx results against the Anolis genome, all GO levels combined  (Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the Anolis genome formatted to a BLAST database)](./FigureCopyDir/GO_Anolis/omicsbox_wordcloud_20191224_155743.png)

Some key words from this wordcloud were used to supplement terms included in the NCBI Gene database search described in section 3.2.3 above and outlined in Appendix 3.  

The Anolis database was analysed first as it is the taxonomically closest available genome for comparison. However in order to not limit results, and to maintain some consistency with the annotation done in section 3.2 above, the entire UniProt Swiss-Prot database BLASTx results were analysed further below.

***

\newpage
 
### UniProt Swiss-Prot Protein Database BLASTx

Final GO data analysis as included in Chapter 4 was performed on the BLASTx search using the Swiss-Prot proteins database in order to remain consistent with other annotation comparisons performed here, and due to a larger number of returned results. This analysis was completed within a Windows x64 v2.0.36 build of the OmicsBox program and figures were exported as below.  

BLASTx was run on the eRSA NECTAR machine, using BLAST 2.9.0 against the UniProt Swiss-Prot database downloaded on 28/05/19.  
Due to import requirements for Blast2GO output format 5 was used instead of output format 6 which is used in all other sections above.
The BLASTx query was:

```{bash, eval=F}
      /mnt/Prog/blast/ncbi-blast-2.9.0+/bin/blastx -db /mnt/Prog/blast/blastdb/sprot/sprot -query /mnt/IsoSeq-analysis/data/ANGEL/pygmy.ANGEL.cds -out pygmy.ANGEL_blastx_sprot-BLAST2GO -outfmt 5 -max_target_seqs 5 -max_hsps 5 -evalue 0.00001 -num_threads 4
```

With maximum target sequences = 5, Maximum hits per sequence = 5, and a required e-value of 0.00001. 
Note this limits the results less than the search used in section 3.1 above, which limited maximum target sequences to 1, and maximum hits per sequence to 1 so that the top result could easily be retrieved manually. As evident in the statistics below, OmicsBox is capable of interpreting multiple hits per query and mapping accordingly without duplicating data beyond the input 13,882 input sequences.

Blastx results were imported into the program Omics Box, where results were then mapped and annotated to GO terms.


\newpage


![Summary data distribution using the UniProt Swiss-Prot protein BLAST database. The file of predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) contains 13,882 unique sequences representing predicted open reading frame of transcripts: 12672 of these sequences produced >1 BLASTx hit with Mapping and GO Annotation, 186 produced a BLASTx hit and mapping only, 19 produced a BLAST hit only, and 1005 did not produce BLASTx hits ](./FigureCopyDir/GO_UniProt/data_distribution_pygmy_angel_blastx_sprot_blast2go_20220308_174621.png)


\newpage


![Summary of number of sequences with length (x) The file of predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) has an average sequence length of 1073bp](./FigureCopyDir/GO_UniProt/number_of_sequences_with_length_x_pygmy_angel_blastx_sprot_blast2go_20220308_174749.png)


\newpage


![Percentage of sequences with length (x) (Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the UniProt Swiss-Prot database formatted to a BLAST database))](./FigureCopyDir/GO_UniProt/4.Annotation_stats/Percentage of Sequences with LengthxAnnotated pygmy.ANGEL_blastx_sprot-BLAST2GO.png)


\newpage


![Number of GO terms annotated to sequences with length (x) (Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the UniProt Swiss-Prot database formatted to a BLAST database)](./FigureCopyDir/GO_UniProt/4.Annotation_stats/Number of GO-terms for Sequences with Lengthxpygmy.ANGEL_blastx_sprot-BLAST2GO.png)


\newpage


![GO level distribution for number of annotated sequences of Predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL) as BLAST query against the UniProt Swiss-Prot database formatted to a BLAST database](./FigureCopyDir/GO_UniProt/4.Annotation_stats/GO-Level Distribution pygmy.ANGEL_blastx_sprot-BLAST2GO.png)


\newpage


![Top 20 GO annotations by GO level 5 for the GO categories Biological Process (BP, Molecular Function (MF) and Cellular Component (CC) for _T. adelaidensis_ transcripts in predicted open reading frame (pygmy.ANGEL) using BLASTx results against the UniProt Swiss-Prot database](./FigureCopyDir/GO_UniProt/4.Annotation_stats/GO Distribution by Level/go_distribution_by_level_5_top_20_20220308_184050.png)


\newpage


![Top 20 GO annotations by GO level 6 for the GO categories Biological Process (BP, Molecular Function (MF) and Cellular Component (CC) for _T. adelaidensis_ transcripts in predicted open reading frame (pygmy.ANGEL) using BLASTx results against the UniProt Swiss-Prot database](./FigureCopyDir/GO_UniProt/4.Annotation_stats/GO Distribution by Level/go_distribution_by_level_6_top_20_20220308_184141.png)


\newpage


![Percent of annotations by number of annotated sequences for GO level 6 (Biological Process -BP) for _T. adelaidensis_ transcripts in predicted open reading frame (pygmy.ANGEL) using BLASTx results against the UniProt Swiss-Prot database](./FigureCopyDir/GO_UniProt/5.Combined_GO/graph_level_6_pie_chart_of_seqs_biological_process_20220308_190404.png)


\newpage


![Top 20 GO annotations by GO level 7 for the GO categories Biological Process (BP, Molecular Function (MF) and Cellular Component (CC) for _T. adelaidensis_ transcripts in predicted open reading frame (pygmy.ANGEL) using BLASTx results against the UniProt Swiss-Prot database](./FigureCopyDir/GO_UniProt/4.Annotation_stats/GO Distribution by Level/go_distribution_by_level_7_top_20_20220308_184303.png)


\newpage


![Top gene ontology terms by number of annotated sequences for all GO levels in the biological process category for predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL)](./FigureCopyDir/GO_UniProt/5.Combined_GO/sequence_distribution_biological_process_20220308_191452.png)


\newpage


![Top gene ontology terms by number of annotated sequences for all GO levels in the molecular function category for predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL)](./FigureCopyDir/GO_UniProt/5.Combined_GO/sequence_distribution_molecular_function_20220308_191509.png)


\newpage


![Top gene ontology terms by number of annotated sequences for all GO levels in the cellular component category for predicted _T. adelaidensis_ transcripts in open reading frame (pygmy.ANGEL)](./FigureCopyDir/GO_UniProt/5.Combined_GO/sequence_distribution_cellular_component_20220308_191520.png) 


\newpage

\clearpage


## Summary 

**15,729** Total transcript isoforms retained in initial cleaning and clustering of redundant isoforms.  

**13,882** sequences in a predicted open reading frame.  

**9,907** clusters identified based on translated proteins of predicted open reading frame.  

**9,813** full length transcripts have been subset into a reference file as the longest representatives of one or more transcript clusters.  

**12,602** of the 13,882 sequences in a predicted open reading frame were successfully annotated to a BLASTx result, protein ID, and gene ontology category when compared to the Anolis protein database. 

**12,672 ** of the 13,882 sequences in a predicted open reading frame were successfully annotated to a BLASTx result, protein ID, and gene ontology category when compared to the UniProt Swiss-Prot protein database

***


\newpage


# Gene Expression Analysis (Chapter 5)


##### Long-reads (Reference Index) - Previous Analysis:
- Isoseq3 pipe
- Collapsing of isoforms & generating fasta of longest transcripts in predicted ORFs (Cogent/ANGEL)
- BLASTx search of coding sequence file for the longest transcripts in predicted ORFs
- Grouping of transcript clusters/putative gene families (CD-HIT)
- Longest representative transcript collapsed into a reference .fasta (R & Shell scripts)
- As this longest transcript was taken from earlier in the pipeline, persisting poly-a tails were trimmed from the reference .fasta file (Perl script)

##### Short-reads - Previous Analysis:
- FASTQC quality assessment
- Cutadapt adapter and quality trimming
- FASTQC quality assessment
(note: further TRINITY assembly not used here for counts)

##### Both Datasets are Combined in this Analysis:
* Transcript counts per million estimated using Kallisto
    * Clean short-reads (above) counted against reference index generated from long-read .fasta (above)
* Kallisto output imported into R for analysis using EdgeR


## Kallisto

Short-reads have already been trimmed as per Chapter 3 (Section 2.2.2 above). These cleaned sequences were imported directly into Kallisto.  

Kallisto was run on the Flinders' HPC Deep Thought and files were moved accordingly, keeping directories relative locations intact.  

Kallisto version version 0.48.0 was installed using Miniconda3/4.9.2 (in the same environment as Cutadapt used in Chapter 3) as below

```{bash, eval=F}
      # Open Deep Thought and navigate to the /scratch disc where all data and scripts are located.
      cd /scratch/user/mahe0050
      
      #Load Miniconda V4.9.2
      module load Miniconda3/4.9.2
      
      # Create a conda environment "ShortConda"
      #conda create -n ShortConda
      
      ## Package Plan ##
      # environment location: /home/mahe0050/.conda/envs/ShortConda
      #
      # To resolve init error initialise conda with bash (already an available shell, when completed states no change to paths etc.
      #     $ conda init bash
      #
      # To activate this environment, use
      #     $ conda activate ShortConda
      #
      # To deactivate an active environment, use
      #     $ conda deactivate
      
      
      #Activate this environment
      source ~/.bashrc 
      conda activate ShortConda
      
      #Check bioconda channels:
      conda config --add channels bioconda
      conda config --add channels conda-forge
      conda config --add channels defaults
      
      # cutadapt version 4.1
      conda install -c bioconda cutadapt

      # kallisto version 0.48.0
      conda install kallisto
      
      # ***
      # Proceed with Kallisto
      # ***
      
      cd /scratch/user/mahe0050/DE-analysis/bash
      source ~/.bashrc 
      conda activate ShortConda
      
      #File: kallisto-clustr-clean.sh
      #uses the trimmed reads to q 5
      
      # called in SLURM script:
      sbatch Slurm-Kallisto_DT-2022.sh
      
      
      # ***
      # Kallisto outputs are then analysed in R studio
      # ***
```

The file reference_transcripts.1Lv.clean.fasta created in the subsetting section of Chapter 3's methods (Section 2.1.7 above) was used as the index.  
All of this was achieved using the following

The following [Slurm](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%205%20-%20Gene%20Expression/Slurm-Kallisto_DT-2022.sh) script was used to run [kallisto-clstr-clean.sh](https://github.com/Carmel-src/T.adelaidensis_SuppInfo/blob/main/Chapter%205%20-%20Gene%20Expression/kallisto-clstr-clean.sh) as below.  

Creation of the index is 'commented out' in the below script as it only needs to be run once and was completed during troubleshooting.

Run kallisto:
```{bash, eval=F}
      #!/bin/bash
      #
      #
      # This script is used to estimate transcript counts using Kallisto - using HiSeq reads against an IsoSeq reference. 
      # This reference was created from Iso-Seq data, clustered by CDHit (based on pygmy.ANGEL.pep, cutoff at .99, job 1598936109) and subset to include the representative transcript for each cluster using dplyR and seqtk.
      # The paths in the script assume that a specific directory structure has been set up.
      # Kallisto must be run on paired samples separately
      # 
      # usage on flinder's Deep Thought Machine via SLURM
      # usage from within: conda activate ShortConda
      #
      # Carmel Maher & Terry Bertozzi
      # Dec 2019, last altered Oct 2022
      
      
      #------adjust these for your run-----
      
      # module load Miniconda3/4.9.2
      # conda activate ShortConda
      # conda install kallisto
      # kallisto, version 0.48.1
      
      
      # ReferenceDIR=/scratch/user/mahe0050/IsoSeq-analysis/data/Seqtk #contains reference_transcripts.1Lvclean.fasta
      # KallistoEXE=/mnt/Prog/miniconda3/envs/anaKallisto/bin
      KallistoDIR=/scratch/user/mahe0050/DE-analysis/2_alignedData/kallisto-clstr-cleanq
      ReadDIR=/scratch/user/mahe0050/DE-analysis/1_trimmedData/q #contains trimmed R1, R2
      
      #------------------------------------
      
      function error_exit
      {
          # Exit function due to fatal error
          # Accepts 1 arg:
          # string - descriptive error message
      
          echo "${PROGNAME}: ${1:-"Unknown error"}" 1>&2
          exit 1
      }
      
      # *** make the index: ***
       cd /scratch/user/mahe0050/DE-analysis/2_alignedData/kallisto-clstr-cleanq
      
      #create the index
      #the below contains the representative transcript for clustered isoforms with (most) of the poly-a tails trimmed:
      # kallisto index -i reference_transcripts.1Lv.clean.idx /scratch/user/mahe0050/IsoSeq-analysis/data/Seqtk/reference_transcripts.1Lv.clean.fasta || error_exit "$LINENO: kallisto index error"
      
      # ------------
      
      ## [build] loading fasta file /scratch/user/mahe0050/IsoSeq-analysis/data/Seqtk/reference_transcripts.1Lv.clean.fasta
      ## [build] k-mer length: 31
      ## [build] counting k-mers ... done.
      ## [build] building target de Bruijn graph ...  done 
      ## [build] creating equivalence classes ...  done
      ## [build] target de Bruijn graph has 59186 contigs and contains 17770125 k-mers 
      
      # ------------
      
      
      # need to be in the input directory for for file
      cd $ReadDIR
      
      for file in *R1_cleanq.fq.gz
      do
      	FILESTEM=${file%_*}
      	#this FILESTEM only cuts to _clean, the _R1 is included
      	FILESTEM=${FILESTEM/R1/}
      	#removes R1 from FILESTEM (FILESTEM ends in _ therefore not needed in "text" names)
      
      echo $FILESTEM
      
      kallisto quant -i $KallistoDIR/reference_transcripts.1Lv.clean.idx -o $KallistoDIR/$FILESTEM"kallisto-out" -b, --bootstrap-samples=100 --threads=12 --pseudobam $file $FILESTEM"R2_cleanq.fq.gz" || error_exit "$LINENO: kallisto error at $FILESTEM"
      
      
      done
      
      echo "kallisto-clstr-cleanq done"

```



## EdgeR Expression Analysis


Kallisto outputs were downloaded and placed into three separate R working directories for each analysis of the different number of samples as per sections 4.3, 4.5 and 4.6.
/RWorkingDir/R-kallisto-clstr-6&7/kallisto-clstr/          #contained folder for final 6 analysed samples 
/RWorkingDir/R-kallisto-clstr/kallisto-clstr/              #contained folder for all eight samples names 
/RWorkingDir/R-kallisto-clstr-4/kallisto-clstr/            #contained folder for four female samples, two from each season group 

All sample folders within these ~/kallisto-clstr/ folders were named as the following:
"G1_SepF" "G2_SepF" "G3_MarF" "G4_MarM" "G5_SepM" "G6_SepM" "G7_MarF" "G8_MarF"
So that names and labels are consistent throughout this document and in figures. 

***

Exploration for these data has been included for different groupings in order to explore the effects of sample removal, or potential effect of sex as a confounding factor. Samples G6 and G7 have ultimately been removed due to batch effects caused by sequencing runs. 

The first analysis below shows the analysis across season of collection for the remaining 6 sampes and represents data included in thesis chapter 5. 

Preliminary analyses of all original eight samples and an additional exploration of only the four female samplesare included at the end of this document.  


***


## Six _T. adelaidensis_ Individuals Collected Between Two Seasonal Periods. 
(These methods exclude samples G6 and G7).  

This document separates samples by Seasonal group factors AND Sex Group Factors separately to explore the effects on the data.  

Import the data:
```{r}
#create paths
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr-6&7/kallisto-clstr/")
DIR <- ("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr-6&7/kallisto-clstr/")

head(DIR)

paths <- list.dirs(path = DIR, full.names = FALSE, recursive = FALSE)
#the working dir only includes kallisto output for the kidneys at this time
head(paths)

Kcaught <- catchKallisto(paths, verbose = TRUE)

setwd("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr-6&7")
#View(Kcaught)
```

```{r, include=FALSE}
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown")
```

Set the group factors. Input Sample files are listed in number order G1-G8 excluding G6 and G7.
Group factors are applied to the DGElist object and are the point of comparison for expression analyses.

#### Season of Collection: 1 = March, 2 = Sep. 
For the purposes of consistent labelling per group factors some April collections are referred to in the March group. Accurate collection information and dates are outlined in the methods chapter.
            
#### Individual Sex: 1 = Female, 2 = Male.  

```{r}
#these factors correlate to season of collection. 1 = March, 2 = Sep.
season_group<-factor(c(2,2,1,1,2,1))

#these factors correlate to individuals sex. 1 = Female, 2 = Male
sex_group<-factor(c(1,1,1,2,2,1))

```

***

### Group Factor: **Season**

Create the EdgeR DGE list for use in subsequent analyses
```{r}
dge_Season <- DGEList(counts=Kcaught$counts/Kcaught$annotation$Overdispersion, genes=Kcaught$annotation, group=season_group)
#View(dge_Season)
#names(dge_Season)
dge_Season
```

Obtain Counts Per Million to standardise count data comparison
```{r}
myTPM_Season <- dge_Season$counts
# head(myTPM_Season)
```


### Initial Data Exploration

Which values in myCPM are greater than 0.5?
This produces a logical matrix with TRUEs and FALSEs. TRUE values are samples with > 0.5 counts in that sample per million
```{r}
thresh_Season <- myTPM_Season > 0.5
# This produces a logical matrix with TRUEs and FALSEs
head(thresh_Season)
```

Insert colour palette for following visualisations consistency
```{r}
library(RColorBrewer)
myPalette <- c("#999999", "#F0E442", "#56B4E9", "#009E73","#D55E00", "#CC79A7")
```

\newpage

Plot CPM distribution with logged CPM
```{r, error=TRUE}
#export the plot as png
png("plotDensities_Season.png", width = 600)
unfilteredExpr_Season <- cpm(dge_Season, log=T)
plotDensities(unfilteredExpr_Season, col=myPalette, legend=TRUE)
```

![Log Counts per million Densities, for poly-a selected mRNA expression data from six _T. adelaidensis_ kidney samples](./FigureCopyDir/6Samples/plotDensities_Season.png)

\newpage

Visualise library sizes per sample
```{r, error=TRUE}
#export the plot as png
png("LibSize_Season.png", width = 600)
# The names argument tells the barplot to use the sample names on the x-axis
# The last argument rotates the axis names
barplot(dge_Season$samples$lib.size, names=colnames(dge_Season), las=2, col=myPalette, legend=TRUE)
#title("Library size per sample")

```

![Library size of poly-a selected mRNA sequenced from kidney tissue of six _T. adelaidensis_ individuals](./FigureCopyDir/6Samples/LibSize_Season.png)

\newpage

Visualise total estimated transcript counts per sample
```{r, error=TRUE}
#export the plot as png
png("boxplot_Season.png", width = 600)
boxplot(dge_Season$counts, col=myPalette, las=2, legend=TRUE)
#title("Boxplot of transcript total estimated counts")
```

![Total estimated counts for poly-a selected mRNA expression data from six _T. adelaidensis_ kidney samples](./FigureCopyDir/6Samples/boxplot_Season.png)

\newpage

Visualise logged total estimated transcript counts per sample
```{r, error=TRUE}
# Get log2 counts per million
logcounts_Season <- cpm(dge_Season,log=TRUE)
# Check distributions of samples using boxplots
#export the plot as png
png("boxplot-logcounts_Season.png", width = 600)
boxplot(logcounts_Season, xlab="", ylab="Log2 counts per million",las=2, col=myPalette, legend=TRUE)
# Let's add a blue horizontal line that corresponds to the median logCPM
abline(h=median(logcounts_Season),col=myPalette, legend=TRUE)
#title("Boxplots of log Counts Per Million")

```

![Log2 of estimated counts per million, for poly-a selected mRNA expression data from six _T. adelaidensis_ kidney samples](./FigureCopyDir/6Samples/boxplot-logcounts_Season.png)

\newpage

Visualise sample variation using an MDS plot
```{r, error=TRUE}
#export the plot as png
png("MDS_Season.png", width = 600)
plotMDS(dge_Season, col=myPalette)
```

![MDS plot of variation among six _T. adelaidensis_ individuals](./FigureCopyDir/6Samples/MDS_Season.png)

\newpage

Prepare data for a HeatMap (based off of [this document](https://combine-australia.github.io/RNAseq-R/06-rnaseq-day1.html))

Estimate the variance for each row in the logcounts matrix
```{r}
var_genes_season <- apply(logcounts_Season, 1, var)
head(var_genes_season)
```

Transcript IDs for the top 250 most variable transcripts
```{r}
select_var_decT_season <- names(sort(var_genes_season, decreasing=TRUE))[1:250]
head(select_var_decT_season)
```

Subset logcounts matrix
```{r}
highly_variable_lcpm_season <- logcounts_Season[select_var_decT_season,]
dim(highly_variable_lcpm_season)
```

```{r}
head(highly_variable_lcpm_season)
```

Reorder columns to cluster Season factor groups, and sexes
```{r}
col.order_season <- c("G1_SepF","G2_SepF","G5_SepM","G3_MarF","G8_MarF","G4_MarM")
highly_variable_lcpm_season <- highly_variable_lcpm_season[,col.order_season]
```

```{r}
head(highly_variable_lcpm_season)
```

Colour pallete
```{r}
mypalette2 <- brewer.pal(11,"RdYlBu")
morecols <- colorRampPalette(mypalette2)
```

Colour vector for season_group factor variable: note - factor groups changed to match column reordering in highly_variable_lcpm March=1 September=2
```{r}
Reordered_season_group<-factor(c(2,2,2,1,1,1))
col.cell_season <- c("chartreuse3","orange")[Reordered_season_group]
```

Create the HeatMap
```{r}
png(file="High_var_genes.heatmap_season.png", width = 7.5*300, height = 5*300, res = 300, pointsize = 8)
# 5 x 300 pixels height = 5*300, res = 300,# 300 pixels per inch
heatmap.2(highly_variable_lcpm_season,col=rev(morecols(50)),Colv=FALSE,trace="none", main="Top 250 most variable genes across samples",ColSideColors=col.cell_season,scale="row", margins=c(8,4), srtCol=45, labRow = "", dendrogram = c("none"))

```

\newpage
 
![Top 250 most variable transcripts among six _T. adelaidensis_ individuals, based on total counts per million values](./FigureCopyDir/6Samples/High_var_genes.heatmap_season.png)

\newpage

Transcript IDs for the bottom 250 most variable transcripts = i.e the least variable transcripts
```{r}
select_var_decF_season <- names(sort(var_genes_season, decreasing=FALSE))[1:250]
head(select_var_decF_season)
```

Subset logcounts matrix
```{r}
low_variable_lcpm_season <- logcounts_Season[select_var_decF_season,]
dim(low_variable_lcpm_season)
```

```{r}
head(low_variable_lcpm_season)
```

Reorder columns to cluster Season factor groups, and sexes
```{r}
col.order_season <- c("G1_SepF","G2_SepF","G5_SepM","G3_MarF","G8_MarF","G4_MarM")
low_variable_lcpm_season <- low_variable_lcpm_season[,col.order_season]
```

```{r}
head(low_variable_lcpm_season)
```

Colour pallete
```{r}
mypalette2 <- brewer.pal(11,"RdYlBu")
morecols <- colorRampPalette(mypalette2)
```

Colour vector for season_group factor variable: note - factor groups changed to match column reordering in low_variable_lcpm March=1 September=2
```{r}
Reordered_season_group<-factor(c(2,2,2,1,1,1))
col.cell_season <- c("chartreuse3","orange")[Reordered_season_group]
```

Create the HeatMap
```{r}
png(file="Low_var_genes.heatmap-season.png", width = 7.5*300, height = 5*300, res = 300, pointsize = 8)
# 5 x 300 pixels height = 5*300, res = 300,# 300 pixels per inch
heatmap.2(low_variable_lcpm_season,col=rev(morecols(50)),Colv=FALSE,trace="none", main="Bottom 250 most variable genes across samples",ColSideColors=col.cell_season,scale="row", margins=c(8,4), srtCol=45, labRow = "", dendrogram = c("none"))
```

\newpage
 
![Bottom 250 most variable transcripts among six _T. adelaidensis_ individuals, based on total counts per million values](./FigureCopyDir/6Samples/Low_var_genes.heatmap-season.png)

\newpage

### Calculations

```{r, error=TRUE}
# Data with actual counts, not TPM = "dge"
head(dge_Season)
head(myTPM_Season)
```

Filter out genes with less than 5 reads in 2 samples for each transcript
```{r, error=TRUE}
filter_Season <- apply(dge_Season, 1, function(x) length(x[x>5])>=2)
filtered_Season <- dge_Season[filter_Season,]
head(filtered_Season)
```

Run calculations
```{r, error=TRUE}
dge_Season<-calcNormFactors(dge_Season)
dge_Season<-estimateCommonDisp(dge_Season)
dge_Season<-estimateTagwiseDisp(dge_Season)
```

```{r, error=TRUE}
dge_Season$common.dispersion
```

```{r, error=TRUE}
head(dge_Season$tagwise.dispersion)
```

### Differentially Expressed Genes:

View the top ten 'differentially expressed genes'
```{r, error=TRUE}
results_Season<-exactTest(dge_Season)
topTags(results_Season)
```

Output the top 25 table of values based on calculated PValue
```{r, error=TRUE}
tab_Season <- topTags(results_Season, n=25, sort.by = "PValue")
head(tab_Season)
write.table(tab_Season, file="Kgenelist-Top25_Season.txt")
```

Join this table of the top 25 differentially expressed genes with the annotation data
```{r, error=TRUE}
Kgenelist_Top25_Season <- read.csv("./Kgenelist-Top25_Season.txt", sep = ' ', row.names = NULL, header = TRUE, stringsAsFactors = FALSE)
Kgenelist_Top25_Season <- Kgenelist_Top25_Season %>% rename(TranscriptID_3 = row.names) #rename column to join with

Kgenelist_Top25_Season_Gene <- left_join(Kgenelist_Top25_Season, BLASTx.gene.join_Database.Full.PBT.Summary, by = "TranscriptID_3") #add annotation info
#head (Kgenelist_Top25_Season_Gene)
colSums(!is.na(Kgenelist_Top25_Season_Gene)) #count column entries that are not NA

write.csv(Kgenelist_Top25_Season_Gene, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/Kgenelist_Top25_Season_Gene.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(Note: the column "TranscriptID_3" indicates the full list of 25 output top differentially expressed genes. "TranscriptID" marks the start of the joined BLASTx data: 16 of these 25 transcripts returned a BLASTx result and gene ID under "Gene". "tax_id" marks the start of the joined genes of interest dataframe: only three of these 16 BLASTx results were also identified in the NCBI gene database search as of particular interest (under "Symbol" genes slc25a15, mcm3 and tk1).


Output the entire table of values
```{r, error=TRUE}
tab_Season_all <- topTags(results_Season, n=Inf, sort.by = "PValue")
write.table(tab_Season_all, file="Kgenelist_Season.txt")
```

Output summary expression data
```{r, error=TRUE}
dim(results_Season)

summary(de_Season <- decideTests(results_Season))
summary_Season <- (de_Season <- decideTests(results_Season))
head(summary_Season)
write.table(summary_Season, file="summary_Season.txt")

detags_Season <- rownames(dge_Season)[as.logical(de_Season)]
head(detags_Season)
head(results_Season)
```

Filter and sort this summary to only include upregulated and downregulated transcripts
```{r, error=TRUE}
summary_Season.sort <- read.csv("./summary_Season.txt", sep = ' ', row.names = NULL, header = TRUE, stringsAsFactors = FALSE)
summary_Season.sort <- summary_Season.sort %>% rename(TranscriptID_3 = row.names, "Regulation" = X2.1) #the name "TranscriptID_3" is used for consistency and joining as seen in the next step
```

Get the Expression statistics for all transcripts together with the BLASTx statistics in one file
```{r, error=TRUE}
#Header for column 1 manually changed to "TranscriptID_3" in "Kgenelist_Season.txt"
All_Season.sort <- read.csv("./Kgenelist_Season.txt", sep = ' ', header = TRUE, stringsAsFactors = TRUE)
All_Season.sort$TranscriptID_3 <- rownames(All_Season.sort)
#head(All_Season.sort)
#head(BLASTx.gene.join_Database.Full.PBT.Summary)
BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp <- left_join(BLASTx.gene.join_Database.Full.PBT.Summary, All_Season.sort, by = "TranscriptID_3")
head(BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp)
```

Export this file as a .csv
```{r, warning=FALSE, Message=FALSE, echo = TRUE}
BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp$other_designations<-gsub(",","-",as.character(BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp$other_designations))
BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp$description<-gsub(",","-",as.character(BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp$description))
#removing commas to retain the integrity of column delimiters
write.csv(BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp.csv", quote = FALSE, row.names = FALSE, col.names = TRUE)
```


filter out the transcripts which are significantly upregulated when compared by season
```{r, error=TRUE}
summary_Season.sort_UP <- filter(summary_Season.sort, Regulation == "1") #upregulated transcripts denoted as +1
head(summary_Season.sort_UP)
nrow(summary_Season.sort_UP)
```

Extract the full information about the putative identiry of these transcripts from the Summary file created in Section 3.2.3 above
```{r, error=TRUE}
summary_Season.sort_UP_info <- semi_join(BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp, summary_Season.sort_UP, by = "TranscriptID_3")
#head (summary_Season.sort_UP_info)
nrow(summary_Season.sort_UP_info)


write.csv(summary_Season.sort_UP_info, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Season.sort_UP_info.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: although there are 189 transcripts significantly upregulated, only 99 of these returned a BLASTx result for their corresponding putative coding region)  

Filter for Transcripts in this list which ALSO have information from the 'genes of interest' list
```{r, error=TRUE}
summary_Season.sort_UP_info.GOI <- summary_Season.sort_UP_info %>% filter(!is.na(GeneID))

nrow(summary_Season.sort_UP_info.GOI)

write.csv(summary_Season.sort_UP_info.GOI, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Season.sort_UP_info.GOI.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: 7 of the 99 upregaulted transcripts that returned a BLASTx result, are represented in the 'genes of interest' list)


Do the same for the Seasonally deownregulated transcripts
```{r, error=TRUE}
summary_Season.sort_DOWN <- filter(summary_Season.sort, Regulation == "-1") #downregulated transcripts denoted as -1
head (summary_Season.sort_DOWN)
nrow(summary_Season.sort_DOWN)
```


Extract the full information about the putative identity of these transcripts from the Summary file created in Section 3.2.3 above
```{r, error=TRUE}
summary_Season.sort_DOWN_info <- semi_join(BLASTx.gene.join_Database.Full.PBT.Summary_SeasonExp, summary_Season.sort_DOWN, by = "TranscriptID_3")
#head (summary_Season.sort_DOWN_info)
nrow(summary_Season.sort_DOWN_info)

write.csv(summary_Season.sort_DOWN_info, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Season.sort_DOWN_info.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: although there are 25 transcripts significantly downregulated, only 19 of these returned a BLASTx result for their corresponding putative coding region)  

Filter for Transcripts in this list which ALSO have information from the 'genes of interest' list
```{r, error=TRUE}
summary_Season.sort_DOWN_info.GOI <- summary_Season.sort_DOWN_info %>% filter(!is.na(GeneID))

nrow(summary_Season.sort_DOWN_info.GOI)

write.csv(summary_Season.sort_DOWN_info.GOI, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Season.sort_DOWN_info.GOI.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: 1 of the 19 downregaulted transcripts that returned a BLASTx result, are represented in the 'genes of interest' list)


\newpage

Visualise the logged Fold Change against Average logged counts per million, in expression of transcripts between season 1 (March/April) and 2 (September) using plotsmear: red indicates PValue < 0.05
```{r, error=TRUE}
png("plotsmear_Season.png", width = 600)
plotSmear(results_Season, de.tags=detags_Season)
```

![Log Fold Change vs average Log counts per million for expression level comparing September to March season variables among six _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05.](./FigureCopyDir/6Samples/plotsmear_Season.png)

\newpage

Visualise differential expression of transcripts between season 1 (March/April) and 2 (September) with negative Logged PValue against Logged Fold Change in expression: Red indicates a transcript with PValue < 0.05, and Log Fold change > 2
```{r}
volcanoData_Season <- cbind(results_Season$table$logFC, -log10(results_Season$table$PValue))
colnames(volcanoData_Season) <- c("logFC", "negLogPval")
DEGs_Season <- results_Season$table$PValue < 0.05 & abs(results_Season$table$logFC) > 2

point.col_Season <- ifelse(DEGs_Season, "red", "black")
png("volcanoData_Season.png", width = 600)
plot(volcanoData_Season, pch = 16, col = point.col_Season, cex = 0.5)
#pch is shape 1-26 are vectors, cex is size
```

![Negative Log of the p-value against Log Fold Change in expression level comparing September to March season variables among six _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05 and _also_ a log fold change >2.](./FigureCopyDir/6Samples/volcanoData_Season.png)




***
\newpage

### Group Factor: **Sex**

Create the EdgeR DGE list for use in subsequent analyses
```{r}
dge_Sex <- DGEList(counts=Kcaught$counts/Kcaught$annotation$Overdispersion, genes=Kcaught$annotation, group=sex_group)
#View(dge_Sex)
#names(dge_Sex)
dge_Sex
```

Obtain Counts Per Million to standardise count data comparison
```{r}
myTPM_Sex <- dge_Sex$counts
# head(myTPM_Sex)
```


### Initial Data Exploration

Which values in myCPM are greater than 0.5?
This produces a logical matrix with TRUEs and FALSEs. TRUE values are samples with > 0.5 counts in that sample per million
```{r}
thresh_Sex <- myTPM_Sex > 0.5
# This produces a logical matrix with TRUEs and FALSEs
head(thresh_Sex)
```

Raw Counts per million and library sizes will be consistent with the Season comparison above. There is no need to plot them again however scripts are included to create separate R objects for downstream application.

CPM distribution with logged CPM
```{r, error=TRUE}
#export the plot as png
png("plotDensities_Sex.png", width = 600)
unfilteredExpr_Sex <- cpm(dge_Sex, log=T)
plotDensities(unfilteredExpr_Sex, col=myPalette, legend=TRUE)
```

Library sizes per sample
```{r, error=TRUE}
#export the plot as png
png("LibSize_Sex.png", width = 600)
# The names argument tells the barplot to use the sample names on the x-axis
# The last argument rotates the axis names
barplot(dge_Sex$samples$lib.size, names=colnames(dge_Sex), las=2, col=myPalette, legend=TRUE)
```

Total estimated transcript counts per sample
```{r, error=TRUE}
#export the plot as png
png("boxplot_Sex.png", width = 600)
boxplot(dge_Sex$counts, col=myPalette, las=2, legend=TRUE)
#title("Boxplot of transcript total estimated counts")
```

Logged total estimated transcript counts per sample
```{r, error=TRUE}
# Get log2 counts per million
logcounts_Sex <- cpm(dge_Sex,log=TRUE)
# Check distributions of samples using boxplots
#export the plot as png
png("boxplot-logcounts_Sex.png", width = 600)
boxplot(logcounts_Sex, xlab="", ylab="Log2 counts per million",las=2, col=myPalette, legend=TRUE)
# Let's add a blue horizontal line that corresponds to the median logCPM
abline(h=median(logcounts_Sex),col=myPalette, legend=TRUE)
#title("Boxplots of log Counts Per Million")
```


### Calculations

```{r, error=TRUE}
# Data with actual counts, not TPM = "dge"
#head(dge_Sex)
#head(myTPM_Sex)
```

Filter out genes with less than 5 reads in 2 samples for each transcript
```{r, error=TRUE}
filter_Sex <- apply(dge_Sex, 1, function(x) length(x[x>5])>=2)
filtered_Sex <- dge_Sex[filter_Sex,]
head(filtered_Sex)
```

Run calculations
```{r, error=TRUE}
dge_Sex<-calcNormFactors(dge_Sex)
dge_Sex<-estimateCommonDisp(dge_Sex)
dge_Sex<-estimateTagwiseDisp(dge_Sex)
```

```{r, error=TRUE}
dge_Sex$common.dispersion
```

```{r, error=TRUE}
head(dge_Sex$tagwise.dispersion)
```

### Differentially Expressed Genes:

View the top ten 'differentially expressed genes'
```{r, error=TRUE}
results_Sex<-exactTest(dge_Sex)
topTags(results_Sex)
```

Output the top 25 table of values based on calculated PValue
```{r, error=TRUE}
tab_Sex <- topTags(results_Sex, n=25, sort.by = "PValue")
#head(tab_Sex)
write.table(tab_Sex, file="Kgenelist-Top25_Sex.txt")
```

Output the entire table of values
```{r, error=TRUE}
tab_Sex_all <- topTags(results_Sex, n=Inf, sort.by = "PValue")
write.table(tab_Sex_all, file="Kgenelist_Sex.txt")
```

Output summary expression data
```{r, error=TRUE}
dim(results_Sex)

summary(de_Sex <- decideTests(results_Sex))
summary_Sex <- (de_Sex <- decideTests(results_Sex))

write.table(summary_Sex, file="summary_Sex.txt")

detags_Sex <- rownames(dge_Sex)[as.logical(de_Sex)]
#head(detags_Sex)
#head(results_Sex)
```


Filter and sort this summary to only include upregulated and downregulated transcripts
```{r, error=TRUE}
summary_Sex.sort <- read.csv("./summary_Sex.txt", sep = ' ', row.names = NULL, header = TRUE, stringsAsFactors = FALSE)
summary_Sex.sort <- summary_Sex.sort %>% rename(TranscriptID_3 = row.names, "Regulation" = X2.1)
```

filter out the transcripts which are significantly upregulated when compared by sex
```{r, error=TRUE}
summary_Sex.sort_UP <- filter(summary_Sex.sort, Regulation == "1") #upregulated transcripts denoted as +1
head(summary_Sex.sort_UP)
nrow(summary_Sex.sort_UP)
```

Extract the full information about the putative identiry of these transcripts from the Summary file created in Section 3.2.3 above
```{r, error=TRUE}
summary_Sex.sort_UP_info <- semi_join(BLASTx.gene.join_Database.Full.PBT.Summary, summary_Sex.sort_UP, by = "TranscriptID_3")
#head (summary_Sex.sort_UP_info)
nrow(summary_Sex.sort_UP_info)


write.csv(summary_Sex.sort_UP_info, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Sex.sort_UP_info.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: although there are 364 transcripts significantly upregulated, only 210 of these returned a BLASTx result for their corresponding putative coding region)  

Filter for Transcripts in this list which ALSO have information from the 'genes of interest' list
```{r, error=TRUE}
summary_Sex.sort_UP_info.GOI <- summary_Sex.sort_UP_info %>% filter(!is.na(GeneID))

nrow(summary_Sex.sort_UP_info.GOI)

write.csv(summary_Sex.sort_UP_info.GOI, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Sex.sort_UP_info.GOI.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: 15 of the 210 upregaulted transcripts that returned a BLASTx result, are represented in the 'genes of interest' list)

Do the same for the deownregulated transcripts
```{r, error=TRUE}
summary_Sex.sort_DOWN <- filter(summary_Sex.sort, Regulation == "-1") #downregulated transcripts denoted as -1
head (summary_Sex.sort_DOWN)
nrow(summary_Sex.sort_DOWN)
```

Extract the full information about the putative identiry of these transcripts from the Summary file created in Section 3.2.3 above
```{r, error=TRUE}
summary_Sex.sort_DOWN_info <- semi_join(BLASTx.gene.join_Database.Full.PBT.Summary, summary_Sex.sort_DOWN, by = "TranscriptID_3")
#head (summary_Sex.sort_DOWN_info)
nrow(summary_Sex.sort_DOWN_info)

write.csv(summary_Sex.sort_DOWN_info, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Sex.sort_DOWN_info.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: although there are 27 transcripts significantly downregulated, 24 of these returned a BLASTx result for their corresponding putative coding region) 

Filter for Transcripts in this list which ALSO have information from the 'genes of interest' list
```{r, error=TRUE}
summary_Sex.sort_DOWN_info.GOI <- summary_Sex.sort_DOWN_info %>% filter(!is.na(GeneID))

nrow(summary_Sex.sort_DOWN_info.GOI)

write.csv(summary_Sex.sort_DOWN_info.GOI, "~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown/ReferenceClusters/summary_Sex.sort_DOWN_info.GOI.csv", quote = FALSE, row.names = FALSE, col.names = TRUE) #write the file to a .csv
```
(note: 3 of the 24 downregaulted transcripts that returned a BLASTx result, are represented in the 'genes of interest' list)



\newpage

Visualise the logged Fold Change against Average logged counts per million, in expression of transcripts between sex 1 (Female) and 2 (Male) using plotsmear: red indicates PValue < 0.05
```{r, error=TRUE}
png("plotsmear_Sex.png", width = 600)
plotSmear(results_Sex, de.tags=detags_Sex)
```

![Log Fold Change vs average Log counts per million for expression level comparing Male to Female sex variables among six _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05.](./FigureCopyDir/6Samples/plotsmear_Sex.png)

\newpage

Visualise differential expression of transcripts between sex 1 (Female) and 2 (Male) with negative Logged PValue against Logged Fold Change in expression: Red indicates a transcript with PValue < 0.05, and Log Fold change > 2
```{r}
volcanoData_Sex <- cbind(results_Sex$table$logFC, -log10(results_Sex$table$PValue))
colnames(volcanoData_Sex) <- c("logFC", "negLogPval")
DEGs_Sex <- results_Sex$table$PValue < 0.05 & abs(results_Sex$table$logFC) > 2

point.col_Sex <- ifelse(DEGs_Sex, "red", "black")
png("volcanoData_Sex.png", width = 600)
plot(volcanoData_Sex, pch = 16, col = point.col_Sex, cex = 0.5)
#pch is shape 1-26 are vectors, cex is size
```

![Negative Log of the p-value against Log Fold Change in expression level comparing Male to Female sex variables among six _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05 and _also_ a log fold change >2.](./FigureCopyDir/6Samples/volcanoData_Sex.png)

\newpage


***

## Sequencing Batch Effects

Note G1, G2, G3, G4, G5 and G8 were all sequenced together on an Illumina Hiseq and library prep was done with the same kit. G6 and G7 were later prepared in a separate batch using a different kit and sequenced on an Illumina Hiseq with fewer samples, and have a much higher read depth. These 2 represent a September Male, and March Female, respectively.  

The removal of G6 and G7 from the dataset results in two groups of 3 per season, each with 2 females and 1 male, as analysed above. The below analyses have been provided for comparison of how group manipulation and inclusion of these samples affects results.  

***

## Eight _T. adelaidensis_ Individuals Collected Between Two Seasonal Periods.  

**Exploration of all Eight samples by each group factor:**

Import the data:
```{r}
#create paths
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr/kallisto-clstr/")
DIR8 <- ("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr/kallisto-clstr/")
head(DIR8)

paths8 <- list.dirs(path = DIR8, full.names = FALSE, recursive = FALSE)
#the working dir only includes kallisto output for the kidneys at this time
#View(paths)
head(paths8)

Kcaught8 <- catchKallisto(paths8, verbose = TRUE)

setwd("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr")
#View(Kcaught)
```

```{r, include=FALSE}
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown")
```

Set the group factors. Input Sample files are listed G1-G8
Group factors are applied to the DGElist object and are the point of comparison for expression analyses.

#### Season of Collection: 1 = March, 2 = Sep. 
For the purposes of consistent labelling per group factors some April collections are referred to in the March group. Accurate collection information and dates are outlined in the methods chapter.
            
#### Individual Sex: 1 = Female, 2 = Male.  

```{r}
#these factors correlate to season of collection. 1 = March, 2 = Sep.
season8_group<-factor(c(2,2,1,1,2,2,1,1))

#these factors correlate to individuals sex. 1 = Female, 2 = Male
sex8_group<-factor(c(1,1,1,2,2,2,1,1))
```


***

### Group Factor: **Season**

Create the EdgeR DGE list
```{r}
dge_Season8 <- DGEList(counts=Kcaught8$counts/Kcaught8$annotation$Overdispersion, genes=Kcaught8$annotation, group=season8_group)
#View(dge_Season8)
names(dge_Season8)
dge_Season8
```

Obtain Counts Per Million
```{r}
myTPM_Season8 <- dge_Season8$counts
# head(myTPM_Season8)
```

### Initial Data Exploration

```{r}
# Which values in myCPM are greater than 0.5?
thresh_Season8 <- myTPM_Season8 > 0.5
# This produces a logical matrix with TRUEs and FALSEs
#head(thresh_Season8)
```

Insert colour pallate for following visualisations consistency
```{r, }
myPalette8 <- c("#999999", "#F0E442", "#56B4E9", "#009E73","#D55E00", "#0072B2", "#E69F00", "#CC79A7")
```


Plot CPM distribution with logged CPM
```{r, error=TRUE}
#export the plot as png
png("plotDensities_Season8.png", width = 600)
unfilteredExpr_Season8 <- cpm(dge_Season8, log=T)
plotDensities(unfilteredExpr_Season8, col=myPalette8, legend=TRUE)
```

![Log Counts per million Densities, for poly-a selected mRNA expression data from eight _T. adelaidensis_ kidney samples](./FigureCopyDir/8Samples/plotDensities_Season8.png)

\newpage

Visualise library sizes per sample
```{r, error=TRUE}
# The names argument tells the barplot to use the sample names on the x-axis
# The las argument rotates the axis names
#export the plot as png
png("Barplot.LibSize_Season8.png", width = 600)
barplot(dge_Season8$samples$lib.size, names=colnames(dge_Season8), las=2, col=myPalette8)
#title("Barplot of library sizes")
```

![Library size of poly-a selected mRNA sequenced from kidney tissue of eight _T. adelaidensis_ individuals](./FigureCopyDir/8Samples/Barplot.LibSize_Season8.png)

\newpage

Visualise transcript total estimated counts per sample
```{r, error=TRUE}
#export the plot as png
png("Boxplot.Counts_Season8.png", width = 600)
boxplot(dge_Season8$counts, col=myPalette8,  las=2, legend=TRUE)
#title("Boxplot of total unfiltered Counts")
```

![Total estimated counts, for poly-a selected mRNA expression data from eight _T. adelaidensis_ kidney samples](./FigureCopyDir/8Samples/Boxplot.Counts_Season8.png)

\newpage

```{r, error=TRUE}
# Get log2 counts per million
logcounts_Season8 <- cpm(dge_Season8,log=TRUE)
# Check distributions of samples using boxplots
#export the plot as png
png("boxplot.CPM_Season8.png", width = 600)
boxplot(logcounts_Season8, xlab="", ylab="Log2 counts per million",las=2, col=myPalette8, legend=TRUE)
# Let's add a blue horizontal line that corresponds to the median logCPM
abline(h=median(logcounts_Season8),col=myPalette8, legend=TRUE)
#title("Boxplots of log Counts Per Million")
```

![Log2 of estimated counts per million, for poly-a selected mRNA expression data from eight _T. adelaidensis_ kidney samples](./FigureCopyDir/8Samples/boxplot.CPM_Season8.png)

\newpage

Visualise sample variation using an MDS plot
```{r, error=TRUE}
#export the plot as png
png("MDS_Season8.png", width = 600)
plotMDS(dge_Season8, col=myPalette8)
#title("MDS plot")
```

![MDS plot of variation among eight _T. adelaidensis_ individuals](./FigureCopyDir/8Samples/MDS_Season8.png)


\newpage

### Calculations

```{r, error=TRUE}
# Data with actual counts, not TPM = "dge"
#head(dge_Season8)
#head(myTPM_Season8)
```

filter out genes with less than 5 reads in 2 samples for each transcript
```{r, error=TRUE}
filter_Season8 <- apply(dge_Season8, 1, function(x) length(x[x>5])>=2)
filtered_Season8 <- dge_Season8[filter_Season8,]
#head(filtered_Season8)
```

Run calculations
```{r, error=TRUE}
dge_Season8<-calcNormFactors(dge_Season8)
dge_Season8<-estimateCommonDisp(dge_Season8)
dge_Season8<-estimateTagwiseDisp(dge_Season8)
```

```{r, error=TRUE}
dge_Season8$common.dispersion
```

```{r, error=TRUE}
head(dge_Season8$tagwise.dispersion)
```

Note that filtering did not change the appearance of the MDS plot
```{r, error=TRUE}
plotMDS(dge_Season8, col=myPalette8)
```

### Differentially Expressed Genes:

View the top ten 'differentially expressed genes'
```{r, error=TRUE}
results_Season8<-exactTest(dge_Season8)
topTags(results_Season8)
```

Output summary expression data
```{r, error=TRUE}
dim(results_Season8)

summary(de_Season8 <- decideTests(results_Season8))
Summary_Season8 <- (de_Season8 <- decideTests(results_Season8))

write.table(Summary_Season8, file="summary_Season8.txt")

detags_Season8 <- rownames(dge_Season8)[as.logical(de_Season8)]
```

\newpage

Visualise with a plotsmear
```{r, error=TRUE}
detags_Season8 <- rownames(dge_Season8)[as.logical(de_Season8)]
#export the plot as png
png("plotSmear_Season8.png", width = 600)
plotSmear(results_Season8, de.tags=detags_Season8)
```

![Log Fold Change vs average Log counts per million for expression level comparing Season 2 (September collection) to Season 1 (April/March collection) among eight _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05.](./FigureCopyDir/8Samples/plotSmear_Season8.png)

\newpage

Visualise with a volcano plot
```{r}
#export the plot as png
png("volcanoData_Season8.png", width = 600)
volcanoData_Season8 <- cbind(results_Season8$table$logFC, -log10(results_Season8$table$PValue))
colnames(volcanoData_Season8) <- c("logFC", "negLogPval")
DEGs_Season8 <- results_Season8$table$PValue < 0.05 & abs(results_Season8$table$logFC) > 2
point.col_Season8 <- ifelse(DEGs_Season8, "red", "black")
plot(volcanoData_Season8, pch = 16, col = point.col_Season8, cex = 0.5)
```

![Negative Log of the p-value against Log Fold Change in expression level comparing Season 2 (September collection) to Season 1 (April/March collection) among eight _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05 and _also_ a log fold change >2.](./FigureCopyDir/8Samples/volcanoData_Season8.png)


***
\newpage


### Group Factor: **Sex**

Create the EdgeR DGE list
```{r}
dge_Sex8 <- DGEList(counts=Kcaught8$counts/Kcaught8$annotation$Overdispersion, genes=Kcaught8$annotation, group=sex8_group)
#View(dge_Sex8)
names(dge_Sex8)
dge_Sex8
```


```{r}
# Obtain Counts Per Million
myTPM_Sex8 <- dge_Sex8$counts
# Have a look at the output
#head(myTPM_Sex8)
```

\newpage

### Initial Data Exploration
```{r}
# Which values in myCPM are greater than 0.5?
thresh_Sex8 <- myTPM_Sex8 > 0.5
# This produces a logical matrix with TRUEs and FALSEs
#head(thresh_Sex8)
```

Raw Counts per million and library sizes will be consistent with the Season comparison above. There is no need to plot them again however scripts are included to create separate R objects for downstream application.

CPM distribution with logged CPM
```{r, error=TRUE}
#export the plot as png
png("plotDensities_Sex8.png", width = 600)
unfilteredExpr_Sex8 <- cpm(dge_Sex8, log=T)
plotDensities(unfilteredExpr_Sex8, col=myPalette8, legend=TRUE)
```

Library sizes per sample
```{r, error=TRUE}
# The names argument tells the barplot to use the sample names on the x-axis
# The las argument rotates the axis names
#export the plot as png
png("Barplot.LibSize_Sex8.png", width = 600)
barplot(dge_Sex8$samples$lib.size, names=colnames(dge_Sex8), las=2, col=myPalette8)
#title("Barplot of library sizes")
```

Transcript total estimated counts per sample
```{r, error=TRUE}
#export the plot as png
png("Boxplot.Counts_Sex8.png", width = 600)
boxplot(dge_Sex8$counts, col=myPalette8,  las=2, legend=TRUE)
#title("Boxplot of total unfiltered Counts")
```

Logged total estimated transcript counts per sample
```{r, error=TRUE}
# Get log2 counts per million
logcounts_Sex8 <- cpm(dge_Sex8,log=TRUE)
# Check distributions of samples using boxplots
#export the plot as png
png("boxplot.CPM_Sex8.png", width = 600)
boxplot(logcounts_Sex8, xlab="", ylab="Log2 counts per million",las=2, col=myPalette8, legend=TRUE)
# Let's add a blue horizontal line that corresponds to the median logCPM
abline(h=median(logcounts_Sex8),col=myPalette8, legend=TRUE)
#title("Boxplots of log Counts Per Million")
```

### Calculations

```{r, error=TRUE}
# Data with actual counts, not TPM = "dge"
#head(dge_Sex8)
#head(myTPM_Sex8)
```

filter out genes with less than 5 reads in 2 samples for each transcript
```{r, error=TRUE}
filter_Sex8 <- apply(dge_Sex8, 1, function(x) length(x[x>5])>=2)
filtered_Sex8 <- dge_Sex8[filter_Sex8,]
#head(filtered_Sex8)
```

Run calculations
```{r, error=TRUE}
dge_Sex8<-calcNormFactors(dge_Sex8)
dge_Sex8<-estimateCommonDisp(dge_Sex8)
dge_Sex8<-estimateTagwiseDisp(dge_Sex8)
```

```{r, error=TRUE}
dge_Sex8$common.dispersion
```

```{r, error=TRUE}
head(dge_Sex8$tagwise.dispersion)
```

### Differentially Expressed Genes:

View the top ten 'differentially expressed genes'
```{r, error=TRUE}
results_Sex8<-exactTest(dge_Sex8)
topTags(results_Sex8)
```

Output summary expression data
```{r, error=TRUE}
dim(results_Sex8)

summary(de_Sex8 <- decideTests(results_Sex8))
Summary_Sex8 <- (de_Sex8 <- decideTests(results_Sex8))

write.table(Summary_Sex8, file="summary_Sex8.txt")

detags_Sex8 <- rownames(dge_Sex8)[as.logical(de_Sex8)]
```

\newpage

Visualise with a plotsmear
```{r, error=TRUE}
#export the plot as png
png("plotSmear_Sex8.png", width = 600)
plotSmear(results_Sex8, de.tags=detags_Sex8)
```

![Log Fold Change vs average Log counts per million for expression level comparing Male to Female as variables among eight _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05.](./FigureCopyDir/8Samples/plotSmear_Sex8.png)

\newpage

Visualise with a volcano plot
```{r}
#export the plot as png
png("volcanoData_Sex8.png", width = 600)
volcanoData_Sex8 <- cbind(results_Sex8$table$logFC, -log10(results_Sex8$table$PValue))
colnames(volcanoData_Sex8) <- c("logFC", "negLogPval")
DEGs_Sex8 <- results_Sex8$table$PValue < 0.05 & abs(results_Sex8$table$logFC) > 2
point.col_Sex8 <- ifelse(DEGs_Sex8, "red", "black")
plot(volcanoData_Sex8, pch = 16, col = point.col_Sex8, cex = 0.5)
```

![Negative Log of the p-value against Log Fold Change in expression level comparing Male to Female as variables among eight _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05 and _also_ a log fold change >2.](./FigureCopyDir/8Samples/volcanoData_Sex8.png) 

***
\newpage


## Four _T. adelaidensis_ individuals collected between two seasonal periods.  

**Exploration of 2:2 Female samples by Season group factor:**  

Skink G5 represents a September Male, and is also markedly different from the remaining 5 samples on the MDS plot when 6 samples are compared in groups of 3. There is a chance G5 is responsible for a large amount of the observed variation in expression, and the skew in the data towards higher expressed genes. 
These Four females represent two from September, and two from March/April, and were sequenced on the same run. This section is purely exploratory on how results may have remained consistent or markedly different on the removal of sample G5 (and on balance, also the other male G4).  

Import the Data:
```{r}
#create paths
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr-4/kallisto-clstr/")
DIR4 <- ("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr-4/kallisto-clstr/")
head(DIR4)

paths4F <- list.dirs(path = DIR4, full.names = FALSE, recursive = FALSE)
#the working dir only includes kallisto output for the kidneys at this time
#View(paths4F)

Kcaught4F <- catchKallisto(paths4F, verbose = TRUE)

setwd("~/@Uni/@Flinders University PhD/RWorkingDir/R-kallisto-clstr-4")
#View(Kcaught4F)
```

```{r, include=FALSE}
setwd("~/@Uni/@Flinders University PhD/RWorkingDir/Workflow-readthedown")
```

Set the group factors. Input Sample files are listed G1-G8
Group factors are applied to the DGElist object and are the point of comparison for expression analyses.

#### Season of Collection: 1 = March, 2 = Sep. 
For the purposes of consistent labelling per group factors some April collections are referred to in the March group. Accurate collection information and dates are outlined in the methods chapter.
            
#### Individual Sex: 1 = Female, 2 = Male.  

```{r}
#these factors correlate to season of collection. 1 = March, 2 = Sep.
Season4F_group<-factor(c(2,2,1,1))
```


### Group Factor: **Season**

Create the EdgeR DGE list
```{r}
dge_Season4F <- DGEList(counts=Kcaught4F$counts/Kcaught4F$annotation$Overdispersion, genes=Kcaught4F$annotation, group=Season4F_group)
#View(dge_Season4F)
names(dge_Season4F)
dge_Season4F
```

Obtain Counts Per Million
```{r}
myTPM_Season4F <- dge_Season4F$counts
# head(myTPM_Season4F)
```

### Initial Data Exploration

```{r}
# Which values in myCPM are greater than 0.5?
thresh_Season4F <- myTPM_Season4F > 0.5
# This produces a logical matrix with TRUEs and FALSEs
#head(thresh_Season4F)
```

Insert colour pallate for following visualisations consistency
```{r, }
myPalette4 <- c("#999999", "#F0E442", "#56B4E9", "#CC79A7")
```

\newpage

Plot CPM distribution with logged CPM
```{r, error=TRUE}
#export the plot as png
png("plotDensities_Season4F.png", width = 600)
unfilteredExpr_Season4F <- cpm(dge_Season4F, log=T)
plotDensities(unfilteredExpr_Season4F, col=myPalette4, legend=TRUE)
```

![Log Counts per million Densities, for poly-a selected mRNA expression data from four _T. adelaidensis_ kidney samples](./FigureCopyDir/4Samples/plotDensities_season4F.png) 

\newpage

Visualise library sizes per sample
```{r, error=TRUE}
# The names argument tells the barplot to use the sample names on the x-axis
# The las argument rotates the axis names
#export the plot as png
png("Barplot.LibSize_Season4F.png", width = 600)
barplot(dge_Season4F$samples$lib.size, names=colnames(dge_Season4F), las=2, col=myPalette4)
#title("Barplot of library sizes")
```

![Library size of poly-a selected mRNA sequenced from kidney tissue of four _T. adelaidensis_ individuals](./FigureCopyDir/4Samples/Barplot.LibSize_Season4F.png)

\newpage

Visualise transcript total estimated counts per sample
```{r, error=TRUE}
#export the plot as png
png("Boxplot.Counts_Season4F.png", width = 600)
boxplot(dge_Season4F$counts, col=myPalette4,  las=2, legend=TRUE)
#title("Boxplot of total unfiltered Counts")
```

![Total estimated counts, for poly-a selected mRNA expression data from four _T. adelaidensis_ kidney samples](./FigureCopyDir/4Samples/Boxplot.Counts_Season4F.png)

\newpage

```{r, error=TRUE}
# Get log2 counts per million
logcounts_Season4F <- cpm(dge_Season4F,log=TRUE)
# Check distributions of samples using boxplots
#export the plot as png
png("boxplot.CPM_Season4F.png", width = 600)
boxplot(logcounts_Season4F, xlab="", ylab="Log2 counts per million",las=2, col=myPalette4, legend=TRUE)
# Let's add a blue horizontal line that corresponds to the median logCPM
abline(h=median(logcounts_Season4F),col=myPalette4, legend=TRUE)
#title("Boxplots of log Counts Per Million")
```

![Log2 of estimated counts per million, for poly-a selected mRNA expression data from four _T. adelaidensis_ kidney samples](./FigureCopyDir/4Samples/boxplot.CPM_Season4F.png)

\newpage

Visualise sample variation using an MDS plot
```{r, error=TRUE}
#export the plot as png
png("MDS_Season4F.png", width = 600)
plotMDS(dge_Season4F, col=myPalette4)
#title("MDS plot")
```

![MDS plot of variation among four _T. adelaidensis_ individuals](./FigureCopyDir/4Samples/MDS_Season4F.png)


\newpage

### Calculations

```{r, error=TRUE}
# Data with actual counts, not TPM = "dge"
#head(dge_Season4F)
#head(myTPM_Season4F)
```

filter out genes with less than 5 reads in 2 samples for each transcript
```{r, error=TRUE}
filter_Season4F <- apply(dge_Season4F, 1, function(x) length(x[x>5])>=2)
filtered_Season4F <- dge_Season4F[filter_Season4F,]
head(filtered_Season4F)
```

Run calculations
```{r, error=TRUE}
dge_Season4F<-calcNormFactors(dge_Season4F)
dge_Season4F<-estimateCommonDisp(dge_Season4F)
dge_Season4F<-estimateTagwiseDisp(dge_Season4F)
```

```{r, error=TRUE}
dge_Season4F$common.dispersion
```

```{r, error=TRUE}
head(dge_Season4F$tagwise.dispersion)
```

Note that at this smaller scale filtering _does_ change the appearance of the MDS plot
```{r, error=TRUE}
plotMDS(dge_Season4F, col=myPalette4)
```

### Differentially Expressed Genes:

View the top ten 'differentially expressed genes'
```{r, error=TRUE}
results_Season4F<-exactTest(dge_Season4F)
topTags(results_Season4F)

tab_Season4F <- topTags(results_Season4F)
write.table(tab_Season4F, file="Kgenelist-Top25_Season4F.txt")
```

Output summary expression data
```{r, error=TRUE}
dim(results_Season4F)

summary(de_Season4F <- decideTests(results_Season4F))
Summary_Season4F <- (de_Season4F <- decideTests(results_Season4F))

write.table(Summary_Season4F, file="summary_Season4F.txt")

detags_Season4F <- rownames(dge_Season4F)[as.logical(de_Season4F)]
```

\newpage

Visualise with a plotsmear
```{r, error=TRUE}
detags_Season4F <- rownames(dge_Season4F)[as.logical(de_Season4F)]
#export the plot as png
png("plotSmear_Season4F.png", width = 600)
plotSmear(results_Season4F, de.tags=detags_Season4F)
```

![Log Fold Change vs average Log counts per million for expression level comparing Season 2 (September collection) to Season 1 (April/March collection) among four _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05.](./FigureCopyDir/4Samples/plotSmear_Season4F.png)

\newpage

Visualise with a volcano plot
```{r}
#export the plot as png
png("volcanoData_Season4F.png", width = 600)
volcanoData_Season4F <- cbind(results_Season4F$table$logFC, -log10(results_Season4F$table$PValue))
colnames(volcanoData_Season4F) <- c("logFC", "negLogPval")
DEGs_Season4F <- results_Season4F$table$PValue < 0.05 & abs(results_Season4F$table$logFC) > 2
point.col_Season4F <- ifelse(DEGs_Season4F, "red", "black")
plot(volcanoData_Season4F, pch = 16, col = point.col_Season4F, cex = 0.5)
```

![Negative Log of the p-value against Log Fold Change in expression level comparing Season 2 (September collection) to Season 1 (April/March collection) among four _T. adelaidensis_ individuals. Red dots indicate transcripts with a significant change over season, with p-values <0.05 and _also_ a log fold change >2.](./FigureCopyDir/4Samples/volcanoData_Season4F.png)

***
